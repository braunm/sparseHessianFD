---
title:  Using sparseHessianFD
author:  Michael Braun
date:  "`r Sys.Date()`"
output:  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sparseHessian example}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---



The **sparseHessianFD** package provides an interface to ACM TOMS Algorithm
636 for estimating a Hessian that is sparse, in that a large proportion of
the cross-partial derivatives are zero.  The user provides the following:

1.  an R function that returns the value of the objective function
whene valuated at a parameter vector $P$;
2.  an R function that returns the gradient of that objective function
at $P$; and
3.  the row and column indices for the non-zero elements in the lower triangle of the Hessian.

The non-zero elements are estimated through finite differencing of
the gradients in a way that exploits the sparsity pattern of the
Hessian.

There are, of course, other methods to estimate sparse Hessians, such
as automatic differentiation (AD).  However, operator-overloading
implementations of AD require the objective function to be written
using specialized libraries.  These libraries are available for C++,
Fortran, Matlab, Python, and possibly other programming languages and
environments.  At the present, there are no AD implementations that
are "native" to R.  Other than deriving the Hessian analytically,
finite differencing remains the best option when the objective and
gradient functions are written purely in R.


## Example function

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse Hessian.
 Suppose we have a dataset of $N$ households, each with $T$ opportunities to purchase a particular product.  Let $y_i$ be the number of times household $i$ purchases the product, out of the $T$ purchase opportunities.  Furthermore, let $p_i$ be the probability of purchase; $p_i$ is the same for all $T$ opportunities, so we can treat $y_i$ as a binomial random variable.  The purchase probability $p_i$ is heterogeneous, and depends on both $k$ continuous covariates $x_i$, and a heterogeneous coefficient vector $\beta_i$, such that
$$
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1\mathellipsis N
$$

The coefficients can be thought of as sensitivities to the covariates, and they are distributed across the population of households following a multivariate normal distribution with mean $\mu$ and covariance $\Sigma$.   We assume that we know $\Sigma$, but we do not know $\mu$.  Instead, we place a multivariate normal prior on $\mu$, with mean $0$ and covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are $k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$. 

The log posterior density, ignoring any normalization constants, is
$$
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)&=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
$$

```{r, echo=FALSE}
N <- 6
k <- 2
nv1 <- (N+1)*2
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k
nnz1LT <- (N+1)*k*(k+1)/2+N*k
Q <- 1000
nv2 <- (Q+1)*2
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k
nnz2LT <- (Q+1)*k*(k+1)/2+Q*k
```

Since the $\beta_i$ are drawn iid from a multivariate normal,
$\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0$ for all $i\neq
j$.  We also know that all of the $\beta_i$ are correlated with $\mu$.
Therefore, the Hessian will be sparse with a "block-arrow" structure.
For example, if $N=$`r N` and $k=$`r k`, then $p=$`r nv1` and the
Hessian will have the following pattern.

```{r}
M <- as(kronecker(diag(N),matrix(1,k,k)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
```

There are `r nels1` elements in this symmetric matrix, but only  `r nnz1` are
non-zero, and only `r nnz1LT` values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=$`r Q` instead.  In that case,
there are `r nv2` variables in the problem, and more than `r nels2`
elements in the Hessian.  However, only `r nnz2` of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only `r nnz2LT` values.


## Using the package

The functions for computing the objective function, gradient and
Hessian are in the R/ex\_funcs.R file, and a sample dataset for
$N=100$ and $k=2$ is provided with the package.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1]$, and simulate a
vector of variables to evaluate the function.

```{r}
set.seed(123)
data(binary)
Y <- binary$Y
X <- binary$X
T <- binary$T
N <- length(Y)
k <- NROW(X)
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
inv.Sigma <- rWishart(1,k+5,diag(k))[,,1]
inv.Omega <- diag(k)
```

The package requires that functions for the objective and gradient
take a single argument:  a vector of parameters.  Since the objective function
depends on data and priors, we write a factory function that returns a
list of functions for the value, the gradient and the
Hessian.  The Hessian function is there so we can compare your
estimates with the "true" value.


```{r}

make.funcs <- function(Y, X, inv.Omega, inv.Sigma) {
    res <- vector("list", length=3)
    names(res) <- c("fn", "gr", "hessian")
    res$fn <-  function(pars) {
        log.f(pars, Y=Y, X=X, inv.Omega=inv.Omega, inv.Sigma=inv.Sigma)
    }
    res$gr <-  function(pars) {
        get.grad(pars, Y=Y, X=X, inv.Omega=inv.Omega, inv.Sigma=inv.Sigma)
    }
    res$hessian <-  function(pars) {
        get.hess(pars, Y=Y, X=X, inv.Omega=inv.Omega, inv.Sigma=inv.Sigma)
    }
    return(res)
}

f <- make.funcs(Y, X, inv.Omega, inv.Sigma)

true.val <- f$fn(P)
true.grad <- f$gr(P)
true.hess <- f$hessian(P)  
```

In this example, we have a function that computes the Hessian. The
package provides a utility for extracting the sparsity pattern from a
sparse matrix, and returning a list of row and column indices.  So we
use that here. In practice, you won't have a sparse Hessian; that's
the whole point of this package.  But presumable, you will know enough
about the objective function that you can identify which elements of
the Hessian will be non-zero.

```{r}
## True Hessian -> lower triangle only
hess.struct <- Matrix.to.Coord(tril(true.hess))
##hess.struct <- Matrix.to.Coord(as(tril(true.hess), "lMatrix"))
```
Next, create a new instance of a sparseHessianFD object.  You need to
provide the length of the parameter vector, as well as the value and
gradient functions.
```{r}
obj <- new("sparseHessianFD", nvars, f$fn, f$gr) )
```

At this point, you can evaluate the function and gradient directly
through this object.
```{r}
obj$fn(P)
all.equal(obj$gr(P), true.grad)  ## should print TRUE
```

Before computing the Hessian, the object needs to be initialized with
the sparsity pattern, and some control parameters for the estimation
method.
```{r}
obj$hessian.init(hess.struct$iRow, hess.struct$jCol, 1, 1e-6)
```
The first two arguments are **integer** vectors for the row and column
indices of the ***lower triangular part*** of the Hessian.  The third
argument determines whether a direct(0) or indirect(1) method is
used (more on that later).  The last argument is the "width" of the
interval used for finite differencing.  This number should be very
small (close to machine precision), although values that are too small
could cause precision or stability problems in some cases.

One the object is initialized, we can estimate the Hessian at any
value, as long as the sparsity pattern of the Hessian does not change.
```{r}
hs <- obj$hessian(P)
all.equal(hs, true.hess) ## should be true
```

The package does provide a convenience function that returns a
sparseHessianFD object in a single step.  Any additional parameters
that are to be passed to the objective and gradient functions must be
named.  This convenience function eliminates the need to write factory
functions, as above.


```{r}
obj2 <- new.sparse.hessian.obj(par, log.f, get.grad, hess.struct, 
                               Y=Y, X=X, inv.Omega=inv.Omega,
                               inv.Sigma=inv.Sigma)
all.equal(true.val, obj2$fn(P))
all.equal(true.grad, obj2$gr(P))
all.equal(true.hess, obj2$hessian(P))
```


\section{Discussion points}

For some functions, deriving and coding a gradient
analytically is straightforward (either by hand, or using a symbolic
computation tool like \pkg{Mathematica}).  For many others, like log
posterior densities, analytic Hessians can be
messy to derive and code.  Even then, storing and working with a $p \times p$ matrix
is expensive when $p$ is large.  The \pkg{sparseHessianFD} package is
useful when the Hessian is sparse and the sparsity pattern is known
in advance, even when $p$ is massively large.  The speed at which
\pkg{sparseHessianFD} computes the Hessian depends on the
sparsity pattern.  For block diagonal Hessians, as in the example
above, computation time will grow with the size of each heterogeneous
parameter, and the number of population-level parameters, but not with
the number of heterogeneous units.  As $N$ grows, the number of
non-zero elements in the Hessian grows linearly, and the number of
gradient differences that need to be computed is constant.

We should note that finite differencing is not the current ``state of
the art'' for estimating sparse Hessians.  Algorithmic differentiation (AD)
packages can be faster and more exact (and of course they can compute
the gradient as well).  A critical requirement of an AD package when we need to differentiate
scalar-valued functions with large $p$ is that it support
``reverse-mode'' differentiation.   For C++, \pkg{CppAD} and \pkg{Adol-C} are
popular choices, and others may be available for Matlab and
Python. \pkg{AD Model Builder} is a scripting language for AD that can
be called from \proglang{R} using the \pkg{R2admb} package \citep{R_R2admb}.

\documentclass[10pt]{article}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{parskip}
\usepackage{setspace}
\linespread{1.1}

\usepackage[dvipsnames,svgnames,x11names,hyperref]{xcolor}
\usepackage[colorlinks=true, urlcolor=NavyBlue]{hyperref}

%%make knitr environment line spacing separate from LaTeX
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\Chol{Chol }
\DeclareMathOperator\cov{cov}

\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
\newcommand{\filename}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}

 \newcommand{\Prior}{\pi(\theta)}
 \newcommand{\Post}{\pi(\theta|y)}
 \newcommand{\Gtheta}{g(\theta)}
 \newcommand{\Phitheta}{\Phi(\theta|y)}
 \newcommand{\Ly}{\mathcal{L}(y)}
 \newcommand{\Dy}{\mathcal{D}(\theta,y)}


\usepackage[style=authoryear,%
			backend=biber,%
			maxbibnames=99,%
                        bibencoding=utf8,%
                        maxcitenames=1,
                        citetracker=true,
                        dashed=false,
                        maxalphanames=1,%
                        backref=false,%
			doi=true,%
                        url=true, %
			isbn=false,%
                        mergedate=basic,%
                        dateabbrev=true,%
                        natbib=true,%
                        uniquename=false,%
                        uniquelist=false,%
                        useprefix=true,%
                        firstinits=false
			]{biblatex}

 \addbibresource{./sparseHessianFD.bib}

 \setlength{\bibitemsep}{1em}
\AtEveryCitekey{\ifciteseen{}{\defcounter{maxnames}{2}}}
\DeclareFieldFormat[article,incollection,unpublished]{title}{#1} %No quotes for article titles
\DeclareFieldFormat[thesis]{title}{\mkbibemph{#1}} % Theses like book
                                % titles
\DeclareFieldFormat{pages}{#1} %% no pp prefix before page numbers
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{ % Don't print In: for journal articles
  \printtext{\bibstring{in}\intitlepunct}} %% but use elsewhere
}
\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \printfield{number}%
  \setunit{\addcomma\addspace}%
  \printfield{eid}}

\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{date}{#1}
\AtEveryBibitem{\clearfield{day}}

\renewbibmacro*{issue+date}{% print month only
  \printtext{%
    \printfield{issue}\addspace%
    \newunit%
%\printtext{\printfield{month}}%
}
  \newunit}

\renewbibmacro*{publisher+location+date}{% no print year
  \printlist{location}%
  \iflistundef{publisher}
    {\setunit*{\addcomma\space}}
    {\setunit*{\addcolon\space}}%
  \printlist{publisher}%
  \setunit*{\addcomma\space}%
%%\printdate
  \newunit}

\renewcommand*{\nameyeardelim}{~} %no comma in cite
\renewcommand{\bibitemsep}{1ex}
\def\bibfont{\small}

\title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
\author{Michael Braun\\Cox School of Business\\Southern Methodist University}
\date{April 12, 2015}

\begin{document}

\maketitle

<<setup1, echo=FALSE, cache=FALSE>>=
library(methods, quietly = TRUE)
library(sparseHessianFD, quietly = TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE, comment = "#",
                           message=FALSE, tidy=FALSE, cache = TRUE) #
@

The Hessian matrix of a log likelihood or log posterior density plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  Also, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $p$ variables has $p^2$
elements, of which $p(p+1)/2$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the size of the
problem.  Even if the Hessian were needed just once, computing it for
functions with hundreds of thousands of variables might not be
practical, given available time and processing power.

However, for a large general class of problems, the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  The most prominent example of this class is the log
posterior density of unknown parameters in a Bayesian hierarchical
model, under the mild condition of conditional independence.  If two
heterogeneous units are conditionally independent, the observed
outcome from one unit provides no information about the outcome from
the other, except for whatever inference we can make about the
distribution of unit-specific parameters.  In that case, the cross-partial derivatives across
unit-level parameters are zero.  As the number of heterogeneous units
increases, the size of the Hessian grows quadratically, but the number
of \emph{non-zero} elements grows only linearly.  For large datasets,
the Hessian becomes increasingly sparse.  Furthermore, if we know
which variables are conditionally independent, we know the
\emph{sparsity pattern} (which elements are zero and which
are not).  The sparsity pattern typically does not change when
evaluating the Hessian at different variable values.

The \pkg{sparseHessianFD} package is a tool for estimation
of a Hessian when the sparsity pattern is known in advance.  The
method is efficient for models and datasets with a large number of
conditionally independent heterogeneous units, a small
number of parameters specific to each unit, and a relatively small
number of population-level parameters.  Such hierarchical models with
``high $N$, low $k$'' appear frequently in fields like marketing
research, in which databases record transactions for a massive
number of households, but each household's activity is governed by a
small number of randomly distributed latent parameters.

\pkg{sparseHessianFD} uses finite differencing (FD) to generate a numerical
estimate of the Hessian.  FD is a well-known method for estimating
derivatives of functions, and we explain it in Section \ref{sec:FD}.
Using FD, approximating the first derivative of $f(x)$ with respect to
$x_k$, for $k=1,...,K$ variables, is
conceptually simple:  the slope between $(x_k, f(x_k))$ and $(x_k+h,
f(x_k+h))$ approaches the derivate as $h$ gets arbitrarily close to
zero. The disadvantages are that the cost of FD grows with the number of
variables $K$, and that as $h$ approaches zero, the result is
increasingly unstable.

The problem is even worse when computing a
dense Hessian.  The $k^{th}$ column of the Hessian, $H_k$, can be approximated
by a difference in the gradients.  Let $e_k$ be the $k^{th}$
coordinate vector, so

\begin{align}
  H_k &\approx\frac{f'(x+he_k)-f'(x)}{h}
\end{align}

If $f'(x)$ is estimated using FD, then the complexity of estimating
the Hessian is quadratic, and numerical error from the gradient and
Hessian computations are combined.  In many cases, the gradient is
computed quickly and accurately by other means, such as through
deriving it analytically or using automatic differentiation (more on
that later).  In that case, numerical error comes from only the
Hessian computation, and complexity is only linear.

In order to justify the use of \pkg{sparseHessianFD}, we need to
maintain a set of assumptions about the underlying model, and the
workflow of the user.

\begin{enumerate}
\item The objective function is twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in an objective function that itself is hard to compute.
\item The gradient can be computed exactly (within machine precision,
  of course), and that computation time is a small, constant multiple of
  the time to compute the objective function, regardless of the number
  of variables.
\item Preferred alternatives to computing the Hessian are not
  available.  Examples include computing the Hessian directly from a
  symbolic derivation, or using AD software, as with the gradient.
  Direct computation of the Hessian will be numerically exact, but not necessarily
  easier or faster than \pkg{sparseHessianFD} when
  \pkg{sparseHessianFD} can exploit the sparsity in the Hessian.  AD
  can be used to compute the Hessian as well as the gradient, and many
  modern AD libraries are facile with sparse matrices.  But AD is not
  always available, hence the need for \pkg{sparseHessianFD}.
  \item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.  The user must be able to provide
    the row and column indices of the non-zero elements in the lower
    triangle of the Hessian.  This should be easy to do for
    hierarchical Bayesian models.
  \item The user can accept some small amount of approximation error.
    Since FD is a numerical approximation technique, the estimate of the Hessian
    will not be exact as either computing the Hessian directly from a
    symbolic derivation, or using AD.  By ``small,'' we mean a
    relative error roughly on the order of $10^{-6}$ or less.  In our
    experience should be widely achievable on double-precision machines.   If such errors are
    too large for a particular application, then \pkg{sparseHessianFD}
    may not be the best tool for the job. Exceptions
    would be cases of objective functions that are poorly conditioned,
    with ridges, plateaus or other pathologies, or cases for which
    such tiny errors will incur large costs.
\end{enumerate}


  The ``time to compute the gradient'' restriction is surprisingly weak,
  since XXX show that gradients computed via a reverse mode automatic
  differentiation meet this restriction.  When AD computation is not
  available, one would need to derive the gradient symbolically.


\section{Example function}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.
 Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities.  Furthermore, let $p_i$ be the probability of
 purchase; $p_i$ is the same for all $T$ opportunities, so we can
 treat $y_i$ as a binomial random variable.  The purchase probability
 $p_i$ is heterogeneous, and depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$, such that
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients can be thought of as sensitivities to the covariates, and they are distributed across the population of households following a multivariate normal distribution with mean $\mu$ and covariance $\Sigma$.   We assume that we know $\Sigma$, but we do not know $\mu$.  Instead, we place a multivariate normal prior on $\mu$, with mean $0$ and covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are $k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}

<<setup1 echo=FALSE>>=
require(Matrix)
require(sparseHessianFD)
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

Since the $\beta_i$ are drawn iid from a multivariate normal,
$\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0$ for all $i\neq
j$.  We also know that all of the $\beta_i$ are correlated with
$\mu$.  The structure of the Hessian depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=`r N`$ and $k=`r k`$, then there are `r nv1` total variables, and the Hessian will have the following pattern.

<<>>=
M <- as(kronecker(diag(N),matrix(1,k,k)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@


There are `r nels1` elements in this symmetric matrix, but only  `r nnz1` are
non-zero, and only `r nnz1LT` values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=`r Q`$ instead.  In that case,
there are `r nv2` variables in the problem, and more than $`r
floor(nels2/10^6)`$ million
elements in the Hessian.  However, only $`r nnz2`$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only `r nnz2LT` values.


Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "off-diagonal" sparsity pattern.

<<>>=
M <- as(kronecker(matrix(1,k,k), diag(N)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@

In both cases, the number of non-zeros is the same.  The
block-diagonal case may lead to slightly faster initialization,
but repeated computation of the Hessian will take the same time as
with the "off-diagonal" case.


\section{Using the package}

The functions for computing the objective function, gradient and
Hessian for this example are in the R/binary.R file.  The package
also includes a sample dataset with simulated data from the binary choice example.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function.

<<>>=
set.seed(123)
data(binary)
str(binary)
N <- length(binary$Y)
k <- NROW(binary$X)
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
@

<<echo=FALSE>>=
options(scipen=-999)
@

This dataset represents the simulated choices for $N= `r N`$ customers
over $T= `r T`$ purchase opportunties, where the probability of purchase
is influenced by $k= `r k`$ covariates.

The objective function for the binary choice example is `binary.f` and the gradient function is
`binary.grad`.  The first argument to both is the variable vector, and
the argument lists must be the same for both.  For this example, we
need to provide the data list ($X$, $Y$ and $T$) and the prior
parameter list
($\Sigma^{-1}$ and $\Omega^{-1}$). The functions also have an
`order.row` argument to change the ordering of the variables (and
thus, the sparsity pattern). If `order.row=TRUE`, then the Hessian
will have an off-diagonal pattern.  If `order.row=FALSE`, then the
Hessian will have a block-arrow pattern.

For testing and demonstration purposes, we also have a `binary.hess`
function that returns the Hessian as a sparse `dgCMatrix` object (see
the Matrix package).

<<>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two **integer**
vectors:  one each for the row and column indices of the non-zero
elements of the ***lower triangular part*** of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the following
convenience function to extract the appropriate index vectors.

<<>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

Next, we create a new instance of a sparseHessianFD object with an
"initial variable" $P$, and the row and column indices of the non-zero
elements in the lower triangle of the Hessian.  We also pass in any
other arguments for `binary.f` and `binary.grad`.  We accept the default
values for other arguments to `sparseHessianFD.new`.

<<>>=
obj <- new("sparseHessianFD", P, binary.f, binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors,
       order.row=FALSE)
@

Now we can evaluate the function value, gradient and Hessian through
`obj`.

<<>>=
f <- obj$fn(P)
gr <- obj$gr(P)
hs <- obj$hessian(P) #$
@

Note that the member functions in the sparseHessianFD class take only
one argument:  the variable vector.  All of the other arguments are
stored in `obj`.

Do we get the same results that we would get after calling `binary.f`,
`binary.grad` and `binary.hess` directly?  Let's see.


<<>>=
all.equal(f, true.f)
all.equal(gr, true.grad)
all.equal(hs, true.hess)
@

If there is any difference, keep in mind that `hs` is a numerical
estimate that is not always exact.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.

\subsection{Speed comparison}

Instead of using this package, we could treat the Hessian as dense,
and use the hessian function numDeriv package. The advantage of the
numDeriv package is that it does not require the gradient.  However,
you can see that it takes some time to run.

<<echo=FALSE>>=
options(scipen=0)
@

<<>>=
library(numDeriv)
hess.time <- system.time(H1 <- obj$hessian(P))
fd.time <- system.time(H2 <- hessian(obj$fn, P))
H2 <- drop0(H2, 1e-7) ## treat values < 1e-8 as zero
print(hess.time)
print(fd.time)
@

The sparseHessianFD package can be substantially faster than estimating a dense
Hessian by brute force finite differencing.  The cost of this speed up
is that the user does need to provide the gradient and the sparsity
structure.  As with everything in life, there are trade-offs.

\subsection{Quick summary}
In short, to use the package, follow the following steps:

\begin{enumerate}
\item Write R functions to return the value and gradient of the
objective function.
\item Determine the row and column indices of the non-zero elements of
the lower triangle of the Hessian.
\item Pick a variable vector (i.e., a starting value) at which you can
    initialize the sparseHessian object.  It doesn't really matter
    what this vector is, as long as the function value and gradient
    elements are all finite.
  \item Create a new sparseHessianFD object using the sparseHessianFD.new
function.  For this example, call that object F.
\item Compute Hessian at x by calling F$hessian(x). %$
\end{document}

\documentclass[10pt]{article}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{colortbl}
\usepackage{parskip}
\usepackage{setspace}
\linespread{1.1}


\usepackage{etoolbox}
\newtoggle{tikz}
\toggletrue{tikz}

\iftoggle{tikz}{
\usepackage{tikz}
}{}

\usepackage{xcolor}
\usepackage{hyperref}

%%make knitr environment line spacing separate from LaTeX
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\Chol{Chol }
\DeclareMathOperator\cov{cov}

\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
\newcommand{\filename}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}


 \newcommand{\df}[3]{\mathsf{d}^{#1}f(#2;#3)}
 \newcommand{\parD}[3]{\mathsf{D}^{#1}_{#2}f(#3)}
\newcommand{\hess}[2]{\mathsf{H}_{#1}f(#2)}
\newcommand{\hessLT}[2]{\mathsf{L}_{#1}f(#2)}
\newcommand{\Mat}[1]{#1}

\usepackage[style=authoryear,%
			backend=biber,%
			maxbibnames=99,%
                        bibencoding=utf8,%
                        maxcitenames=1,
                        citetracker=true,
                        dashed=false,
                        maxalphanames=1,%
                        backref=false,%
			doi=true,%
                        url=true, %
			isbn=false,%
                        mergedate=basic,%
                        dateabbrev=true,%
                        natbib=true,%
                        uniquename=false,%
                        uniquelist=false,%
                        useprefix=true,%
                        firstinits=false
			]{biblatex}

 %%\addbibresource{~/OneDriveBusiness/References/Papers/braun_refs.bib}
\addbibresource{./braun_refs.bib}
\title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
\author{Michael Braun\\Cox School of Business\\Southern Methodist University}
\date{April 23, 2015}

\begin{document}

\maketitle

<<setup1, echo=FALSE, cache=FALSE>>=
library(methods, quietly = TRUE)
library(numDeriv, quietly = TRUE)
library(Matrix, quietly = TRUE)
library(sparseHessianFD, quietly=TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE,
                          message=FALSE, tidy=FALSE, cache = TRUE)
@

The Hessian matrix of a log likelihood function or log posterior
density function plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  More broadly, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $M$ variables has $M^2$
elements, of which $M(M+1)/2$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the number of
decision variables.   For
functions with hundreds of thousands of variables, computing the
Hessian even once might not be practical for applications constrained
by time, storage or processor availability.

For many problems, the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  Consider a log
posterior density in a Bayesian hierarchical
model.  If the outcomes across heterogeneous units are conditionally
independent, the cross-partial derivatives with respect to those
parameters are zero.  As the number of units
increases, the size of the Hessian still grows quadratically, but the number
of \emph{non-zero} elements grows only linearly, and the Hessian
becomes increasingly sparse.  The row and column indices of the
non-zero elements comprise the \emph{sparsity pattern} of the Hessian,
and are typically known in advance, before computing the values of
those elements.

The \pkg{sparseHessianFD} package is a tool for estimating
sparse Hessians using \emph{finite differencing}.  Section \ref{sec:theory} will
cover the specifics, but the basic idea is as follows.  Consider a
function $f(x)$ and its gradient $Df(x)^\top$ (the transpose of the
derivative).  Let $e_m$ be the $m$th
coordinate vector, and let $\delta$ be a sufficiently small scalar
constant. The vector $\hess{m}{x}=\left(\parD{}{}{x+\delta e_m}- \parD{}{}{x}\right)/\delta$ is
a linear approximation of the $m$th column of the Hessian matrix
$\hess{}{x}$. Estimating a dense Hessian involves $M+1$ calculations of the
derivative: one for the derivative at $x$, and one after perturbing each
of the $M$ elements of $x$ one at a time.  However, if the Hessian has
a sparsity pattern that allows it, we could perturb
more than one element of $x$ at a time, evaluate the gradient fewer
than $M+1$ times, and still recover the non-zero
Hessian values.  For some sparsity patterns, estimating a Hessian in
this way can be profoundly efficient.  In fact, for the hierarchical models that we consider in
this paper, the number of gradient evaluations is \emph{constant},
even as additional heterogeneous units are added to the model.  How to
decide which variables can be permuted together is actually a graph
coloring problem, which we discuss in Section \ref{sec:coloring}.

At the outset, we want to mention that there may be some applications for
which \pkg{sparseHessianFD} is not an appropriate package to use. To
extract the maximum benefit from using \pkg{sparseHessianFD}, we need
to accept a few conditions or assumptions.

\begin{enumerate}
  \item Preferred alternatives to computing the Hessian are not
  available.  Finite differencing is not generally a ``first choice''
  method.  Deriving a gradient or Hessian symbolically, and writing a
  subroutine to compute it, will give an exact answer, but might be
  tedious or difficult to implement. Algorithmic differentiation (AD) is
  probably the most efficient method, but requires specialized
  libraries that, at this moment, are not yet available in \proglang{R}.
 \pkg{sparseHessianFD} makes the most sense when the gradient is easy to get, but the Hessian is
  not.
  \item The application can tolerate the approximation error in the Hessian
  that comes with finite differencing methods.
\item The objective function $f(x)$ is  twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in a function that itself is hard to compute.
\item The gradient can
  be computed quickly, easily and \emph{exactly} (within machine
  precision).   We do not recommend using finite differenced gradients
  when computing finite differenced Hessians, for two reasons.  First,
  the approximation errors will be compounded.  Second, the time
  complexity of computing a gradient grows with the number of
  variables when using finite differencing, but can be just a constant
  multiple of the time to compute the objective function using other
  methods like AD \citep[p. xii]{GriewankWalther2008}.
\item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.
\end{enumerate}

Some users may find the requirement to provide the gradient
burdensome.  We take the position that deriving a vector of first
derivatives, and writing \proglang{R} functions to compute them, is a
lot easier than doing the same for a matrix of second derivatives.
Even when we have derived and
coded the
Hessian matrix symbolically, in practice it may still be faster to estimate the Hessian
using \pkg{sparseHessianFD} than coding it directly.   These are the
situations in which \pkg{sparseHessianFD} adds the most value.  If AD software is available to compute the gradient, then
it is probably available for sparse Hessians as well, and
\pkg{sparseHessianFD} would not be needed.

The rest of this article proceeds as follows.  In Section
\ref{sec:sparsity}, we discuss matrix sparsity in the context of a
hierarchical models.  In Section \ref{sec:using}, we demonstrate how
to use the package.  Section \ref{sec:timing} includes some time and
accuracy tests.  We save the discussion of the underlying theory, and
the specific algorithms that we use, for Section \ref{sec:theory}.
If they wish, end users can skip that last section.


\section{Sparsity patterns}\label{sec:sparsity}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.  Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities, and let $p_i$ be the probability of
 purchase.  The heterogeneous parameter $p_i$ is the same for all $T$
 opportunities, so $y_i$ is a binomial random variable.  Define each $p_i$ such that it depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$.
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients are distributed across the population of households
following a multivariate normal distribution with mean $\mu$ and
covariance $\Sigma$.   Assume that we know $\Sigma$, but not $\mu$.
Instead, place a multivariate normal prior on $\mu$, with mean $0$ and
covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are
$k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}
We will return to this example throughout the article.

<<setup2, echo=FALSE>>=
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

The log posterior density in Equation \ref{eq:LPD} has a sparse
Hessian.  Since the $\beta_i$ are drawn iid from a multivariate
normal, and the $y_i$ are conditionally independent,
$\mathsf{H}_{ij}=\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0$ for all $i\neq
j$.  However, all of the $\beta_i$ are correlated with
$\mu$.

The sparsity pattern depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=\Sexpr{N}$ and $k=\Sexpr{k}$, then there are `\Sexpr{nv1}` total variables, and the Hessian will have the following pattern.

<<>>=
Mat <- as(Matrix::kronecker(Matrix::Diagonal(N),Matrix(1,k,k)),"nMatrix")
Mat <- rBind(Mat, Matrix(TRUE,k,N*k))
Mat <- cBind(Mat, Matrix(TRUE, k*(N+1), k))
print(Mat)
@

Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "off-diagonal" sparsity pattern.

<<>>=
Mat <- as(Matrix::kronecker(Matrix(1,k,k), Matrix::Diagonal(N)),"nMatrix")
Mat <- rBind(Mat, Matrix(TRUE,k,N*k))
Mat <- cBind(Mat, Matrix(TRUE, k*(N+1), k))
print(Mat)
@

In both cases, the number of non-zeros is the same.   There are \Sexpr{nels1} elements in this symmetric matrix, but only \Sexpr{nnz1} are
non-zero, and only \Sexpr{nnz1LT} values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=\Sexpr{Q}$ instead.  In that case,
there are \Sexpr{nv2} variables in the problem, and more than $\Sexpr{floor(nels2/10^6)}$ million
elements in the Hessian.  However, only $\Sexpr{nnz2}$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only \Sexpr{nnz2LT} values.



\section{Using the package}\label{sec:using}

\subsection{The sparseHessianFD class}\label{sec:class}

The package functionality is implemented as a reference class
\class{sparseHessianFD}.  The initializer takes the following arguments.
\begin{itemize}
\item[x.init] A numeric vector of variables at which the object will be
  initialized and tested.  It is not stored in the object, so it can
  really be any value, as long as the objective function, gradient and
  Hessian are all finite.
\item[fn,gr] \proglang{R} functions that return the value of the
  objective function, and its gradient. The first argument is the numeric
  variable vector.  Other named arguments can be passed to \func{fn}
  and {gr} as well (as elements of the \funcarg{...} argument).
\item[rows, cols] Integer vectors of the row and column indices of
  the non-zero elements in the \emph{lower triangle} of the Hessian.
\item[direct] This argument is deprecated, and is included only for
  backwards compatibility with earlier versions.
\item[eps] The perturbation amount for finite differencing of the
  gradient to compute the Hessian. Defaults to
  \code{sqrt(.Machine\$double.eps)}.
\item [index1] If \variable{TRUE} (the default), \funcarg{row} and \funcarg{col} use one-based
  indexing.  If \variable{FALSE}, zero-based indexing is used (which is the
  internal storage format for matrix classes in the \pkg{Matrix}
  package).
\item [...] Additional arguments to be passed to \func{fn} and \func{gr}.
\end{itemize}

To create a \class{sparseHessianFD} object, just call
\func{sparseHessianFD}.  If you are accepting all of the default
arguments, the call will look like:

<<eval=FALSE>>=
obj <- sparseHessianFD(x.init, fn, gr, rows, cols, ...)
@
where \funcarg{...} represents all other named arguments that are
passed to \funcarg{fn} and \funcarg{gr}.

The class defines a number of different fields, none of which should
be accessed directly.    The initializer automatically calls the graph
coloring subroutine, evaluates the Hessian at $x$, and performs some
other tests, so it may take some time to create the object.


\subsection{Providing the sparsity pattern}

The sparsity pattern of the Hessian is defined as the row and column
indices of the non-zero elements in the \emph{lower triangle} the Hessian.  Internally, this
pattern is stored in a compressed format, but the
\class{sparseHessianFD} initializer requires rows and columns, to keep
things simple.  It is the responsibility of the user to ensure that
the sparsity pattern is correct. Any elements in the upper triangle
will be automatically removed, but there is no check that a
corresponding element in the lower triangle exists.

The \func{Matrix.to.Coord} function extracts
row and column indices from a sparse matrix.  The input matrix to \func{Matrix.to.Coord} does not have to include
the values (if the full Hessian were known, that would possibly defeat the
purpose of this package).  It is sufficient to supply a logical
or pattern matrix, such as \class{lgCMatrix} or \class{ngCMatrix}.
Rather than trying to keep track of the row and column indices
directly, it might be easier to construct a pattern matrix first,
check visually that the matrix has the right pattern, and then extract
the indices.

The following code constructs a block diagonal matrix, and extracts
the sparsity pattern from its lower triangle.

<<exPattern>>=
Mat <- as(kronecker(Diagonal(3), Matrix(T,2,2)),"nMatrix")
Mat
tril(Mat)
mc <- Matrix.to.Coord(tril(Mat))
mc
@
The list elements \funcarg{mc\$row} and \funcarg{mc\$col} can be
passed to \func{sparseHessianFD} as the \funcarg{row} and
\funcarg{col} arguments, respectively.

To visually check that a proposed sparsity pattern represents the
intended matrix, use the \func{Coord.to.Pattern.Matrix} function,
which is just a wrapper to \pkg{Matrix}'s \func{sparseMatrix} constructor.

<<>>=
pattern <- Coord.to.Pattern.Matrix(mc$rows, mc$cols, dims=dim(Mat))
pattern
@



\subsection{Evaluating the Hessian}

The \func{fn}, \func{gr} and \func{hessian} methods respectively evaluate the
function, gradient and Hessian at a variable vector $x$.

<<eval=FALSE>>=
f <- obj$fn(x)
df <- obj$gr(x)
hess <- obj$hessian(x)
@

The \func{fn} and \func{gr} methods call the same functions that were
provided to the class initializer.  Since the
additional arguments were already supplied as \texttt{...}, they do
not need to be supplied again.  This feature makes subsequent calling
of \func{fn} and \func{gr} simpler, because only the variable is
included in the call.

Similarly, the \func{hessian} method takes the single argument $x$.
The return value is always a \class{dgCMatrix} object (defined in the
\pkg{Matrix} package).  \class{dgCMatrix} objects are sparse matrices,
stored in a compressed, column-oriented format, and includes all
non-zero elements in both the upper and lower triangles.

The \func{fngr} method returns the function and gradient as a list.
The \func{fngrhs} includes the Hessian as well.


\subsection{The example}

Now we can use \pkg{sparseHessianFD} to estimate the Hessian for the
log posterior density of the model from Section \ref{sec:sparsity}.
The package includes functions that compute the value
(\func{binary.f}), the gradient (\func{binary.grad}) and the Hessian
{\func{binary.hess}.  The result from \func{binary.hess} is a
  ``true'' value against which we will compare the estimates from \class{sparseHessianFD}.

  The package also includes sample datasets of different sizes.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function. The \func{binary.f} and \func{binary.grad} functions take the data and
priors as lists.  The \func{data()} call adds the appropriate data
list to the environment, but we need to construct the prior list ourselves.

<<>>=
library(sparseHessianFD)
set.seed(123)
data(binary_small)
binary <- binary_small
str(binary)
N <- length(binary[["Y"]])
k <- NROW(binary[["X"]])
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
nvars
@

<<echo=FALSE>>=
options(scipen=-999)
@

KnThis dataset represents the simulated choices for $N= \Sexpr{N}$ customers
over $T= \Sexpr{T}$ purchase opportunties, where the probability of purchase
is influenced by $k= \Sexpr{k}$ covariates.

The following code chunk evaluates the ``true'' value, gradient and
Hessian.  The \funcarg{order.row} argument tells the function whether
the variables are ordered by household (\variable{TRUE}) or by covariate
(\variable{FALSE}). If \funcarg{order.row} is \variable{TRUE}, then the Hessian
will have an off-diagonal pattern.  If \funcarg{order.row} is \variable{FALSE}, then the
Hessian will have a block-arrow pattern.

<<>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two integer
vectors:  one each for the row and column indices of the non-zero
elements of the lower triangule of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the
\func{Matrix.to.Coord} function to extract the appropriate index vectors.

<<>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

If not, you need to determine the row and column indices manually.  If
the model is hierarchical, you could use the method of constructing
the matrices in \ref{sec:sparsity}.

Now we can create a new instance of a \class{sparseHessianFD} object.

<<>>=
obj <- sparseHessianFD(P, fn=binary.f, gr=binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors, order.row=FALSE)
@

Now we can evaluate the function value, gradient and Hessian through
`obj`, and compare to the true values.

<<>>=
f <- obj$fn(P) #$
all.equal(f, true.f)
gr <- obj$gr(P) #$
all.equal(gr, true.grad)
hs <- obj$hessian(P) #$
all.equal(hs, true.hess)
@

If there is any difference, keep in mind that \variable{hs} is a numerical
estimate that is not always exact.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.

\section{Speed comparison}\label{sec:timing}

The \func{hessian} function in the \pkg{numDeriv} package also
estimates Hessians with finite differences, but treats all Hessians a
dense.  The advantage of using \pkg{numDeriv} over
\pkg{sparseHessianFD} is that \pkg{numDeriv} does not require the gradient.  However,
it does takes some time to run.  As with everything in life, there are trade-offs.

<<echo=FALSE>>=
options(scipen=0)
@

<<>>=
hess.time <- system.time(H1 <- obj$hessian(P)) #$
print(hess.time)
fd.time <- system.time(H2 <- hessian(obj$fn, P)) #$
print(fd.time)
@


    \section{Theory and Implementation}\label{sec:theory}

    This section explains why and how the package works, and can be
    skipped by end users with little interest in such things.


  \subsection{Numerical differentiation}\label{sec:numdiff}


  Let $f(x)$ be a scalar-valued function, and let $x$ and $u$ be
  $M$-dimensional vectors.  For $M=1$, then the definition of the first \emph{derivative} of $f(x)$ is
  \begin{align}
    \label{eq:defDeriv1}
    f'(x)=\lim\limits_{u\to 0}\frac{f(x+u)-f(x)}{u}
  \end{align}

which is equivalent to
\begin{align}
  \label{eq:defDeriv2}
  f(x+u)=f(x)+uf'(x)+o(u)
\end{align}

\footnote{A function $g(x)\in o(x)$ if $\lim\limits_{x\to 0}\dfrac{g(x)}{x}=0$.}

For $M>1$, define the \emph{partial} derivative of $f(x)$ with respect to $x_j$ (the $j$th
component of $x$) as
\begin{align}
  \label{eq:defParD}
\parD{}{j}{x}=\lim\limits_{\delta\to 0}\frac{f(x+\delta e_j)-f(x)}{\delta}
\end{align}
Thus, we can compute a linear
approximation to $\parD{}{j}{x}$ by computing

\begin{align}
  \label{eq:25}
  \parD{}{j}{x}\approx\frac{f(x+\delta e_j)-f(x)}{\delta}
\end{align}
for a sufficiently small $\delta$.


The vector $\parD{}{}{x}=\left(\parD{}{1}{x},\mathellipsis,\parD{}{M}{x}\right)$ is
the vector of all partial derivatives.  The \emph{gradient} is defined as $\nabla f(x)=\left(\parD{}{}{x}\right)^\top$.

The second-order partial derivative is defined as
\begin{align}
  \label{eq:14}
  \parD{2}{jk}{x}=\lim\limits_{\delta\to 0}\frac{\parD{}{j}{x+\delta e_k}-\parD{}{j}{x}}{\delta}
\end{align}
and the Hessian matrix is defined as
\begin{align}
  \label{eq:15}
  \hess{}{x}=
  \begin{pmatrix}
    \parD{2}{11}{x}&  \parD{2}{12}{x}&  \mathellipsis &  \parD{2}{1M}{x}\\
    \parD{2}{21}{x}&  \parD{2}{22}{x}&  \mathellipsis &  \parD{2}{2M}{x}\\
    \vdots&\vdots&&\vdots\\
    \parD{2}{K1}{x}&  \parD{2}{K2}{x}&  \mathellipsis &  \parD{2}{MM}{x}
    \end{pmatrix}
\end{align}

To estimate the $k$th column of $\hess{}{x}$, we again choose a
sufficiently small $\delta$, and compute
\begin{align}
  \label{eq:1}
  \hess{k}{x}&\approx\frac{\parD{}{}{x+\delta e_k}-\parD{}{}{x}}{\delta}
\end{align}


For $M=2$, our estimate of a general $\hess{}{x}$ would be
\begin{align}
  \label{eq:FDhess2}
  \hess{}{x}&=
  \begin{pmatrix}
    \parD{}{1}{x_1+\delta, x_2}-\parD{}{1}{x_1,x_2}& \parD{}{1}{x_1,x_2+\delta}-\parD{}{1}{x_1,x_2}\\
        \parD{}{2}{x_1+\delta, x_2}-\parD{}{2}{x_1,x_2}&  \parD{}{2}{x_1,x_2+\delta}-\parD{}{2}{x_1,x_2}
    \end{pmatrix}/\delta
\end{align}

This estimate requires three evaluations of the gradient to get
$\parD{}{}{x_1,x_2}$, $\parD{}{}{x_1+\delta,x_2}$, and
$\parD{}{}{x_1,x_2+\delta}$.

Now suppose that the Hessian is sparse, and that the off-diagonal
elements are zero.  Not only are
\begin{align}
  \label{eq:3}
  \parD{}{1}{x_1,x_2+\delta}-\parD{}{1}{x_1,x_2}&=0\\
    \parD{}{2}{x_1+\delta,x_2}-\parD{}{2}{x_1,x_2}&=0
\end{align},
but also,
\begin{align}
  \label{eq:3a}
  \parD{}{1}{x_1+\delta,x_2+\delta}-\parD{}{1}{x_1+\delta,x_2}&=0\\
    \parD{}{2}{x_1+\delta,x_2+\delta}-\parD{}{2}{x_1,x_2+\delta}&=0
\end{align}
Therefore,
\begin{align}
  \label{eq:FDhess2sp}
  \hess{}{x}\delta&=
  \begin{pmatrix}
    \parD{}{1}{x_1+\delta, x_2+\delta}-\parD{}{1}{x_1,x_2}&0\\
        0&  \parD{}{2}{x_1+\delta,x_2+\delta}-\parD{}{2}{x_1,x_2}
    \end{pmatrix}
\end{align}

This estimate requires only
$\parD{}{}{x_1,x_2}$ and $\parD{}{}{x_1+\delta,x_2+\delta}$, which we
get from only two evaluations of the gradient.  Reducing
the number of gradient evaluations from 3 to 2 depends on knowing that
the cross-partial derivatives are zero.

Now let's consider a more general case. To begin, partition the variables
into $C$ mutually exclusive groups,' so $c_m$ indexes
the color of variable $m$. Let $\Mat{G}$ and $\Mat{Y}$ be $M\times C$
matrices, where $\Mat{G}_{mc}=\delta$ if variable $m$ belongs to group $c$, and zero
otherwise, and let $G_c$ be the $c$th column of $G$.

Next, define column in $Y$ is defined as
\begin{align}
  \label{eq:Yg}
  Y_c^\top&=\parD{}{}{x+G_c}-\parD{}{}{x}
\end{align}

If $C=K$ and $c_m=m$, then $G$ is a diagonal matrix with $\delta$ in each
diagonal element.  The matrix equation $\hess{}{x}G=Y$ represents the linear approximation
$\hess{im}{x}\delta\approx y_{im}$, and we can solve for all elements of $\hess{}{x}$
just by computing $Y$. But if $C<M$,
there must be at least one column $G_c$ with $\delta$ in at least two
rows. Column $Y_c$ would have been computed by perturbing two variables at
once, and we would not have been able to solve for any $\hess{im}{x}$
without further constraints.

The necessary restrictions come from the sparsity pattern and symmetry
pattern of the Hessian. Consider an example with the following values and
sparsity pattern.

\begin{align}
  \label{eq:7}
 \hess{}{x}= \begin{pmatrix}
    h_{11}&0&h_{31}&0&0\\
    0&h_{22}&0&h_{42}&0\\
    h_{31}&0&h_{33}&0&h_{53}\\
    0&h_{42}&0&h_{44}&0\\
    0&0&h_{53}&0&h_{55}
  \end{pmatrix}
\end{align}

Suppose $C=2$, and define the colors of the five variables through the following $\Mat{G}$ matrix.

\begin{align}
  \label{eq:7}
 \Mat{G}= \begin{pmatrix}
   \delta&0\\
   \delta&0\\
   0&\delta\\
   0&\delta\\
   \delta&0
  \end{pmatrix}
\end{align}
Variables 1 and 2 are in group 1, and variables 3, 4 and 5 are in
group 2.  For the moment, we will postpone the discussion of how to
choose $C$ and how to partition the variables, until Section \ref{sec:coloring}.

Next, compute the columns of $\Mat{Y}$ using Equation \ref{eq:Yg}.  We now
have the following system of linear equations from $\hess{}{x}\Mat{G}=\Mat{Y}$.
\begin{align}
  \label{eq:11}
  \begin{aligned}[c]
  h_{11}&=y_{11}\\
  h_{22}&=y_{21}\\
  h_{31}+h_{53}&=y_{31}\\
  h_{42}&=y_{41}\\
  h_{55}&=y_{51}\\
  \end{aligned}
  \qquad
  \begin{aligned}[c]
  h_{31}&=y_{12}\\
  h_{42}&=y_{22}\\
  h_{33}&=y_{32}\\
  h_{44}&=y_{42}\\
  h_{53}&=y_{52}
  \end{aligned}
\end{align}

Note that this system is overdetermined.  Both $h_{31}=y_{12}$ and $h_{53}=y_{52}$
can be determined directly, but $h_{31}+h_{53}=y_{31}$, and $h_{42}$
could be either $y_{41}$ or $y_{22}$.  \Citet{PowellToint1979} prove
that it is sufficient to solve $\hessLT{}{x}G=Y$ via a substitution method, where $\hessLT{}{x}$ is the lower
triangular part of $\hess{}{x}$.  This has the effect of removing the equations
$h_{42}=y_{22}$ and $h_{31}=y_{12}$ from the system, but retaining
$h_{53}=y_{52}$.  We can then solve for
$h_{31}=y_{31}-h_{53}=y_{31}-y_{52}$. Thus, we have determined a
$5\times 5$ Hessian matrix with only three gradient evaluations, in
contrast with the six that would have been needed had $\hess{}{x}$ been treated
as dense.

The $\Mat{Y}$ matrix is, in essence, a compressed representation of
$\hess{}{x}$.  The values in $\Mat{Y}$ are finite differences after
one \emph{or more} variables are perturbed.  Hence, we need two
different algorithms to estimate a sparse Hessian.  The first is to
determine which variables can be perturbed together when computing
$\Mat{Y}$, and the second is to extract the values of $\hess{}{x}$
from $\Mat{Y}$.  We discuss these algorithms next.


\subsection{Partitioning the variables}\label{sec:coloring}


\Citet{PowellToint1979} write that a partitioning is consistent with a
substitution method for estimating a sparse Hessian if and only if no columns
of the of lower triangle of the Hessian that are
in the same group have a non-zero element in the same
row. It turns out that finding valid, efficient partitions can be
characterized as a vertex coloring problem from graph theory \citep{ColemanMore1984}.  In this
sense, each variable is a vertex in an undirected graph, and an edge connects two
vertices $i$ and $j$ if $\hess{ij}{x}\neq 0$.  The
sparsity pattern of the Hessian is the adjacency matrix of the graph.

Before continuing, we need to define some additional terms related to
graph theory. A ``proper'' coloring of a graph is one in which two
vertices with a common edge do not have the same
color. \Citet{ColemanMore1984} define a ``triangular coloring'' of a graph is
a proper coloring with the additional condition that common
neighbors of a vertex do not have the same color.  A triangular
coloring is a special case of an ``acyclic coloring,'' in which any cycle in the graph
uses at least three colors \citep{GebremedhinTarafdar2007}.

In our context, ``intersection set'' of two columns of
$\hessLT{}{x}$ consists of the row indices of non-zero elements that
are common to both columns.  In the ``intersection graph,'' two
vertices are connected if their intersection set is non-empty.  Thus,
the vertices of two columns are connected in the intersection graph if they have any
non-zero element in a common row.

The intersection graph of $\hessLT{}{x}$ is not invariant to
permutation of the rows and columns of $\hess{}{x}$.  Let $\pi$
represent such a permutation, and let $\hessLT{\pi}{x}$ be the lower
triangle of $\pi\hess{}{x}{\pi^\top}$.

\Citet{ColemanCai1986} prove that a partitioning is consistent with a
substitution method if and only if it is an acyclic coloring of the
graph of the sparsity pattern of the Hessian.   \Citet[Theorem 6.1]{ColemanMore1984} show that a coloring is
triangular if and only if it is also a coloring of the intersection
graph of $\hessLT{\pi}{x}$ for some permutation $\pi$.  Therefore,
finding an optimal partitioning of the variables involves finding an
optimal combination of a permutation $\pi$, and coloring algorithm
for the intersection graph of $\hessLT{\pi}{x}$.

Check out the example in Figure \ref{fig:graph1}.  Figure
\ref{fig:graph1adj} is the sparsity pattern of the lower triangle of a
Hessian, and Figure \ref{fig:graph1pic} is an illustration of the
graph.  The coloring for the graph itself is proper, but not
necessarily consistent for a substitution method. For that, we need a
coloring of the intersection graph.  We can deduce the
intersection graph from Figure \ref{fig:graph1adj} by noting that
every column (and thus, every pair of columns) has a non-zero element
in row 7.  Thus, the intersection graph is Figure \ref{fig:graph1int},
which requires seven colors for a proper coloring.  Estimating a
sparse Hessian with this partitioning scheme would be no more
efficient than if treating the Hessian as if it were dense.

\begin{figure}
  \begin{subfigure}[b]{.33\textwidth}\centering
  \begin{tabular}{r|ccccccc}
   & 1&2&3&4&5&6&7\\
    \hline
    1& 1&&&&&&\\
    2&1&1&&&&&\\
    3&0&0&1&&&&\\
    4&0&0&1&1&&&\\
    5&0&0&0&0&1&&\\
    6&1&0&0&0&1&1&\\
    7&1&1&1&1&1&1&1
  \end{tabular}
  \caption{Adjacency matrix}\label{fig:graph1adj}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Sparsity graph}\label{fig:graph1pic}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=yellow!20] (v3) {3};
\node at (180:1) [fill=purple!20] (v4) {4};
\node at (240:1) [fill=brown!20] (v5) {5};
\node at (300:1) [fill=white!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v6)
(v2) -- (v6)
(v3) -- (v6)
(v4) -- (v6)
(v5) -- (v6)
(v1) -- (v5)
(v2) -- (v5)
(v3) -- (v5)
(v4) -- (v5)
(v1) -- (v4)
(v2) -- (v4)
(v3) -- (v4)
(v1) -- (v3)
(v2) -- (v3)
(v1) -- (v2);
\end{tikzpicture}
}{} %% end toggle
\caption{Intersection graph}\label{fig:graph1int}
\end{subfigure}
\caption{Unpermuted matrix}\label{fig:graph1}
\end{figure}

Now suppose that we were to rearrange $\hess{}{x}$ so the last row and
and column were moved to the front.  The sparsity graph of the
Hessian is unchanged, but the lower triangular adjacency matrix and
the corresponding intersection graph, are different (Figure
\ref{fig:graph2}).  In Figure \ref{fig:graph2adj}, all columns share at least one non-zero row with
the column for variable 7, but variable groups $\{2,4,6\}$ and
$\{1,3,5\}$ have empty intersection sets.  The intersection graph in
Figure \ref{fig:graph2pic} has many fewer edges, and can be colored
with only three colors.

\begin{figure}
  \begin{subfigure}[b]{.33\textwidth}\centering
  \begin{tabular}{r|ccccccc}
   & 7&1&2&3&4&5&6\\
    \hline
    7& 1&&&&&&\\
    1&1&1&&&&&\\
    2&1&1&1&&&&\\
    3&1&0&0&1&&&\\
    4&1&0&0&1&1&&\\
    5&1&0&0&0&0&1&\\
    6&1&0&0&0&0&1&1
  \end{tabular}
  \caption{Adjacency matrix}\label{fig:graph2adj}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Sparsity graph}\label{fig:graph2pic}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Intersection graph}\label{fig:graph2int}
\end{subfigure}
\caption{Permuted matrix}\label{fig:graph2}
\end{figure}

\subsection{Algorithm and Permutations}

The \pkg{sparseHessianFD} implements algorithms for the following tasks:

\begin{enumerate}
\item Permuting rows and columns of the matrix to reduce the number of
  colors;
\item Finding a cyclic coloring that is consistent with the
  substitution method;
\item Computing the compressed matrix of finite differences of
  gradients; and
\item Solving triangular linear system to estimate the non-zero
  elements of the Hessian.
\end{enumerate}

None of these algorithms are meant to be optimal, or even the ``best
available.''  However, they are meant to be easy to implement and
maintain with standard libraries.

\subsubsection{Permuting Hessian}

Finding an appropriate permutation of the Hessian is similar to, but
exactly the same as, finding a fill-reducing permutation for a
Cholesky decomposition.  Instead of reducing the total number of zeros
in the lower triangle, we want to reduce the number of columns with
common non-zero rows.  A reasonable heuristic is to permute the matrix
such that the number of non-zero elements in each row is decreasing.
This is the ``smallest-last'' ordering discussed in
\citet{ColemanMore1984}.  The reason for this order is simple.
Suppose non-zeros within a row are randomly distributed across
columns.  If the row is near the top of the matrix, there is a higher
probability that the non-zero element is in the upper triangle, not in
the lower.  We can put sparser rows near the bottom, since the cells
that are in the lower triangle are less likely to be non-zero.

\subsubsection{Cyclic coloring}


As discussed above, we can get a triangular coloring of the sparsity
graph with a proper coloring of the intersection graph of the
columns.  Let $\hessLT{\pi}{\cdot}$ be the lower triangle of the
sparsity pattern of the Hessian, with rows and columns permuted by
$\pi$.  The adjacency matrix of the intersection graph $\Gamma$ is the Boolean
crossproduct, $\Gamma = L^\top L$.


This is a ``greedy'' coloring algorithm on the permuted matrix.

\begin{algorithm}
  \begin{algorithmic}
    \FOR{$i=1$ \TO $M$}
        \STATE{$P[i]\leftarrow$ set of column indices of non-zero
      elements in row $i$}
       \STATE{Initialize $F[i]$ as set of  ``forbidden'' colors for vertex $i$.}
        \STATE{$C[i]\leftarrow 0$.}
    \ENDFOR
    \STATE{$k\leftarrow 0$}
    \STATE{Insert $0$ in $U$}
    \FOR{$i=1$ \TO $M$}
       \IF{$F[i]$ is empty}
           \STATE{$C[i]\leftarrow 0$}
        \ELSE
           \STATE{$V\leftarrow U\cap F[i]$}
           \IF{$V$ is empty}
              \STATE{$k\leftarrow k+1$}
              \STATE{Insert $k$ into $U$}
              \STATE{$C[i]\leftarrow k$}
        \ELSE
             \STATE{$C[i]\leftarrow\min(V)$}
       \ENDIF
    \ENDIF
    \ENDFOR
    \FOR{$j$ in $P[i]$}
    \STATE{Insert $C[i]$ into $F[j]$}
    \ENDFOR
    \RETURN{$C$}
    \end{algorithmic}
    \caption{Consistent partitioning of variables for a triangular
      substitution method}\label{alg:coloring}
    \end{algorithm}

\subsection{Implementing the substitution method}

Now we can fill in the matrix.  Let $C_m$ be the assigned color to
variable $m$.  From \citet[Equation 6.1]{ColemanMore1984},
\begin{align}
  \label{eq:2}
  \hess{ij}{x}&=Y_{i,C_j}/\delta - \sum_{l>i,l\in C_j}\hess{li}{x}
\end{align}

We implement this substitution method using Algorithm \ref{alg:subst}.
\begin{algorithm}
  \begin{algorithmic}
    \STATE{Initialize Hessian $H$.}
    \STATE{Initialize $B\leftarrow$ a $k\times M$ matrix of zeros}
    \FOR{$i$ = $M$ \TO $1$}
      \STATE{$P_i\leftarrow$ set of column indices of non-zero
      elements in row $i$}
    \FOR{All $j$ in $P_i$}
    \STATE{$z\leftarrow Y[i,C[j]]/\delta - B[C[j], i]$}
    \STATE{$B[C[i], j]\leftarrow B[C[i], j] + z$}
    \STATE{$H[i,j]\leftarrow z$}
    \STATE{$H[j,i]\leftarrow H[i,j]$}
    \ENDFOR
    \ENDFOR
  \end{algorithmic}
  \caption{Triangular substitution method}\label{alg:subst}
\end{algorithm}










\subsection{Partitioning the variables}





 \subsection{Note:  Who proved what}


 *** \Citet{ColemanMore1984} The ``no nonzero in same row'' rule for
the permuted lower triangular matrix is equivalent to the
\emph{intersection graph}.  Theorem 6.1 shows that a proper coloring of the
intersection graph of $L_\pi$ is also a triangular coloring of the
full symmetric graph.

Theorem 6.2 justifies the sequential smallest-last ordering.

This is what our algorithm does.




Consistent partitioning of $L$:  No column in the same group has a
non-zero element in the same row.

\Citet{PowellToint1979} showed that a consistent partitioning of $L$
allows for a substitution method (mentioned by
\citet{ColemanMore1984}.  This is a substitutable partition.

\Citet{PowellToint1979} show that the order in which variables are
assigned to partitions affects the partition.

\Citet{ColemanMore1984} characterize \cite{PowellToint1979} as a graph
coloring problem.  Also justify using the smallest-last ordering on the lower
triangle to color the variables



\Citet{ColemanCai1986}:  there are other substitutable partitions than
lower triangular.  Theorem 2.2.  A mapping induces a substitution
method if and only if the mapping is a cyclic coloring.

Cyclic coloring:  At least 3 colors in every cycle.

From \citet{ColemanCai1986}, general result of substitutable
(beyond \citet{PowellToint1979}.  Order nonzero elements
  $1\mathellipsis M$. If, for nonzero element $(i_m, j_m)$,


  \begin{enumerate}
\item  columns  $j_m$ and another other $j_{m'}$ are in the same
  group, and both $j_m$ and $j_{m'}$ have a nonzero in row $i_m$, then
  $i_{m'}, j_{m'}$ must be ordered before $i_m, j_m$; \emph{or}
\item  columns  $i_m$ and another other $i_{m'}$ are in the same
  group, and both $i_m$ and $i_{m'}$ have a nonzero in row $j_m$, then
  $j_{m'}, i_{m'}$ must be ordered before $j_m, i_m$
  \end{enumerate}

  What this means is that we can ignore column intersections that
  occur in lower rows (what?).

  The point is that lower triangular substitution qualifies, which
  means that we just need a cyclic coloring of the graph.


  Note:  My coloring algorithm is a smallest last ordering of the full
  symmetric Hessian  This is \emph{slpt} in \citet{ColemanMore1984}.
  What I should really do is a smallest-last on the lower triangle
  (\emph{slsl} in citet{ColemanMore1984}.  So I need to change that.

  In any event, it's still finding a cyclic coloring of the adjacency
  graph.  Just using a different heuristic for the coloring.

\printbibliography

\end{document}

\documentclass[nojss]{jss}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabularx}

\usepackage{colortbl}

\usepackage{etoolbox}
\newtoggle{tikz}
\togglefalse{tikz}

\iftoggle{tikz}{
\usepackage{tikz}
}{}



%%\newcommand{\pkg}[1]{\emph{#1}}
%%\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
%%\newcommand{\filename}[1]{\textit{#1}}
%%\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}


 \newcommand{\df}[3]{\mathsf{d}^{#1}f(#2;#3)}
 \newcommand{\parD}[3]{\mathsf{D}^{#1}_{#2}#3}
\newcommand{\hess}[2]{\mathsf{H}_{#1}f(#2)}
\newcommand{\hessLT}[2]{\mathsf{L}_{#1}f(#2)}
\newcommand{\Mat}[1]{#1}

\author{Michael Braun\\Edwin L. Cox School of Business\\Southern Methodist University}
%% \Plainauthor{Michael Braun}
%% \title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
%% \Shorttitle{sparseHessianFD:  Sparse Hessians}
%% %% \Abstract{
%%   Abstract goes here
%%   }

%%   \Keywords{sparse Hessians, sparse matrices}
%%   \Address{
%%     Michael Braun\\
%%     Edwin L. Cox School of Business\\
%%     Southern Methodist University\\
%%     6212 Bishop Blvd.\\
%%     Dallas, TX 75275\\
%%     E-mail:  \email{braunm@smu.edu}\\
%%     URL:  \url{http://coxprofs.cox.smu.edu/braunm/}
%%   }

  %% need no \usepackage{Sweave.sty}

\date{April 30, 2015}

\begin{document}

<<setup1, echo=FALSE, cache=FALSE>>=
library(methods, quietly = TRUE)
library(numDeriv, quietly = TRUE)
library(Matrix, quietly = TRUE)
library(sparseHessianFD, quietly=TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE,
                          message=FALSE, tidy=FALSE, cache = TRUE)
@

The Hessian matrix of a log likelihood function or log posterior
density function plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  More broadly, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $M$ variables has $M^2$
elements, of which $M(M+1)/2$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the number of
decision variables.   For
functions with hundreds of thousands of variables, computing the
Hessian even once might not be practical for applications constrained
by time, storage or processor availability.  Hierarchical models, in
which each additional heterogeneous unit is associated with its own subset of
variables, are particularly vulnerable to this curse of dimensionality

For many problems (hierarchical models among them), the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  Consider a log
posterior density in a Bayesian hierarchical
model.  If the outcomes across units are conditionally
independent, the cross-partial derivatives with respect to those
parameters are zero.  As the number of units
increases, the size of the Hessian still grows quadratically, but the number
of \emph{non-zero} elements grows only linearly; the Hessian
becomes increasingly sparse.  The row and column indices of the
non-zero elements comprise the \emph{sparsity pattern} of the Hessian,
and are typically known in advance, before computing the values of
those elements.

The \pkg{sparseHessianFD} package is a tool for estimating
sparse Hessians using \emph{finite differencing}.  Section \ref{sec:numdiff} will
cover the specifics, but the basic idea is as follows.  Consider a
function $f(x)$, its gradient $\parD{}{}{f(x)}^\top$ (the transpose of the
derivative), and its Hessian $\hess{}{x}$.  Let $e_m$ be the $m$th
coordinate vector, and let $\delta$ be a sufficiently small scalar
constant. The vector $\hess{m}{x}\approx\left(\parD{}{}{f(x+\delta e_m)}- \parD{}{}{f(x)}\right)/\delta$ is
a linear approximation to $\hess{m}{x}$, the $m$th column of the
Hessian. Estimating a dense Hessian in this way involves $M+1$ calculations of the
gradient: one for the gradient at $x$, and one after perturbing each
of the $M$ elements of $x$, one at a time.  However, if the Hessian has
a sparsity pattern that allows it, we could perturb
more than one element of $x$ at a time, evaluate the gradient fewer
than $M+1$ times, and still recover the non-zero
Hessian values.  For some sparsity patterns, estimating a Hessian in
this way can be profoundly efficient.  In fact, for the hierarchical models that we consider in
this paper, the number of gradient evaluations is \emph{constant},
even as additional units are added to the model.  How to
decide which variables can be perturbed together is actually a graph
coloring problem, which we discuss in Section \ref{sec:coloring}.

At the outset, we want to mention that there may be some applications for
which \pkg{sparseHessianFD} is not an appropriate package to use. To
extract the maximum benefit from using \pkg{sparseHessianFD}, we need
to accept a few conditions or assumptions.

\begin{enumerate}
  \item Preferred alternatives to computing the Hessian are not
  available.  Finite differencing is not generally a ``first choice''
  method.  Deriving a gradient or Hessian symbolically, and writing a
  subroutine to compute it, will give an exact answer, but might be
  tedious or difficult to implement. Algorithmic differentiation (AD) is
  probably the most efficient method, but requires specialized
  libraries that, at this moment, are not yet broadly available in \proglang{R}.
 \pkg{sparseHessianFD} makes the most sense when the gradient is easy to compute, but the Hessian is
  not.
  \item The application can tolerate the approximation error in the Hessian
  that comes with finite differencing methods.
\item The objective function $f(x)$ is  twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in a function that itself is hard to compute.
\item The gradient can
  be computed quickly, easily and \emph{exactly} (within machine
  precision).   We do not recommend using finite differenced gradients
  when computing finite differenced Hessians, for two reasons.  First,
  the approximation errors will be compounded.  Second, the time
  complexity of computing a gradient grows with the number of
  variables when using finite differencing, but not with other methods
  like AD \citep[p. xii]{GriewankWalther2008}.
\item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.
\end{enumerate}

Some users may find the requirement to provide a function that
computes the gradient to be burdensome.  We take the position that deriving a vector of first
derivatives, and writing \proglang{R} functions to compute them, is a
lot easier than doing the same for a matrix of second derivatives.
Even when we have derived and
coded the
Hessian matrix symbolically, in practice it may still be faster to estimate the Hessian
using \pkg{sparseHessianFD} than coding it directly.   These are the
situations in which \pkg{sparseHessianFD} adds the most value.  If AD software is available to compute the gradient, then
it is probably available for sparse Hessians as well, and
\pkg{sparseHessianFD} would not be needed.

The rest of this article proceeds as follows.  In Section
\ref{sec:background}, we introduce some fundamental material on
sparsity patterns, numerical differentiation, and graph theory.  In Section \ref{sec:using}, we demonstrate how
to use the package, and summarize the underlying algorithms.  Section \ref{sec:timing} includes some time and
accuracy tests.

\section{Background}\label{sec:background}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.  Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities, and let $p_i$ be the probability of
 purchase.  The heterogeneous parameter $p_i$ is the same for all $T$
 opportunities, so $y_i$ is a binomial random variable.  Define each $p_i$ such that it depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$.
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients are distributed across the population of households
following a multivariate normal distribution with mean $\mu$ and
covariance $\Sigma$.   Assume that we know $\Sigma$, but not $\mu$.
Instead, place a multivariate normal prior on $\mu$, with mean $0$ and
covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are
$k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}
We will return to this example throughout the article.

\subsection{Sparsity patterns}\label{sec:sparsity}

<<setup2, echo=FALSE>>=
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

The log posterior density in Equation \ref{eq:LPD} has a sparse
Hessian.  Since the $\beta_i$ are drawn iid from a multivariate
normal, and the $y_i$ are conditionally independent,
$\hess{\beta_i,\beta_j}{}=\parD{2}{\beta_i, \beta_j}{\log\pi}=0$ for all $i\neq
j$.  However, all of the $\beta_i$ are correlated with
$\mu$.  Thus, $\hess{\beta_i,\mu_k}{}\neq 0$ for all $i$.

The sparsity pattern depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=\Sexpr{N}$ and $k=\Sexpr{k}$, then there are `\Sexpr{nv1}` total
variables, and the Hessian will have the pattern in Figure \ref{fig:blockarrow}..



Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "banded" sparsity pattern, as in Figure \ref{fig:banded}.

\begin{figure}[htb]
  \begin{subfigure}[b]{.5\textwidth}
<<blockarrow, echo=FALSE>>=
Mat <- as(Matrix::kronecker(Matrix::Diagonal(N),Matrix(1,k,k)),"nMatrix")
Mat <- rBind(Mat, Matrix(TRUE,k,N*k))
Mat <- cBind(Mat, Matrix(TRUE, k*(N+1), k))
print(Mat)
@
\caption{A ``block-arrow sparsity pattern''}\label{fig:blockarrow}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}
<<banded, echo=FALSE>>=
Mat <- as(Matrix::kronecker(Matrix(1,k,k), Matrix::Diagonal(N)),"nMatrix")
Mat <- rBind(Mat, Matrix(TRUE,k,N*k))
Mat <- cBind(Mat, Matrix(TRUE, k*(N+1), k))
print(Mat)
@
\caption{A ``banded'' sparsity pattern}\label{fig:banded}
\end{subfigure}
\caption{Two examples of sparsity patterns for a hierarchical model}\label{fig:patterns}
\end{figure}

In both cases, the number of non-zeros is the same.   There are \Sexpr{nels1} elements in this symmetric matrix, but only \Sexpr{nnz1} are
non-zero, and only \Sexpr{nnz1LT} values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=\Sexpr{Q}$ instead.  In that case,
there are \Sexpr{nv2} variables in the problem, and more than $\Sexpr{floor(nels2/10^6)}$ million
elements in the Hessian.  However, only $\Sexpr{nnz2}$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only \Sexpr{nnz2LT} values.


\subsection{Sparse matrices in R}

The \pkg{sparseHessianFD} package uses the sparse matrix classes that are defined
in the \pkg{Matrix} package \citep{R_Matrix}.  \pkg{Matrix} is now a
recommended \proglang{R} package, with a great deal of functionality
and flexibility. We refer the reader to that class's documentation for
all of the details, but we want to highlight a few important points.


\subsubsection{Class names and inheritance}

All sparse matrix classes in \pkg{Matrix} are subclasses of
\class{sparseMatrix}.  Names of classes of sparse matrices, summarized in Table
\ref{tab:MatrixClasses}, depend on the data type, matrix structure,
storage format.  The levels of these three factors determines the
prefix of letters in each class name. For example, a triangular sparse matrix of numeric data, stored in
column-compressed format, has a class \class{dtCMatrix}.

\begin{table}[htb]
  \centering
  \begin{tabular}{ll|lll}
   Storage&Layout&\multicolumn{3}{c}{Data type}\\
    &&numeric&logical&pattern\\
    \hline
    Triplet&general&\class{dgTMatrix}&\class{lgTMatrix}&\class{ngTMatrix}\\
    &triangular&\class{dtTMatrix}&\class{ltTMatrix}&\class{ntTMatrix}\\
      &symmetric&\class{dsTMatrix}&\class{lsTMatrix}&\class{nsTMatrix}\\
    \hline
        Row-compressed&general&\class{dgRMatrix}&\class{lgRMatrix}&\class{ngRMatrix}\\
    &triangular&\class{dtRMatrix}&\class{ltRMatrix}&\class{ntRMatrix}\\
      &symmetric&\class{dsRMatrix}&\class{lsRMatrix}&\class{nsRMatrix}\\
    \hline
            Column-compressed&general&\class{dgCMatrix}&\class{lgCMatrix}&\class{ngCMatrix}\\
    &triangular&\class{dtCMatrix}&\class{ltCMatrix}&\class{ntCMatrix}\\
      &symmetric&\class{dsCMatrix}&\class{lsCMatrix}&\class{nsCMatrix}
  \end{tabular}
  \caption{Class names for sparse matrices, as defined in the
    \pkg{Matrix} package.}\label{tab:MatrixClasses}
\end{table}

In a matrix with a ``general'' layout, all non-zero elements are
stored.  For symmetric matrices, only the elements in one triangle are
stored.  Symmetric or triangular matrices take advantage of
optimized mathematical operations that take advantage of their known
structure.  Values in numeric and logical matrices correspond to the
\proglang{R} data types of the same names.  Pattern matrices contain
row and column information for the non-zero elements, but no values.

Diagonal matrices with classes
\class{ddiMatrix} and \class{ldiMatrix} can be constructed with
the \func{Diagonal} function.  \pkg{Matrix} also defines several
classes of dense matrices that we will not discuss here.


\subsubsection{Storage formats}

Matrices with a ``triplet'' format store each non-zero element as a
(row, column, value) set.  The values are either numeric or logical,
for \class{d*TMatrix} or \class{l*TMatrix} matrices, respectively.
Pattern matrices of class \class{n*TMatrix} store only row and column
indices, but no values at all.

Row-compressed (CSR) and column-compressed (CSC) formats are more
efficient alternatives.  Instead of storing the row and column indices, CSC
matrices store only the row indices, and ``pointers''
to the values in the row index and value vectors that start
each column\footnote{For the CSR format, switch references to columns and rows
in the preceding sentence.}.

For example, let's construct a numeric matrix in triplet form using
the \func{sparseMatrix} function. The arguments \funcarg{i},
\funcarg{j} and \funcarg{x} are the row and column indices, and
values, respectively, of the non-zero elements.  By default, \func{sparseMatrix} expects the input vectors \funcarg{i} and \funcarg{j} to
have one-based indexing, and stores $X$ in CSC format.
<<trip>>=
A <- sparseMatrix(i=c(1,2,3,4,3,4), j=c(1,2,2,2,3,4), x=c(6,7,8,9,10,11))
A
str(A)
@

The slot \funcarg{x} contains the matrix values exactly as we entered
them.  Slot \funcarg{i} stores the row indices of those values, and
slot \funcarg{p} stores column pointers.  The column indices are not
stored explicitly.  To reduce confusion,
let's just print the values, row indices and pointer values with one-based indexing.

<<indexing>>=
print(x <- A@x)
print(i <- A@i+1)
print(p <- A@p+1)
@

Let's go through the values column-by-column, starting with column 1.
Pointer element $p_1$ is 1, so the first element in column 1 is in row
$i_1=1$, with value $x_1=6$.  Thus, $A_{11}=6$.  Since $p_2$ is 2,
column 2 begins with $x_2=7$, at row $i_2=2$, so $A_{22}=7$.  Column 3
does not start until element $p_3=5$ in $i$ and $x$, so the next two non-zeros, in
rows $i_3=3$ and $i_4=4$, must still be in column 2.  That means that
$A_{32}=8$ and $A_{42}=9$.  Now we move to column 3, where
$A_{33}=10$, and to column 4, where $A_{44}=11$.  The final value in
$p$ is the number of non-zeros in the matrix.  The differences between
adjacent elements in $p$ are the number of non-zeros in each column of $A$.

There are two important advantages to storing sparse matrices in a
compressed format.  First, the storage requirements are usually smaller.  A
matrix with $M$ columns and $N$ non-zeros requires $3N$ elements in a
triplet format, but only $2N+M+1$ elements in a compressed format.
Second, computation on the matrix is more efficient, since all elements in a
single column (CSC) or row (CSR) are stored in contiguous memory, in
order.  In contrast, there is no unique storage order for the triplet format.


\subsubsection{Conversion of sparse matrices}

The \pkg{Matrix} package uses the \func{as} function to convert sparse
matrices from one format to another, and to convert a \pkg{base}
\proglang{R} matrix to one of the \pkg{Matrix} classes.

In addition,, this package defines helper functions to convert
between sparse matrices, and the vectors of row and column indices
required by the \class{sparseHessianFD} initializer (see Section
\ref{sec:using}).  These functions are summarized in Table \ref{tab:helpers}.

\begin{table}[tbh]
  \centering
  \setlength\extrarowheight{.25em}
  \begin{tabularx}{1.0\linewidth}{lX}
   \func{Matrix.to.Coord} & Returns a list of vectors containing row
                            and column indices of the non-zero
                            elements of a matrix.\\
    \func{Coord.to.Pattern.Matrix}& Converts vectors of row and column
                                    indices to a sparse pattern matrix
                                    (useful for visualizing a sparsity pattern).\\
    \func{Matrix.to.Pointers}&Returns indices and pointers from a
                               sparse matrix.\\
    \func{Coord.to.Pointers}&Converts list of row and column indices
                              (triplet format) to a list of indices
                              and pointers (compressed format).
  \end{tabularx}
  \caption{\class{sparseHessianFD} Matrix conversion functions}
  \label{tab:helpers}
\end{table}



\subsubsection{Matrix operations}

The \pkg{Matrix} package defines some specialized methods for objects
of \class{sparseMatrix} subclasses.  Many of these methods have the
same name as their \pkg{base} \proglang{R} counterparts, such as
\func{t()} and \func{rowSums()}.  In some
cases, it may be necessary to prefix functions with \func{Matrix::} to
ensure that the correct version is called.




\subsection{Numerical differentiation}\label{sec:numdiff}

The partial derivative of a scalar-valued function $f(x)$ with respect to $x_j$ (the $j$th
component of $x$) is defined as
\begin{align}
  \label{eq:defParD}
\parD{}{j}{f(x)}=\lim\limits_{\delta\to 0}\frac{f(x+\delta e_j)-f(x)}{\delta}
\end{align}
For a sufficiently small $\delta$, this definition allows for a
linear approximation to $\parD{}{j}{f(x)}$.  The derivative of $f(x)$
is the vector of all $M$ partial derivatives.

\begin{align}
  \parD{}{}{f(x)}=\left(\parD{}{1}{f(x)},\mathellipsis,\parD{}{M}{f(x)}\right)
  \end{align}
 The \emph{gradient} is defined as $\nabla f(x)=\parD{}{}{f(x)}^\top$.

We define the second-order partial derivative as
\begin{align}
  \label{eq:14}
  \parD{2}{jk}{f(x)}=\lim\limits_{\delta\to 0}\frac{\parD{}{j}{f(x+\delta e_k)}-\parD{}{j}{f(x)}}{\delta}
\end{align}
and the Hessian matrix as
\begin{align}
  \label{eq:15}
  \hess{}{x}=
  \begin{pmatrix}
    \parD{2}{11}{f(x)}&  \parD{2}{12}{f(x)}&  \mathellipsis &  \parD{2}{1M}{f(x)}\\
    \parD{2}{21}{f(x)}&  \parD{2}{22}{f(x)}&  \mathellipsis &  \parD{2}{2M}{f(x)}\\
    \vdots&\vdots&&\vdots\\
    \parD{2}{M1}{f(x)}&  \parD{2}{M2}{f(x)}&  \mathellipsis &  \parD{2}{MM}{f(x)}
    \end{pmatrix}
\end{align}

The Hessian is symmetric, so $\parD{2}{ij}{}=\parD{2}{ji}{}$.

To estimate the $k$th column of $\hess{}{x}$, we again choose a
sufficiently small $\delta$, and compute
\begin{align}
  \label{eq:1}
  \hess{k}{x}&\approx\frac{\parD{}{}{f(x+\delta e_k)}-\parD{}{}{f(x)}}{\delta}
\end{align}


For $M=2$, our estimate of a general $\hess{}{x}$ would be
\begin{align}
  \label{eq:FDhess2}
  \hess{}{x}&=
  \begin{pmatrix}
    \parD{}{1}{f(x_1+\delta, x_2)}-\parD{}{1}{f(x_1,x_2)}& \parD{}{1}{f(x_1,x_2+\delta)}-\parD{}{1}{f(x_1,x_2)}\\
        \parD{}{2}{f(x_1+\delta, x_2)}-\parD{}{2}{f(x_1,x_2)}&  \parD{}{2}{f(x_1,x_2+\delta)}-\parD{}{2}{f(x_1,x_2)}
    \end{pmatrix}/\delta
\end{align}

This estimate requires three evaluations of the gradient to get
$\parD{}{}{f(x_1,x_2)}$, $\parD{}{}{f(x_1+\delta,x_2)}$, and
$\parD{}{}{f(x_1,x_2+\delta)}$.

Now suppose that the Hessian is sparse, and that the off-diagonal
elements are zero.  Not only are
\begin{align}
  \label{eq:3}
  \parD{}{1}{f(x_1,x_2+\delta)}-\parD{}{1}{f(x_1,x_2)}&=0\\
    \parD{}{2}{f(x_1+\delta,x_2)}-\parD{}{2}{f(x_1,x_2)}&=0
\end{align},
but also,
\begin{align}
  \label{eq:3a}
  \parD{}{1}{f(x_1+\delta,x_2+\delta)}-\parD{}{1}{f(x_1+\delta,x_2)}&=0\\
    \parD{}{2}{f(x_1+\delta,x_2+\delta)}-\parD{}{2}{f(x_1,x_2+\delta)}&=0
\end{align}
Therefore,
\begin{align}
  \label{eq:FDhess2sp}
  \hess{}{x}&=
  \begin{pmatrix}
    \parD{}{1}{f(x_1+\delta, x_2+\delta)}-\parD{}{1}{f(x_1,x_2)}&0\\
        0&  \parD{}{2}{f(x_1+\delta,x_2+\delta)}-\parD{}{2}{f(x_1,x_2)}
    \end{pmatrix}/\delta
\end{align}

This estimate requires only two evaluations of the gradient to get
$\parD{}{}{f(x_1,x_2)}$ and $\parD{}{}{f(x_1+\delta,x_2+\delta)}$.
Being able to reduce the number of gradient evaluations from 3 to 2 depends on knowing that
the cross-partial derivatives are zero.

The ``trick'' for the general case of sparse Hessians is to partition
the decision variables into $C$ mutually exclusive groups in such as way that the number of gradient
evaluations is minimized.  Let $\Mat{G}$ be a $M\times C$
matrix, where $\Mat{G}_{mc}=\delta$ if variable $m$ belongs to group $c$, and zero
otherwise.  Define $G_c$ as the $c$th column of $G$.

Next, let $\Mat{Y}$ be a $M\times C$ matrix, where each column is a
difference in gradients.
\begin{align}
  \label{eq:Yg}
  Y_c&=\nabla f(x+G_c)-\nabla f(x)
\end{align}

If $C=K$, then $\Mat{G}$ is a diagonal matrix with $\delta$ in each
diagonal element.  The matrix equation $\hess{}{x}G=Y$ represents the linear approximation
$\hess{im}{x}\delta\approx y_{im}$, and we can solve for all elements of $\hess{}{x}$
just by computing $Y$. But if $C<M$,
there must be at least one $G_c$ with $\delta$ in at least two
rows. Column $Y_c$ would have been computed by perturbing two variables at
once, and we would not have been able to solve for any $\hess{im}{x}$
without further constraints.

These constraints come from the sparsity pattern and symmetry
of the Hessian. Consider an example with the following values and
sparsity pattern.

\begin{align}
  \label{eq:7}
 \hess{}{x}= \begin{pmatrix}
    h_{11}&0&h_{31}&0&0\\
    0&h_{22}&0&h_{42}&0\\
    h_{31}&0&h_{33}&0&h_{53}\\
    0&h_{42}&0&h_{44}&0\\
    0&0&h_{53}&0&h_{55}
  \end{pmatrix}
\end{align}

Suppose $C=2$, and define the colors of the five variables through the following $\Mat{G}$ matrix.
\begin{align}
  \label{eq:7}
 \Mat{G}^\top= \begin{pmatrix}
   \delta&\delta&0&0&\delta\\
   0&0&\delta&\delta&0
  \end{pmatrix}
\end{align}
Variables 1 and 2 are in group 1, and variables 3, 4 and 5 are in
group 2.  For the moment, we will postpone the discussion of how to
choose $C$ and how to partition the variables until Section \ref{sec:coloring}.

Next, compute the columns of $\Mat{Y}$ using Equation \ref{eq:Yg}.  We now
have the following system of linear equations from $\hess{}{x}\Mat{G}=\Mat{Y}$.
\begin{align}
  \label{eq:11}
  \begin{aligned}[c]
  h_{11}&=y_{11}\\
  h_{22}&=y_{21}\\
  h_{31}+h_{53}&=y_{31}\\
  h_{42}&=y_{41}\\
  h_{55}&=y_{51}\\
  \end{aligned}
  \qquad
  \begin{aligned}[c]
  h_{31}&=y_{12}\\
  h_{42}&=y_{22}\\
  h_{33}&=y_{32}\\
  h_{44}&=y_{42}\\
  h_{53}&=y_{52}
  \end{aligned}
\end{align}

Note that this system is overdetermined.  Both $h_{31}=y_{12}$ and $h_{53}=y_{52}$
can be determined directly, but $h_{31}+h_{53}=y_{31}$, and $h_{42}$
could be either $y_{41}$ or $y_{22}$.  \Citet{PowellToint1979} prove
that it is sufficient to solve $\hessLT{}{x}G=Y$ instead via a \emph{substitution method}, where $\hessLT{}{x}$ is the lower
triangular part of $\hess{}{x}$.  This has the effect of removing the equations
$h_{42}=y_{22}$ and $h_{31}=y_{12}$ from the system, but retaining
$h_{53}=y_{52}$.  We can then solve for
$h_{31}=y_{31}-y_{52}$. Thus, we have determined a
$5\times 5$ Hessian matrix with only three gradient evaluations, in
contrast with the six that would have been needed had $\hess{}{x}$ been treated
as dense.

The $\Mat{Y}$ matrix is, in essence, a compressed representation of
$\hess{}{x}$.  The values in $Y_c$ are finite differences after
one \emph{or more} variables are perturbed.  Hence, we need two
different algorithms to estimate a sparse Hessian.  The first is to
determine which variables can be perturbed together when computing
$\Mat{Y}$, and the second is to extract the values of $\hess{}{x}$
from $\Mat{Y}$.  We discuss these algorithms next.


\subsection{Partitioning the variables}\label{sec:coloring}

It turns out that finding consistent, efficient partitions can be
characterized as a vertex coloring problem from graph theory \citep{ColemanMore1984}.  In this
sense, each variable is a vertex in an undirected graph, and an edge connects two
vertices $i$ and $j$ if and only if $\hess{ij}{x}\neq 0$.  The
sparsity pattern of the Hessian is the adjacency matrix of the graph.
By ``color,'' we mean nothing more than group assignment; if a
variable is in a group, then its vertex has the color associated with
that group.  A ``proper'' coloring of a graph is one in which two
vertices with a common edge do not have the same
color. \Citet{ColemanMore1984} define a ``triangular coloring'' of a graph is
a proper coloring with the additional condition that common
neighbors of a vertex do not have the same color.  A triangular
coloring is a special case of an ``cyclic coloring,'' in which any cycle in the graph
uses at least three colors \citep{GebremedhinTarafdar2007}.

We can also assign
to each vertex a set of other characteristics.  The ``intersection
set'' contains characteristics that are common to two vertices, and
the ``intersection graph'' connects vertices whose intersection set is
not empty.  In our context, the set in question is the row indices of
the non-zero elements in each column of $\hessLT{}{x}$.  In the
intersection graph, two vertices are connected if the corresponding
columns in $\hessLT{}{x}$ have at least one non-zero element in the same row.

\Citet{PowellToint1979} write that a partitioning is consistent with a
substitution method if and only if no columns
of the of lower triangle of the Hessian that are
in the same group have a non-zero element in the same
row.  An equivalent statement is that no two vertices in the
intersection graph can have the same color.  Thus, we can partition
the variables by creating a proper coloring of the intersection graph
of $\hessLT{}{x}$.

This intersection graph, and the number of colors needed to
color it, are not invariant to
permutation of the rows and columns of $\hess{}{x}$.  Let $\pi$
represent such a permutation, and let $\hessLT{\pi}{x}$ be the lower
triangle of $\pi\hess{}{x}{\pi^\top}$.    \Citet[Theorem 6.1]{ColemanMore1984} show that a coloring is
triangular if and only if it is also a coloring of the intersection
graph of $\hessLT{\pi}{x}$ for some permutation $\pi$.  Furthermore, \citet{ColemanCai1986} prove that a partitioning is consistent with a
substitution method if and only if it is an acyclic coloring of the
graph of the sparsity pattern of the Hessian.   Therefore,
finding an optimal partitioning of the variables involves finding an
optimal combination of a permutation $\pi$, and coloring algorithm
for the intersection graph of $\hessLT{\pi}{x}$.

These ideas are illustrated in Figures \ref{fig:graph1} and
\ref{fig:graph2}.  Figure
\ref{fig:graph1adj} shows the sparsity pattern of the lower triangle of a
Hessian as an adjacency matrix, and Figure \ref{fig:graph1pic} is the
associated graph with a proper vertex coloring.  We can deduce from
Figure \ref{fig:graph1adj} by noting that
every column (and thus, every pair of columns) has a non-zero element
in row 7.  There are no non-empty intersection sets across the
columns.  All vertices are connected to each other in the
intersection graph (Figure \ref{fig:graph1int}), which requires seven colors for a proper coloring.  Estimating a
sparse Hessian with this partitioning scheme would be no more
efficient than if treating the Hessian as if it were dense.

\begin{figure}
  \begin{subfigure}[b]{.33\textwidth}\centering
  \begin{tabular}{r|ccccccc}
   & 1&2&3&4&5&6&7\\
    \hline
    1& 1&&&&&&\\
    2&1&1&&&&&\\
    3&0&0&1&&&&\\
    4&0&0&1&1&&&\\
    5&0&0&0&0&1&&\\
    6&1&0&0&0&1&1&\\
    7&1&1&1&1&1&1&1
  \end{tabular}
  \caption{Adjacency matrix}\label{fig:graph1adj}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Sparsity graph}\label{fig:graph1pic}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=yellow!20] (v3) {3};
\node at (180:1) [fill=purple!20] (v4) {4};
\node at (240:1) [fill=brown!20] (v5) {5};
\node at (300:1) [fill=white!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v6)
(v2) -- (v6)
(v3) -- (v6)
(v4) -- (v6)
(v5) -- (v6)
(v1) -- (v5)
(v2) -- (v5)
(v3) -- (v5)
(v4) -- (v5)
(v1) -- (v4)
(v2) -- (v4)
(v3) -- (v4)
(v1) -- (v3)
(v2) -- (v3)
(v1) -- (v2);
\end{tikzpicture}
}{} %% end toggle
\caption{Intersection graph}\label{fig:graph1int}
\end{subfigure}
\caption{Unpermuted matrix}\label{fig:graph1}
\end{figure}

Now suppose that we were to rearrange $\hess{}{x}$ so the last row and
and column were moved to the front.  The variables are reordered, but the meanings of the cross-partial
derivatives have not changed.  However, as we see in Figure
\ref{fig:graph2}, the lower triangular adjacency matrix and
the corresponding intersection graph are different from Figure \ref{fig:graph2} .  In Figure \ref{fig:graph2adj}, all columns share at least one non-zero row with
the column for variable 7, but variable groups $\{2,4,6\}$ and
$\{1,3,5\}$ have empty intersection sets.  The intersection graph in
Figure \ref{fig:graph2pic} has many fewer edges, and can be colored
with only three colors.

\begin{figure}
  \begin{subfigure}[b]{.33\textwidth}\centering
  \begin{tabular}{r|ccccccc}
   & 7&1&2&3&4&5&6\\
    \hline
    7& 1&&&&&&\\
    1&1&1&&&&&\\
    2&1&1&1&&&&\\
    3&1&0&0&1&&&\\
    4&1&0&0&1&1&&\\
    5&1&0&0&0&0&1&\\
    6&1&0&0&0&0&1&1
  \end{tabular}
  \caption{Adjacency matrix}\label{fig:graph2adj}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Sparsity graph}\label{fig:graph2pic}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Intersection graph}\label{fig:graph2int}
\end{subfigure}
\caption{Permuted matrix}\label{fig:graph2}
\end{figure}


The specifics of how to permute and partition the variables, and how
to implement the substitution method to compute the Hessian, should be
invisible to the user of the \pkg{sparseHessianFD} package.  These are
all discussed in Section \ref{sec:algorithms}, which should be
considered optional reading.




\section{Using the package}\label{sec:using}

In this section we discuss how to use \pkg{sparseHessianFD} to
estimate sparse Hessians.

\subsection{The sparseHessianFD class}

The function \func{sparseHessianFD} is an initializer that returns a reference to a
\class{sparseHessianFD} object.  The initializer determines an
appropriate permutation and partitioning
of the variables, and performs some additional validation tests.  The arguments to the initializer are
described in Table \ref{tab:init}.

{
\setlength{\extrarowheight}{.25em}
\begin{table}[h]
\begin{tabularx}{\linewidth}{>{\bfseries}r X}
x& A numeric vector, with length $M$ at which the object will be
  initialized and tested.\\
fn,gr& \proglang{R} Functions that return the value of the
  objective function, and its gradient. The first argument is the numeric
  variable vector.  Other named arguments can be passed to \func{fn}
  and {gr} as well (see the \funcarg{...} argument below).\\
rows, cols& Integer vectors of the row and column indices of
  the non-zero elements in the \emph{lower triangle} of the Hessian.\\
direct& This argument is deprecated, and is included only for
  backwards compatibility with earlier versions.\\
eps& The perturbation amount for finite differencing of the
  gradient to compute the Hessian (the $\delta$ in Section \ref{sec:numdiff}).  Defaults to
  \code{sqrt(.Machine\$double.eps)}.\\
index1& If \variable{TRUE} (the default), \funcarg{row} and \funcarg{col} use one-based
  indexing.  If \variable{FALSE}, zero-based indexing is used.\\
...& Additional arguments to be passed to \func{fn} and \func{gr}.
\end{tabularx}
\caption{Arguments to the \class{sparseHessianFD} initializer}\label{tab:init}
\end{table}
}

To create a \class{sparseHessianFD} object, just call
\func{sparseHessianFD}.  If you are accepting all of the default
arguments, the call will look like:

<<eval=FALSE>>=
obj <- sparseHessianFD(x, fn, gr, rows, cols, ...)
@
where \funcarg{...} represents all other named arguments that are
passed to \funcarg{fn} and \funcarg{gr}.

The class defines a number of different fields, none of which should
be accessed directly.


\subsection{Providing the sparsity pattern}

The sparsity pattern of the Hessian is defined as the row and column
indices of the non-zero elements in the \emph{lower triangle} the
Hessian.  It is the responsibility of the user to ensure that
the sparsity pattern is correct.

In practice, rather than trying to keep track of the row and column indices
directly, it might be easier to construct a pattern matrix first,
check visually that the matrix has the right pattern, and then extract
the indices.  The \func{Matrix.to.Coord} function extracts
row and column indices from a sparse matrix.  The input matrix to
\func{Matrix.to.Coord} does not have to include
the values.  It is sufficient to supply a logical
or pattern matrix, such as \class{lgCMatrix} or \class{ngCMatrix}.


The following code constructs a block diagonal matrix, and extracts
the sparsity pattern from its lower triangle.

<<exPattern>>=
Mat <- as(kronecker(Diagonal(3), Matrix(T,2,2)),"nMatrix")
Mat
tril(Mat)
mc <- Matrix.to.Coord(tril(Mat))
mc
@

The vectors \funcarg{mc\$row} and \funcarg{mc\$col} can be
passed to \func{sparseHessianFD} as the \funcarg{row} and
\funcarg{col} arguments, respectively.

To visually check that a proposed sparsity pattern represents the
intended matrix, use the \func{Coord.to.Pattern.Matrix} function,
which is a wrapper to \pkg{Matrix}'s \func{sparseMatrix} constructor.

<<>>=
pattern <- Coord.to.Pattern.Matrix(mc$rows, mc$cols, dims=dim(Mat))
pattern
@



\subsection{Evaluating the Hessian}

The \func{fn}, \func{gr} and \func{hessian} methods respectively evaluate the
function, gradient and Hessian at a variable vector $x$.

<<eval=FALSE>>=
f <- obj$fn(x)
df <- obj$gr(x)
hess <- obj$hessian(x) #$
@



The \func{fn} and \func{gr} methods call the same functions that were
provided to the class initializer.  Since the
additional arguments were already supplied to the initializer as \funcarg{...}, they do
not need to be supplied again.  This feature simplifies calls
to \func{fn} and \func{gr}, because only the variable is
included in the call.

Similarly, the \func{hessian} method takes a single argument.
The return value is always a \class{dgCMatrix} object (defined in the
\pkg{Matrix} package).  \class{dgCMatrix} objects are sparse matrices,
stored in a compressed, column-oriented format, and includes all
non-zero elements in both the upper and lower triangles.

The \func{fngr} method returns the function and gradient as a list.
The \func{fngrhs} includes the Hessian as well.


\subsection{An example}

Now we can use \pkg{sparseHessianFD} to estimate the Hessian for the
log posterior density of the model from Section \ref{sec:sparsity}.
The package includes functions that compute the value
(\func{binary.f}), the gradient (\func{binary.grad}) and the Hessian
{\func{binary.hess}.  The result from \func{binary.hess} is a
  ``true'' value against which we will compare the estimates from
  \class{sparseHessianFD}.  The package also includes sample datasets of different sizes, which
are access with the \func{data} function.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function. The \func{binary.f} and \func{binary.grad} functions take the data and
priors as lists.  The \func{data()} call adds the appropriate data
list to the environment, but we need to construct the prior list ourselves.

<<binaryInit>>=
library(sparseHessianFD)
set.seed(123)
data(binary_small)
binary <- binary_small
str(binary)
N <- length(binary[["Y"]])
k <- NROW(binary[["X"]])
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
nvars
@

<<echo=FALSE>>=
options(scipen=-999)
@

This dataset represents the simulated choices for $N= \Sexpr{N}$ customers
over $T= \Sexpr{T}$ purchase opportunties, where the probability of purchase
is influenced by $k= \Sexpr{k}$ covariates.

The next code chunk evaluates the ``true'' value, gradient and
Hessian.  The \funcarg{order.row} argument tells the function whether
the variables are ordered by household (\variable{TRUE}) or by covariate
(\variable{FALSE}). If \funcarg{order.row} is \variable{TRUE}, then the Hessian
will have an off-diagonal pattern.  If \funcarg{order.row} is \variable{FALSE}, then the
Hessian will have a block-arrow pattern.

<<trueValues>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two integer
vectors:  one each for the row and column indices of the non-zero
elements of the lower triangule of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the
\func{Matrix.to.Coord} function to extract the appropriate index
vectors.  If not, you need to determine the row and column indices manually.  If
the model is hierarchical, you could use the method of constructing
the matrices in \ref{sec:sparsity}.

<<binaryRowsCols>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

Now we create an instance of a \class{sparseHessianFD} object,
evaluate the function, gradient and Hessian, and compare the output to
the true values.

<<usingSparseHessianFD>>=
obj <- sparseHessianFD(P, fn=binary.f, gr=binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors, order.row=FALSE)
f <- obj$fn(P) #$
all.equal(f, true.f)
gr <- obj$gr(P) #$
all.equal(gr, true.grad)
hs <- obj$hessian(P) #$
all.equal(hs, true.hess)
@

Finite differencing methods return linear approximations, not exact
values.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.


\section{Speed and scalability}\label{sec:timing}

As far as we know, \pkg{numDeriv} \citep{R_numDeriv} is the only \proglang{R} package
that computes numerical approximations to derivatives.  It differs from
\pkg{sparseHessianFD} in some important ways.

\begin{enumerate}
\item It treats all Hessians as dense;
  \item It computes each element of the Hessian using a second-order
    finite differencing approximation that does not require the user
    to supply the gradient; and
    \item It implements iterative algorithms to improve accuracy, as
      the expense of speed.
\end{enumerate}

Nevertheless, it is an easy-to-use tool for numerical differentiation,
so it is worthwhile to compare its performance to that of
\pkg{sparseHessianFD}.  To prepare Table \ref{tab:numDeriv}, we
estimated Hessians of the hierarchical model with different numbers of
units ($N$) and within-unit parameters ($k$).  As in the previous
section, the total number of
variables is $M=(N+1)k$.  Table \ref{tab:numDeriv} shows the mean
and standard deviations (across 20 replications) for the time (in milliseconds) to
to compute a Hessian using each package.  The difference in run times is dramatic,
especially because the computation time for \pkg{numDeriv} grows
quadratically in the number of variables.  The setup time for
\pkg{sparseHessianFD} was about 7 milliseconds for all cases.  Time
were collected using the \pkg{microbenchmark} package \citep{R_microbenchmark}.



\begin{table}\centering
\input{table_numDeriv_fixed.tex}
\caption{Computation times (milliseconds) for computing Hessians using
  \pkg{numDeriv} and \pkg{sparseHessian}.  Rows are ordered by the
  number of variables.}\label{tab:numDeriv}
\end{table}


Because \pkg{numDeriv} does not scale, we cannot use it as a benchmark
against which to assess the performance of \pkg{sparseHessianFD} for
large, sparse Hessians.  To help us understand just how scalable
\pkg{sparseHessianFD} is, we ran another set of simulations, for the
same hierarchical model, for different values of $N$ and $k$.  We then
recorded the run times  for 200 replications for different tasks in the sparse Hessian estimation.

The time measures are summarized in Table \ref{tab:timeMeasures}.

\begin{table}\centering
  \begin{tabularx}{1.0\linewidth}{>{\bfseries}l X}
Function&estimating the objective function\\
Gradient&estimating the gradient\\
Hessian&computing the Hessian (not including initialization or
  partitioning time)\\
Partitioning&finding a consistent partitioning of the
  variables (the vertex coloring problem)\\
Initialization&total setup time (including the partitioning time)\\
Hessian/Gradient&ratio of the Hessian time to the
    gradient time
  \end{tabularx}
  \caption{Summary of timing tests (see Figure \ref{fig:timing})}\label{tab:timeMeasures}
\end{table}

In the plots in Figure \ref{fig:timing}, the number of heterogeneous units ($N$) is
on the x-axis, and mean run time, in milliseconds, is on the y-axis.  Each panel shows the
relationship between $N$ and run time for a different measure of time,
and each curve in a panel represents a different number of within-unit
parameters ($k$).  Recall that for the binary choice example, $k$ is
also the number of population-level parameters, so the total number of
variables is $M=(N+1)k$.



\begin{figure}[tbh]
  \centering
\includegraphics{timings}
  \caption{Run times for sparse Hessian computation}
  \label{fig:timing}
\end{figure}

We include Hessian/Gradient in this list because it can take
more time to compute even a sparse Hessian as the total number of
variables increases.  No matter how we compute the Hessian, the
function and gradient computations take longer as the number of
variables and the amount of data increases.  There are simply more
arithmetic operations involved. The time to evaluate the Hessian,
relative to the time to compute the gradient, is a more useful measure
of the scalability of the Hessian estimation method than the Hessian
computation time alone.





The first thing to note is that the times to compute the function, and the
gradient grow linearly with the number of
heterogeneous units. We expect that, since larger data sets involve
more arithmetic operations.  If the Hessian were dense, we would
expect Hessian computation time to grow quadratically with $N$.

For our example, the Hessian is sparse, the time increases linearly in
$N$, which is certainly better than quadratically.  But where does the
time come from?  First, we see that most of the time is in the setup
(validation, colors, etc.).  But the actual computation is still
linear in $N$.  However, if we were to plot the Hessian computation
time ratios, with respect to time to compute the function and
gradient, we see that the Hessian computation time is about constant
with respect to $N$.  Thus, for a given $k$, the time to compute the
Hessian is a fixed multiple of the time to compute the gradient,
regardless of how many units are in the data set.

However, that multiple grows with $k$.  The reason is that as $k$
grows, the number of groups needed to partition the variables grows as
well.  Since there are more groups, we need more evaluations of the
gradient to recover the Hessian.  For each additional $N$, each
gradient takes longer to compute, but the number of gradient
evaluations goes only with the number of groups.  The key to the
scalability of \pkg{sparseHessianFD} for hierarchical models is that even as the number of
heterogeneous units goes up, the number of groups stays the same.






\section{Algorithm and Permutations}\label{sec:algorithms}

At this point, we must note that, in general, vertex coloring problems
are NP-Hard.  No algorithm can, as a general rule, find
the \emph{smallest} number of colors needed to color a graph, or
determine the coloring associated with that number.  Fortunately, for
practical purposes we do not need to partition the variables in an
optimal way.  We just need a reasonably good partition that will
reduce the number of gradient evaluations enough to offer a
substantial resource savings when estimating the Hessian.  We make no
claims that any of the algorithms in \pkg{sparseHessianFD} are
optimal, or even ``best available.''  But we do expect them to
generate the correct results faster than the other alternatives.


\subsection{Partitioning variables}
An efficient partitioning of the variables is one that minimizes the
number of gradient evaluations required to recover the Hessian.  The
partitioning is consistent with a substitution method if it can be characterized as a cyclic coloring
of the graph of the sparsity pattern, or, equivalently, as a proper
coloring of the intersection graph.

The first step is to permute the rows and columns of the Hessian.   A
reasonable choice is a permutation that sorts the rows and columns in
decreasing order of the number of elements.
This is the ``smallest-last'' ordering suggested by
\citet[Theorem 6.2]{ColemanMore1984}.  To justify this permutation, suppose
non-zeros within a row are randomly distributed across
columns.  If the row is near the top of the matrix, there is a higher
probability that the non-zero element is in the upper triangle, not in
the lower.  We can put sparser rows near the bottom.  This ordering
does not affect the total number of non-zeros in the lower triangle,
but it should minimize the number of non-zeros in each row.  Thus, we
would expect the number of columns with non-zero elements in common
rows to be smaller, and the intersection graph to be sparser.

Call the chosen permutation $\pi$.   Let $\hessLT{\pi}{}$ be the lower triangle of the
sparsity pattern of the Hessian, with rows and columns permuted by
$\pi$.  The adjacency matrix of the intersection graph $\Gamma$ is the Boolean
crossproduct, $\Gamma = L^\top L$.  Algorithm \ref{alg:coloring} is a
``greedy'' vertex coloring algorithm, in which vertices are colored
sequentially.  Running Algorithm \ref{alg:coloring} on $\Gamma$
generates a proper coloring on $\hessLT{\pi}{}$, which in turn is a
consistent partitioning of the variables.


\begin{algorithm}
  \begin{algorithmic}
    \REQUIRE{$P[i],i=1,\mathellipsis,M$: sets of column indices  of
      non-zero elements in row $i$.}
    \REQUIRE{$F[i],i=1,\mathellipsis,M$:  sets of ``forbidden'' colors for
         vertex $i$ (initially empty)}
    \REQUIRE{$U$:  set of used colors (initially empty)}
    \REQUIRE{$C[i], i=1,\mathellipsis,M$: vector to store output of
      assigned colors (initially all zero)}.
    \STATE{$k\leftarrow 0$}\COMMENT{Largest color index used}
    \STATE{Insert $0$ in $U$}
    \FOR{$i=1$ \TO $M$}
       \IF{$F[i]$ is empty (no forbidden colors)}
           \STATE{$C[i]\leftarrow 0$}
        \ELSE
           \STATE{$V\leftarrow U - F[i]$}\COMMENT{Used colors that are
             not forbidden}
           \IF{$V$ is empty}
              \STATE{$k\leftarrow k+1$}
              \STATE{Insert $k$ into $U$}
              \STATE{$C[i]\leftarrow k$}
        \ELSE
             \STATE{$C[i]\leftarrow\min(V)$}
\COMMENT{Assign smallest existing non-forbidden color to $i$}
             \ENDIF
    \ENDIF
    \ENDFOR
    \FOR{$j$ in $P[i]$}
    \STATE{Insert $C[i]$ into $F[j]$}
\COMMENT{Make $i$'s color forbidden to all of
      its uncolored neighbors}
    \ENDFOR
    \RETURN{$C$}
    \end{algorithmic}
    \caption{Consistent partitioning of variables for a triangular
      substitution method}\label{alg:coloring}
    \end{algorithm}

\subsection{Computing the Hessian by substitution}

Let $C_m$ be the assigned color to variable $m$.  From \citet[Equation 6.1]{ColemanMore1984},
\begin{align}
  \label{eq:2}
  \hess{ij}{x}&=Y_{i,C_j}/\delta - \sum_{l>i,l\in C_j}\hess{li}{x}
\end{align}

We implement the substitution method using Algorithm \ref{alg:subst}.
This algorithm completes the bottom row of the lower triangle, copies
values to the corresponding column in the upper triangle, and advances upwards.

\begin{algorithm}
  \begin{algorithmic}
    \REQUIRE{$P[i],i=1,\mathellipsis,M$: sets of column indices  of
      non-zero elements in row $i$.}
    \REQUIRE{$C[i], i=1,\mathellipsis,M$: vector of
      assigned colors}
    \REQUIRE{$H$, an $M\times M$ Hessian (initialized to zero)}
    \REQUIRE{$B$, a $\max(C)\times M$ matrix (initialized to zero)}
    \FOR{$i$ = $M$ \TO $1$}
    \FOR{All $j$ in $P_i$}
    \STATE{$z\leftarrow Y[i,C[j]]/\delta - B[C[j], i]$}
    \STATE{$B[C[i], j]\leftarrow B[C[i], j] + z$}
    \STATE{$H[i,j]\leftarrow z$}
    \STATE{$H[j,i]\leftarrow H[i,j]$}
    \ENDFOR
    \ENDFOR
  \end{algorithmic}
  \caption{Triangular substitution method}\label{alg:subst}
\end{algorithm}


\bibliography{braun_refs}

\end{document}

\documentclass[10pt]{article}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{colortbl}
\usepackage{parskip}
\usepackage{setspace}
\linespread{1.1}


\usepackage{etoolbox}
\newtoggle{tikz}
\togglefalse{tikz}

\iftoggle{tikz}{
\usepackage{tikz}
}{}

\usepackage{xcolor}
\usepackage{hyperref}

%%make knitr environment line spacing separate from LaTeX
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\Chol{Chol }
\DeclareMathOperator\cov{cov}

\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
\newcommand{\filename}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}


 \newcommand{\df}[3]{\mathsf{d}^{#1}f(#2;#3)}
 \newcommand{\parD}[3]{\mathsf{D}^{#1}_{#2}#3}
\newcommand{\hess}[2]{\mathsf{H}_{#1}f(#2)}
\newcommand{\hessLT}[2]{\mathsf{L}_{#1}f(#2)}
\newcommand{\Mat}[1]{#1}

\usepackage[style=authoryear,%
			backend=biber,%
			maxbibnames=99,%
                        bibencoding=utf8,%
                        maxcitenames=1,
                        citetracker=true,
                        dashed=false,
                        maxalphanames=1,%
                        backref=false,%
			doi=true,%
                        url=true, %
			isbn=false,%
                        mergedate=basic,%
                        dateabbrev=true,%
                        natbib=true,%
                        uniquename=false,%
                        uniquelist=false,%
                        useprefix=true,%
                        firstinits=false
			]{biblatex}

 %%\addbibresource{~/OneDriveBusiness/References/Papers/braun_refs.bib}
\addbibresource{./braun_refs.bib}
\title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
\author{Michael Braun\\Cox School of Business\\Southern Methodist University}
\date{April 30, 2015}

\begin{document}

\maketitle

<<setup1, echo=FALSE, cache=FALSE>>=
library(methods, quietly = TRUE)
library(numDeriv, quietly = TRUE)
library(Matrix, quietly = TRUE)
library(sparseHessianFD, quietly=TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE,
                          message=FALSE, tidy=FALSE, cache = TRUE)
@

The Hessian matrix of a log likelihood function or log posterior
density function plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  More broadly, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $M$ variables has $M^2$
elements, of which $M(M+1)/2$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the number of
decision variables.   For
functions with hundreds of thousands of variables, computing the
Hessian even once might not be practical for applications constrained
by time, storage or processor availability.  Hierarchical models, in
which each additional heterogeneous unit is associated with its own subset of
variables, are particularly vulnerable to this curse of dimensionality

For many problems (hierarchical models among them), the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  Consider a log
posterior density in a Bayesian hierarchical
model.  If the outcomes across units are conditionally
independent, the cross-partial derivatives with respect to those
parameters are zero.  As the number of units
increases, the size of the Hessian still grows quadratically, but the number
of \emph{non-zero} elements grows only linearly; the Hessian
becomes increasingly sparse.  The row and column indices of the
non-zero elements comprise the \emph{sparsity pattern} of the Hessian,
and are typically known in advance, before computing the values of
those elements.

The \pkg{sparseHessianFD} package is a tool for estimating
sparse Hessians using \emph{finite differencing}.  Section \ref{sec:theory} will
cover the specifics, but the basic idea is as follows.  Consider a
function $f(x)$, its gradient $\parD{}{}{f(x)}^\top$ (the transpose of the
derivative), and its Hessian $\hess{}{x}$.  Let $e_m$ be the $m$th
coordinate vector, and let $\delta$ be a sufficiently small scalar
constant. The vector $\hess{m}{x}\approx\left(\parD{}{}{f(x+\delta e_m)}- \parD{}{}{f(x)}\right)/\delta$ is
a linear approximation to $\hess{m}{x}$, the $m$th column of the
Hessian. Estimating a dense Hessian in this way involves $M+1$ calculations of the
gradient: one for the gradient at $x$, and one after perturbing each
of the $M$ elements of $x$, one at a time.  However, if the Hessian has
a sparsity pattern that allows it, we could perturb
more than one element of $x$ at a time, evaluate the gradient fewer
than $M+1$ times, and still recover the non-zero
Hessian values.  For some sparsity patterns, estimating a Hessian in
this way can be profoundly efficient.  In fact, for the hierarchical models that we consider in
this paper, the number of gradient evaluations is \emph{constant},
even as additional units are added to the model.  How to
decide which variables can be perturbed together is actually a graph
coloring problem, which we discuss in Section \ref{sec:coloring}.

At the outset, we want to mention that there may be some applications for
which \pkg{sparseHessianFD} is not an appropriate package to use. To
extract the maximum benefit from using \pkg{sparseHessianFD}, we need
to accept a few conditions or assumptions.

\begin{enumerate}
  \item Preferred alternatives to computing the Hessian are not
  available.  Finite differencing is not generally a ``first choice''
  method.  Deriving a gradient or Hessian symbolically, and writing a
  subroutine to compute it, will give an exact answer, but might be
  tedious or difficult to implement. Algorithmic differentiation (AD) is
  probably the most efficient method, but requires specialized
  libraries that, at this moment, are not yet broadly available in \proglang{R}.
 \pkg{sparseHessianFD} makes the most sense when the gradient is easy to compute, but the Hessian is
  not.
  \item The application can tolerate the approximation error in the Hessian
  that comes with finite differencing methods.
\item The objective function $f(x)$ is  twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in a function that itself is hard to compute.
\item The gradient can
  be computed quickly, easily and \emph{exactly} (within machine
  precision).   We do not recommend using finite differenced gradients
  when computing finite differenced Hessians, for two reasons.  First,
  the approximation errors will be compounded.  Second, the time
  complexity of computing a gradient grows with the number of
  variables when using finite differencing, but not with other methods
  like AD \citep[p. xii]{GriewankWalther2008}.
\item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.
\end{enumerate}

Some users may find the requirement to provide a function that
computes the gradient to be burdensome.  We take the position that deriving a vector of first
derivatives, and writing \proglang{R} functions to compute them, is a
lot easier than doing the same for a matrix of second derivatives.
Even when we have derived and
coded the
Hessian matrix symbolically, in practice it may still be faster to estimate the Hessian
using \pkg{sparseHessianFD} than coding it directly.   These are the
situations in which \pkg{sparseHessianFD} adds the most value.  If AD software is available to compute the gradient, then
it is probably available for sparse Hessians as well, and
\pkg{sparseHessianFD} would not be needed.

The rest of this article proceeds as follows.  In Section
\ref{sec:background}, we introduce some fundamental material on
sparsity patterns, numerical differentiation, and graph theory.  In Section \ref{sec:using}, we demonstrate how
to use the package, and summarize the underlying algorithms.  Section \ref{sec:timing} includes some time and
accuracy tests.

\section{Background}\label{sec:background}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.  Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities, and let $p_i$ be the probability of
 purchase.  The heterogeneous parameter $p_i$ is the same for all $T$
 opportunities, so $y_i$ is a binomial random variable.  Define each $p_i$ such that it depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$.
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients are distributed across the population of households
following a multivariate normal distribution with mean $\mu$ and
covariance $\Sigma$.   Assume that we know $\Sigma$, but not $\mu$.
Instead, place a multivariate normal prior on $\mu$, with mean $0$ and
covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are
$k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}
We will return to this example throughout the article.

\subsection{Sparsity patterns}\label{sec:sparsity}

<<setup2, echo=FALSE>>=
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

The log posterior density in Equation \ref{eq:LPD} has a sparse
Hessian.  Since the $\beta_i$ are drawn iid from a multivariate
normal, and the $y_i$ are conditionally independent,
$\hess{\beta_i,\beta_j}{}=\parD{2}{\beta_i, \beta_j}{\log\pi}=0$ for all $i\neq
j$.  However, all of the $\beta_i$ are correlated with
$\mu$.  Thus, $\hess{\beta_i,\mu_k}{}\neq 0$ for all $i$.

The sparsity pattern depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=\Sexpr{N}$ and $k=\Sexpr{k}$, then there are `\Sexpr{nv1}` total
variables, and the Hessian will have the pattern in Figure \ref{fig:blockarrow}..



Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "banded" sparsity pattern, as in Figure \ref{fig:banded}.

\begin{figure}[htb]
  \begin{subfigure}[b]{.5\textwidth}
<<blockarrow, echo=FALSE>>=
Mat <- as(Matrix::kronecker(Matrix::Diagonal(N),Matrix(1,k,k)),"nMatrix")
Mat <- rBind(Mat, Matrix(TRUE,k,N*k))
Mat <- cBind(Mat, Matrix(TRUE, k*(N+1), k))
print(Mat)
@
\caption{A ``block-arrow sparsity pattern''}\label{fig:blockarrow}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}
<<banded, echo=FALSE>>=
Mat <- as(Matrix::kronecker(Matrix(1,k,k), Matrix::Diagonal(N)),"nMatrix")
Mat <- rBind(Mat, Matrix(TRUE,k,N*k))
Mat <- cBind(Mat, Matrix(TRUE, k*(N+1), k))
print(Mat)
@
\caption{A ``banded'' sparsity pattern}\label{fig:banded}
\end{subfigure}
\caption{Two examples of sparsity patterns for a hierarchical model}\label{fig:patterns}
\end{figure}

In both cases, the number of non-zeros is the same.   There are \Sexpr{nels1} elements in this symmetric matrix, but only \Sexpr{nnz1} are
non-zero, and only \Sexpr{nnz1LT} values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=\Sexpr{Q}$ instead.  In that case,
there are \Sexpr{nv2} variables in the problem, and more than $\Sexpr{floor(nels2/10^6)}$ million
elements in the Hessian.  However, only $\Sexpr{nnz2}$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only \Sexpr{nnz2LT} values.

\subsection{Numerical differentiation}\label{sec:numdiff}

The partial derivative of a scalar-valued function $f(x)$ with respect to $x_j$ (the $j$th
component of $x$) is defined as
\begin{align}
  \label{eq:defParD}
\parD{}{j}{f(x)}=\lim\limits_{\delta\to 0}\frac{f(x+\delta e_j)-f(x)}{\delta}
\end{align}
For a sufficiently small $\delta$, this definition allows for a
linear approximation to $\parD{}{j}{f(x)}$.  The derivative of $f(x)$
is the vector of all $M$ partial derivatives.

\begin{align}
  \parD{}{}{f(x)}=\left(\parD{}{1}{f(x)},\mathellipsis,\parD{}{M}{f(x)}\right)
  \end{align}
 The \emph{gradient} is defined as $\nabla f(x)=\parD{}{}{f(x)}^\top$.

We define the second-order partial derivative as
\begin{align}
  \label{eq:14}
  \parD{2}{jk}{f(x)}=\lim\limits_{\delta\to 0}\frac{\parD{}{j}{f(x+\delta e_k)}-\parD{}{j}{f(x)}}{\delta}
\end{align}
and the Hessian matrix as
\begin{align}
  \label{eq:15}
  \hess{}{x}=
  \begin{pmatrix}
    \parD{2}{11}{f(x)}&  \parD{2}{12}{f(x)}&  \mathellipsis &  \parD{2}{1M}{f(x)}\\
    \parD{2}{21}{f(x)}&  \parD{2}{22}{f(x)}&  \mathellipsis &  \parD{2}{2M}{f(x)}\\
    \vdots&\vdots&&\vdots\\
    \parD{2}{M1}{f(x)}&  \parD{2}{M2}{f(x)}&  \mathellipsis &  \parD{2}{MM}{f(x)}
    \end{pmatrix}
\end{align}

The Hessian is symmetric, so $\parD{2}{ij}{}=\parD{2}{ji}{}$.

To estimate the $k$th column of $\hess{}{x}$, we again choose a
sufficiently small $\delta$, and compute
\begin{align}
  \label{eq:1}
  \hess{k}{x}&\approx\frac{\parD{}{}{f(x+\delta e_k)}-\parD{}{}{f(x)}}{\delta}
\end{align}


For $M=2$, our estimate of a general $\hess{}{x}$ would be
\begin{align}
  \label{eq:FDhess2}
  \hess{}{x}&=
  \begin{pmatrix}
    \parD{}{1}{f(x_1+\delta, x_2)}-\parD{}{1}{f(x_1,x_2)}& \parD{}{1}{f(x_1,x_2+\delta)}-\parD{}{1}{f(x_1,x_2)}\\
        \parD{}{2}{f(x_1+\delta, x_2)}-\parD{}{2}{f(x_1,x_2)}&  \parD{}{2}{f(x_1,x_2+\delta)}-\parD{}{2}{f(x_1,x_2)}
    \end{pmatrix}/\delta
\end{align}

This estimate requires three evaluations of the gradient to get
$\parD{}{}{f(x_1,x_2)}$, $\parD{}{}{f(x_1+\delta,x_2)}$, and
$\parD{}{}{f(x_1,x_2+\delta)}$.

Now suppose that the Hessian is sparse, and that the off-diagonal
elements are zero.  Not only are
\begin{align}
  \label{eq:3}
  \parD{}{1}{f(x_1,x_2+\delta)}-\parD{}{1}{f(x_1,x_2)}&=0\\
    \parD{}{2}{f(x_1+\delta,x_2)}-\parD{}{2}{f(x_1,x_2)}&=0
\end{align},
but also,
\begin{align}
  \label{eq:3a}
  \parD{}{1}{f(x_1+\delta,x_2+\delta)}-\parD{}{1}{f(x_1+\delta,x_2)}&=0\\
    \parD{}{2}{f(x_1+\delta,x_2+\delta)}-\parD{}{2}{f(x_1,x_2+\delta)}&=0
\end{align}
Therefore,
\begin{align}
  \label{eq:FDhess2sp}
  \hess{}{x}&=
  \begin{pmatrix}
    \parD{}{1}{f(x_1+\delta, x_2+\delta)}-\parD{}{1}{f(x_1,x_2)}&0\\
        0&  \parD{}{2}{f(x_1+\delta,x_2+\delta)}-\parD{}{2}{f(x_1,x_2)}
    \end{pmatrix}/\delta
\end{align}

This estimate requires only two evaluations of the gradient to get
$\parD{}{}{f(x_1,x_2)}$ and $\parD{}{}{f(x_1+\delta,x_2+\delta)}$.
Being able to reduce the number of gradient evaluations from 3 to 2 depends on knowing that
the cross-partial derivatives are zero.

The ``trick'' for the general case of sparse Hessians is to partition
the decision variables into $C$ mutually exclusive groups in such as way that the number of gradient
evaluations is minimized.  Let $\Mat{G}$ be a $M\times C$
matrix, where $\Mat{G}_{mc}=\delta$ if variable $m$ belongs to group $c$, and zero
otherwise.  Define $G_c$ as the $c$th column of $G$.

Next, let $\Mat{Y}$ be a $M\times C$ matrix, where each column is a
difference in gradients.
\begin{align}
  \label{eq:Yg}
  Y_c&=\nabla f(x+G_c)-\nabla f(x)
\end{align}

If $C=K$, then $\Mat{G}$ is a diagonal matrix with $\delta$ in each
diagonal element.  The matrix equation $\hess{}{x}G=Y$ represents the linear approximation
$\hess{im}{x}\delta\approx y_{im}$, and we can solve for all elements of $\hess{}{x}$
just by computing $Y$. But if $C<M$,
there must be at least one $G_c$ with $\delta$ in at least two
rows. Column $Y_c$ would have been computed by perturbing two variables at
once, and we would not have been able to solve for any $\hess{im}{x}$
without further constraints.

These constraints come from the sparsity pattern and symmetry
of the Hessian. Consider an example with the following values and
sparsity pattern.

\begin{align}
  \label{eq:7}
 \hess{}{x}= \begin{pmatrix}
    h_{11}&0&h_{31}&0&0\\
    0&h_{22}&0&h_{42}&0\\
    h_{31}&0&h_{33}&0&h_{53}\\
    0&h_{42}&0&h_{44}&0\\
    0&0&h_{53}&0&h_{55}
  \end{pmatrix}
\end{align}

Suppose $C=2$, and define the colors of the five variables through the following $\Mat{G}$ matrix.
\begin{align}
  \label{eq:7}
 \Mat{G}^\top= \begin{pmatrix}
   \delta&\delta&0&0&\delta\\
   0&0&\delta&\delta&0
  \end{pmatrix}
\end{align}
Variables 1 and 2 are in group 1, and variables 3, 4 and 5 are in
group 2.  For the moment, we will postpone the discussion of how to
choose $C$ and how to partition the variables until Section \ref{sec:coloring}.

Next, compute the columns of $\Mat{Y}$ using Equation \ref{eq:Yg}.  We now
have the following system of linear equations from $\hess{}{x}\Mat{G}=\Mat{Y}$.
\begin{align}
  \label{eq:11}
  \begin{aligned}[c]
  h_{11}&=y_{11}\\
  h_{22}&=y_{21}\\
  h_{31}+h_{53}&=y_{31}\\
  h_{42}&=y_{41}\\
  h_{55}&=y_{51}\\
  \end{aligned}
  \qquad
  \begin{aligned}[c]
  h_{31}&=y_{12}\\
  h_{42}&=y_{22}\\
  h_{33}&=y_{32}\\
  h_{44}&=y_{42}\\
  h_{53}&=y_{52}
  \end{aligned}
\end{align}

Note that this system is overdetermined.  Both $h_{31}=y_{12}$ and $h_{53}=y_{52}$
can be determined directly, but $h_{31}+h_{53}=y_{31}$, and $h_{42}$
could be either $y_{41}$ or $y_{22}$.  \Citet{PowellToint1979} prove
that it is sufficient to solve $\hessLT{}{x}G=Y$ instead via a \emph{substitution method}, where $\hessLT{}{x}$ is the lower
triangular part of $\hess{}{x}$.  This has the effect of removing the equations
$h_{42}=y_{22}$ and $h_{31}=y_{12}$ from the system, but retaining
$h_{53}=y_{52}$.  We can then solve for
$h_{31}=y_{31}-y_{52}$. Thus, we have determined a
$5\times 5$ Hessian matrix with only three gradient evaluations, in
contrast with the six that would have been needed had $\hess{}{x}$ been treated
as dense.

The $\Mat{Y}$ matrix is, in essence, a compressed representation of
$\hess{}{x}$.  The values in $Y_c$ are finite differences after
one \emph{or more} variables are perturbed.  Hence, we need two
different algorithms to estimate a sparse Hessian.  The first is to
determine which variables can be perturbed together when computing
$\Mat{Y}$, and the second is to extract the values of $\hess{}{x}$
from $\Mat{Y}$.  We discuss these algorithms next.


\subsection{Partitioning the variables}\label{sec:coloring}

It turns out that finding valid, efficient partitions can be
characterized as a vertex coloring problem from graph theory \citep{ColemanMore1984}.  In this
sense, each variable is a vertex in an undirected graph, and an edge connects two
vertices $i$ and $j$ if and only if $\hess{ij}{x}\neq 0$.  The
sparsity pattern of the Hessian is the adjacency matrix of the graph.
By ``color,'' we mean nothing more than group assignment; if a
variable is in a group, then its vertex has the color associated with
that group.  A ``proper'' coloring of a graph is one in which two
vertices with a common edge do not have the same
color. \Citet{ColemanMore1984} define a ``triangular coloring'' of a graph is
a proper coloring with the additional condition that common
neighbors of a vertex do not have the same color.  A triangular
coloring is a special case of an ``cyclic coloring,'' in which any cycle in the graph
uses at least three colors \citep{GebremedhinTarafdar2007}.

We can also assign
to each vertex a set of other characteristics.  The ``intersection
set'' contains characteristics that are common to two vertices, and
the ``intersection graph'' connects vertices whose intersection set is
not empty.  In our context, the set in question is the row indices of
the non-zero elements in each column of $\hessLT{}{x}$.  In the
intersection graph, two vertices are connected if the corresponding
columns in $\hessLT{}{x}$ have at least one non-zero element in the same row.

\Citet{PowellToint1979} write that a partitioning is consistent with a
substitution method if and only if no columns
of the of lower triangle of the Hessian that are
in the same group have a non-zero element in the same
row.  An equivalent statement is that no two vertices in the
intersection graph can have the same color.  Thus, we can partition
the variables by creating a proper coloring of the intersection graph
of $\hessLT{}{x}$.

This intersection graph, and the number of colors needed to
color it, are not invariant to
permutation of the rows and columns of $\hess{}{x}$.  Let $\pi$
represent such a permutation, and let $\hessLT{\pi}{x}$ be the lower
triangle of $\pi\hess{}{x}{\pi^\top}$.    \Citet[Theorem 6.1]{ColemanMore1984} show that a coloring is
triangular if and only if it is also a coloring of the intersection
graph of $\hessLT{\pi}{x}$ for some permutation $\pi$.  Furthermore, \citet{ColemanCai1986} prove that a partitioning is consistent with a
substitution method if and only if it is an acyclic coloring of the
graph of the sparsity pattern of the Hessian.   Therefore,
finding an optimal partitioning of the variables involves finding an
optimal combination of a permutation $\pi$, and coloring algorithm
for the intersection graph of $\hessLT{\pi}{x}$.

These ideas are illustrated in Figures \ref{fig:graph1} and
\ref{fig:graph2}.  Figure
\ref{fig:graph1adj} shows the sparsity pattern of the lower triangle of a
Hessian as an adjacency matrix, and Figure \ref{fig:graph1pic} is the
associated graph with a proper vertex coloring.  We can deduce from
Figure \ref{fig:graph1adj} by noting that
every column (and thus, every pair of columns) has a non-zero element
in row 7.  There are no non-empty intersection sets across the
columns.  All vertices are connected to each other in the
intersection graph (Figure \ref{fig:graph1int}), which requires seven colors for a proper coloring.  Estimating a
sparse Hessian with this partitioning scheme would be no more
efficient than if treating the Hessian as if it were dense.

\begin{figure}
  \begin{subfigure}[b]{.33\textwidth}\centering
  \begin{tabular}{r|ccccccc}
   & 1&2&3&4&5&6&7\\
    \hline
    1& 1&&&&&&\\
    2&1&1&&&&&\\
    3&0&0&1&&&&\\
    4&0&0&1&1&&&\\
    5&0&0&0&0&1&&\\
    6&1&0&0&0&1&1&\\
    7&1&1&1&1&1&1&1
  \end{tabular}
  \caption{Adjacency matrix}\label{fig:graph1adj}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Sparsity graph}\label{fig:graph1pic}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=yellow!20] (v3) {3};
\node at (180:1) [fill=purple!20] (v4) {4};
\node at (240:1) [fill=brown!20] (v5) {5};
\node at (300:1) [fill=white!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v6)
(v2) -- (v6)
(v3) -- (v6)
(v4) -- (v6)
(v5) -- (v6)
(v1) -- (v5)
(v2) -- (v5)
(v3) -- (v5)
(v4) -- (v5)
(v1) -- (v4)
(v2) -- (v4)
(v3) -- (v4)
(v1) -- (v3)
(v2) -- (v3)
(v1) -- (v2);
\end{tikzpicture}
}{} %% end toggle
\caption{Intersection graph}\label{fig:graph1int}
\end{subfigure}
\caption{Unpermuted matrix}\label{fig:graph1}
\end{figure}

Now suppose that we were to rearrange $\hess{}{x}$ so the last row and
and column were moved to the front.  The variables are reordered, but the meanings of the cross-partial
derivatives have not changed.  However, as we see in Figure
\ref{fig:graph2}, the lower triangular adjacency matrix and
the corresponding intersection graph are different from Figure \ref{fig:graph2} .  In Figure \ref{fig:graph2adj}, all columns share at least one non-zero row with
the column for variable 7, but variable groups $\{2,4,6\}$ and
$\{1,3,5\}$ have empty intersection sets.  The intersection graph in
Figure \ref{fig:graph2pic} has many fewer edges, and can be colored
with only three colors.

\begin{figure}
  \begin{subfigure}[b]{.33\textwidth}\centering
  \begin{tabular}{r|ccccccc}
   & 7&1&2&3&4&5&6\\
    \hline
    7& 1&&&&&&\\
    1&1&1&&&&&\\
    2&1&1&1&&&&\\
    3&1&0&0&1&&&\\
    4&1&0&0&1&1&&\\
    5&1&0&0&0&0&1&\\
    6&1&0&0&0&0&1&1
  \end{tabular}
  \caption{Adjacency matrix}\label{fig:graph2adj}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Sparsity graph}\label{fig:graph2pic}
\end{subfigure}
\begin{subfigure}[b]{.33\textwidth}\centering
\iftoggle{tikz}{
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\node at (0,0) [fill=green!20] (v7) {7};
\node at (0:1) [fill=blue!20] (v1) {1};
\node at (60:1) [fill=red!20] (v2) {2};
\node at (120:1) [fill=blue!20] (v3) {3};
\node at (180:1) [fill=red!20] (v4) {4};
\node at (240:1) [fill=blue!20] (v5) {5};
\node at (300:1) [fill=red!20] (v6) {6};
\draw
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v7)
(v6) -- (v7)
(v1) -- (v2)
(v3) -- (v4)
(v5) -- (v6);
\end{tikzpicture}
}{} %% end toggle
\caption{Intersection graph}\label{fig:graph2int}
\end{subfigure}
\caption{Permuted matrix}\label{fig:graph2}
\end{figure}


The specifics of how to permute and partition the variables, and how
to implement the substitution method to compute the Hessian, should be
invisible to the user of the \pkg{sparseHessianFD} package.  These are
all discussed in Section \ref{sec:algorithms}, which should be
considered optional reading.




\section{Using the package}\label{sec:using}

In this section we discuss how to use \pkg{sparseHessianFD} to
estimate sparse Hessians.    The package functionality is implemented
as a reference class.  A \class{sparseHessianFD} object is initialized
with \proglang{R} routines that compute the function and its gradient,
as well as the sparsity pattern of the lower triangle of the Hessian.

\subsection{The \class{sparseHessianFD} class}


The function \func{sparseHessianFD} is an initializer that takes the
following arguments, and returns a reference to a
\class{sparseHessianFD} object.
\begin{itemize}
\item[x] A numeric vector, with length $M$ at which the object will be
  initialized and tested.
\item[fn,gr] \proglang{R} Functions that return the value of the
  objective function, and its gradient. The first argument is the numeric
  variable vector.  Other named arguments can be passed to \func{fn}
  and {gr} as well (see the \funcarg{...} argument below).
\item[rows, cols] Integer vectors of the row and column indices of
  the non-zero elements in the \emph{lower triangle} of the Hessian.
\item[direct] This argument is deprecated, and is included only for
  backwards compatibility with earlier versions.
\item[eps] The perturbation amount for finite differencing of the
  gradient to compute the Hessian (the $\delta$ in Section \ref{sec:numdiff}).  Defaults to
  \code{sqrt(.Machine\$double.eps)}.
\item [index1] If \variable{TRUE} (the default), \funcarg{row} and \funcarg{col} use one-based
  indexing.  If \variable{FALSE}, zero-based indexing is used.
\item [...] Additional arguments to be passed to \func{fn} and \func{gr}.
\end{itemize}

To create a \class{sparseHessianFD} object, just call
\func{sparseHessianFD}.  If you are accepting all of the default
arguments, the call will look like:

<<eval=FALSE>>=
obj <- sparseHessianFD(x, fn, gr, rows, cols, ...)
@
where \funcarg{...} represents all other named arguments that are
passed to \funcarg{fn} and \funcarg{gr}.

The class defines a number of different fields, none of which should
be accessed directly.    The initializer automatically calls the graph
coloring subroutine, evaluates the Hessian at $x$, and performs some
other tests, so it may take some time to create the object.


\subsection{Providing the sparsity pattern}

The sparsity pattern of the Hessian is defined as the row and column
indices of the non-zero elements in the \emph{lower triangle} the
Hessian.  It is the responsibility of the user to ensure that
the sparsity pattern is correct.

The \func{Matrix.to.Coord} function extracts
row and column indices from a sparse matrix.  The input matrix to \func{Matrix.to.Coord} does not have to include
the values (if the full Hessian were known, that would possibly defeat the
purpose of this package).  It is sufficient to supply a logical
or pattern matrix, such as \class{lgCMatrix} or \class{ngCMatrix}.
In practice, rather than trying to keep track of the row and column indices
directly, it might be easier to construct a pattern matrix first,
check visually that the matrix has the right pattern, and then extract
the indices.

The following code constructs a block diagonal matrix, and extracts
the sparsity pattern from its lower triangle.

<<exPattern>>=
Mat <- as(kronecker(Diagonal(3), Matrix(T,2,2)),"nMatrix")
Mat
tril(Mat)
mc <- Matrix.to.Coord(tril(Mat))
mc
@
The vectors \funcarg{mc\$row} and \funcarg{mc\$col} can be
passed to \func{sparseHessianFD} as the \funcarg{row} and
\funcarg{col} arguments, respectively.

To visually check that a proposed sparsity pattern represents the
intended matrix, use the \func{Coord.to.Pattern.Matrix} function,
which is just a wrapper to \pkg{Matrix}'s \func{sparseMatrix} constructor.

<<>>=
pattern <- Coord.to.Pattern.Matrix(mc$rows, mc$cols, dims=dim(Mat))
pattern
@



\subsection{Evaluating the Hessian}

The \func{fn}, \func{gr} and \func{hessian} methods respectively evaluate the
function, gradient and Hessian at a variable vector $x$.

<<eval=FALSE>>=
f <- obj$fn(x)
df <- obj$gr(x)
hess <- obj$hessian(x) #$
@



The \func{fn} and \func{gr} methods call the same functions that were
provided to the class initializer.  Since the
additional arguments were already supplied as \funcarg{...}, they do
not need to be supplied again.  This feature makes subsequent calling
of \func{fn} and \func{gr} simpler, because only the variable is
included in the call.

Similarly, the \func{hessian} method takes the single argument $x$.
The return value is always a \class{dgCMatrix} object (defined in the
\pkg{Matrix} package).  \class{dgCMatrix} objects are sparse matrices,
stored in a compressed, column-oriented format, and includes all
non-zero elements in both the upper and lower triangles.

The \func{fngr} method returns the function and gradient as a list.
The \func{fngrhs} includes the Hessian as well.


\subsection{An example}

Now we can use \pkg{sparseHessianFD} to estimate the Hessian for the
log posterior density of the model from Section \ref{sec:sparsity}.
The package includes functions that compute the value
(\func{binary.f}), the gradient (\func{binary.grad}) and the Hessian
{\func{binary.hess}.  The result from \func{binary.hess} is a
  ``true'' value against which we will compare the estimates from
  \class{sparseHessianFD}.  The package also includes sample datasets of different sizes, which
are access with the \func{data} function.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function. The \func{binary.f} and \func{binary.grad} functions take the data and
priors as lists.  The \func{data()} call adds the appropriate data
list to the environment, but we need to construct the prior list ourselves.

<<binaryInit>>=
library(sparseHessianFD)
set.seed(123)
data(binary_small)
binary <- binary_small
str(binary)
N <- length(binary[["Y"]])
k <- NROW(binary[["X"]])
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
nvars
@

<<echo=FALSE>>=
options(scipen=-999)
@

This dataset represents the simulated choices for $N= \Sexpr{N}$ customers
over $T= \Sexpr{T}$ purchase opportunties, where the probability of purchase
is influenced by $k= \Sexpr{k}$ covariates.

The next code chunk evaluates the ``true'' value, gradient and
Hessian.  The \funcarg{order.row} argument tells the function whether
the variables are ordered by household (\variable{TRUE}) or by covariate
(\variable{FALSE}). If \funcarg{order.row} is \variable{TRUE}, then the Hessian
will have an off-diagonal pattern.  If \funcarg{order.row} is \variable{FALSE}, then the
Hessian will have a block-arrow pattern.

<<trueValues>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two integer
vectors:  one each for the row and column indices of the non-zero
elements of the lower triangule of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the
\func{Matrix.to.Coord} function to extract the appropriate index
vectors.  If not, you need to determine the row and column indices manually.  If
the model is hierarchical, you could use the method of constructing
the matrices in \ref{sec:sparsity}.

<<binaryRowsCols>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

Now we create an instance of a \class{sparseHessianFD} object,
evaluate the function, gradient and Hessian, and compare the output to
the true values.

<<usingSparseHessianFD>>=
obj <- sparseHessianFD(P, fn=binary.f, gr=binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors, order.row=FALSE)
f <- obj$fn(P) #$
all.equal(f, true.f)
gr <- obj$gr(P) #$
all.equal(gr, true.grad)
hs <- obj$hessian(P) #$
all.equal(hs, true.hess)
@

Finite differencing methods return linear approximations, not exact
values.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.


\section{Speed and scalability}\label{sec:timing}

As far as we know, \pkg{numDeriv} \citep{R_numDeriv} is the only \proglang{R} package
that computes numerical approximations to derivatives.  It differs from
\pkg{sparseHessianFD} in some important ways.

\begin{enumerate}
\item It treats all Hessians as dense;
  \item It computes each element of the Hessian using a second-order
    finite differencing approximation that does not require the user
    to supply the gradient; and
    \item It implements iterative algorithms to improve accuracy, as
      the expense of speed.
\end{enumerate}

Nevertheless, it is an easy-to-use tool for numerical differentiation,
so it is worthwhile to compare its performance to that of \pkg{sparseHessianFD}.

<<echo=FALSE>>=
options(scipen=0)
@

<<numDerivCompare>>=
hess.time <- system.time(H1 <- obj$hessian(P)) #$
print(hess.time)
fd.time <- system.time(H2 <- hessian(obj$fn, P)) #$
print(fd.time)
@

Since the time and storage requirements for \pkg{numDeriv} grow
quadratically in the number of variables, it would not be useful to
compare its performance to \pkg{sparseHessianFD} for larger matrices.
But it may still be interesting to better understand just how scalable
\pkg{sparseHessianFD} is for hierarchical models, as the number of
heterogeneous units increases.




To better understand just how scalable \pkg{sparseHessianFD} is, we
recorded run times for different steps of the computation of the
Hessian for the binary example. In the plots in Figure XX,
each panel is a different step in the computation.

\begin{enumerate}
\item[Coloring] Time to find a consistent partitioning of the
  variables
\item[Setup] Total setup time (including coloring time)
\item[f] Time to estimate the objective function
\item[gr] Time to estimate the gradient
\item[Hessian] Time to compute the Hessian
  \item[Hessian/gr] Ratio of the Hessian time to the
    gradient time
\end{enumerate}

The x-axes represent $N$ (the number of hierarchical units in the binary choice example)
and each curve is $k$ (both the number of parameters within each unit,
and the number of population parameters) units.  The y-axes are mean
times across 200 replications,
in milliseconds, as recorded by the routines in the
\pkg{microbenchmark} package.

The first thing to note is that the times to compute the function, and the
gradient grow linearly with the number of
heterogeneous units. We expect that, since larger data sets involve
more arithmetic operations.  If the Hessian were dense, we would
expect Hessian computation time to grow quadratically with $N$.

For our example, the Hessian is sparse, the time increases linearly in
$N$, which is certainly better than quadratically.  But where does the
time come from?  First, we see that most of the time is in the setup
(validation, colors, etc.).  But the actual computation is still
linear in $N$.  However, if we were to plot the Hessian computation
time ratios, with respect to time to compute the function and
gradient, we see that the Hessian computation time is about constant
with respect to $N$.  Thus, for a given $k$, the time to compute the
Hessian is a fixed multiple of the time to compute the gradient,
regardless of how many units are in the data set.

However, that multiple grows with $k$.  The reason is that as $k$
grows, the number of groups needed to partition the variables grows as
well.  Since there are more groups, we need more evaluations of the
gradient to recover the Hessian.  For each additional $N$, each
gradient takes longer to compute, but the number of gradient
evaluations goes only with the number of groups.  The key to the
scalability of \pkg{sparseHessianFD} for hierarchical models is that even as the number of
heterogeneous units goes up, the number of groups stays the same.





    \section{Theory and Implementation}\label{sec:theory}

    This section explains why and how the package works, and can be
    skipped by end users with little interest in such things.









\section{Algorithm and Permutations}

At this point, we must note that, in general, vertex coloring problems
are NP-Hard (CITE HERE).  No algorithm can, as a general rule, find
the \emph{smallest} number of colors needed to color a graph, or
determine the coloring associated with that number.  Fortunately, for
practical purposes we do not need to partition the variables in an
optimal way.  We just need a reasonably good partition that will
reduce the number of gradient evaluations enough to offer a
substantial resource savings when estimating the Hessian.  We make no
claims that any of the algorithms in \pkg{sparseHessianFD} are
optimal, or even ``best available.''  But we do expect them to
generate the correct results faster than the other alternatives.







The \pkg{sparseHessianFD} implements algorithms for the following tasks:

\begin{enumerate}
\item Permuting rows and columns of the matrix to reduce the number of
  colors;
\item Finding a cyclic coloring that is consistent with the
  substitution method;
\item Computing the compressed matrix of finite differences of
  gradients; and
\item Solving triangular linear system to estimate the non-zero
  elements of the Hessian.
\end{enumerate}

None of these algorithms are meant to be optimal, or even the ``best
available.''  However, they are meant to be easy to implement and
maintain with standard libraries.

\subsubsection{Permuting Hessian}

Finding an appropriate permutation of the Hessian is similar to, but
exactly the same as, finding a fill-reducing permutation for a
Cholesky decomposition.  Instead of reducing the total number of zeros
in the lower triangle, we want to reduce the number of columns with
common non-zero rows.  A reasonable heuristic is to permute the matrix
such that the number of non-zero elements in each row is decreasing.
This is the ``smallest-last'' ordering discussed in
\citet{ColemanMore1984}.  The reason for this order is simple.
Suppose non-zeros within a row are randomly distributed across
columns.  If the row is near the top of the matrix, there is a higher
probability that the non-zero element is in the upper triangle, not in
the lower.  We can put sparser rows near the bottom, since the cells
that are in the lower triangle are less likely to be non-zero.

\subsubsection{Cyclic coloring}


As discussed above, we can get a triangular coloring of the sparsity
graph with a proper coloring of the intersection graph of the
columns.  Let $\hessLT{\pi}{\cdot}$ be the lower triangle of the
sparsity pattern of the Hessian, with rows and columns permuted by
$\pi$.  The adjacency matrix of the intersection graph $\Gamma$ is the Boolean
crossproduct, $\Gamma = L^\top L$.


This is a ``greedy'' coloring algorithm on the permuted matrix.

\begin{algorithm}
  \begin{algorithmic}
    \FOR{$i=1$ \TO $M$}
        \STATE{$P[i]\leftarrow$ set of column indices of non-zero
      elements in row $i$}
       \STATE{Initialize $F[i]$ as set of  ``forbidden'' colors for vertex $i$.}
        \STATE{$C[i]\leftarrow 0$.}
    \ENDFOR
    \STATE{$k\leftarrow 0$}
    \STATE{Insert $0$ in $U$}
    \FOR{$i=1$ \TO $M$}
       \IF{$F[i]$ is empty}
           \STATE{$C[i]\leftarrow 0$}
        \ELSE
           \STATE{$V\leftarrow U\cap F[i]$}
           \IF{$V$ is empty}
              \STATE{$k\leftarrow k+1$}
              \STATE{Insert $k$ into $U$}
              \STATE{$C[i]\leftarrow k$}
        \ELSE
             \STATE{$C[i]\leftarrow\min(V)$}
       \ENDIF
    \ENDIF
    \ENDFOR
    \FOR{$j$ in $P[i]$}
    \STATE{Insert $C[i]$ into $F[j]$}
    \ENDFOR
    \RETURN{$C$}
    \end{algorithmic}
    \caption{Consistent partitioning of variables for a triangular
      substitution method}\label{alg:coloring}
    \end{algorithm}

\subsection{Implementing the substitution method}

Now we can fill in the matrix.  Let $C_m$ be the assigned color to
variable $m$.  From \citet[Equation 6.1]{ColemanMore1984},
\begin{align}
  \label{eq:2}
  \hess{ij}{x}&=Y_{i,C_j}/\delta - \sum_{l>i,l\in C_j}\hess{li}{x}
\end{align}

We implement this substitution method using Algorithm \ref{alg:subst}.
\begin{algorithm}
  \begin{algorithmic}
    \STATE{Initialize Hessian $H$.}
    \STATE{Initialize $B\leftarrow$ a $k\times M$ matrix of zeros}
    \FOR{$i$ = $M$ \TO $1$}
      \STATE{$P_i\leftarrow$ set of column indices of non-zero
      elements in row $i$}
    \FOR{All $j$ in $P_i$}
    \STATE{$z\leftarrow Y[i,C[j]]/\delta - B[C[j], i]$}
    \STATE{$B[C[i], j]\leftarrow B[C[i], j] + z$}
    \STATE{$H[i,j]\leftarrow z$}
    \STATE{$H[j,i]\leftarrow H[i,j]$}
    \ENDFOR
    \ENDFOR
  \end{algorithmic}
  \caption{Triangular substitution method}\label{alg:subst}
\end{algorithm}









 \subsection{Note:  Who proved what}


 *** \Citet{ColemanMore1984} The ``no nonzero in same row'' rule for
the permuted lower triangular matrix is equivalent to the
\emph{intersection graph}.  Theorem 6.1 shows that a proper coloring of the
intersection graph of $L_\pi$ is also a triangular coloring of the
full symmetric graph.

Theorem 6.2 justifies the sequential smallest-last ordering.

This is what our algorithm does.




Consistent partitioning of $L$:  No column in the same group has a
non-zero element in the same row.

\Citet{PowellToint1979} showed that a consistent partitioning of $L$
allows for a substitution method (mentioned by
\citet{ColemanMore1984}.  This is a substitutable partition.

\Citet{PowellToint1979} show that the order in which variables are
assigned to partitions affects the partition.

\Citet{ColemanMore1984} characterize \cite{PowellToint1979} as a graph
coloring problem.  Also justify using the smallest-last ordering on the lower
triangle to color the variables



\Citet{ColemanCai1986}:  there are other substitutable partitions than
lower triangular.  Theorem 2.2.  A mapping induces a substitution
method if and only if the mapping is a cyclic coloring.

Cyclic coloring:  At least 3 colors in every cycle.

From \citet{ColemanCai1986}, general result of substitutable
(beyond \citet{PowellToint1979}.  Order nonzero elements
  $1\mathellipsis M$. If, for nonzero element $(i_m, j_m)$,


  \begin{enumerate}
\item  columns  $j_m$ and another other $j_{m'}$ are in the same
  group, and both $j_m$ and $j_{m'}$ have a nonzero in row $i_m$, then
  $i_{m'}, j_{m'}$ must be ordered before $i_m, j_m$; \emph{or}
\item  columns  $i_m$ and another other $i_{m'}$ are in the same
  group, and both $i_m$ and $i_{m'}$ have a nonzero in row $j_m$, then
  $j_{m'}, i_{m'}$ must be ordered before $j_m, i_m$
  \end{enumerate}

  What this means is that we can ignore column intersections that
  occur in lower rows (what?).

  The point is that lower triangular substitution qualifies, which
  means that we just need a cyclic coloring of the graph.


  Note:  My coloring algorithm is a smallest last ordering of the full
  symmetric Hessian  This is \emph{slpt} in \citet{ColemanMore1984}.
  What I should really do is a smallest-last on the lower triangle
  (\emph{slsl} in citet{ColemanMore1984}.  So I need to change that.

  In any event, it's still finding a cyclic coloring of the adjacency
  graph.  Just using a different heuristic for the coloring.

\printbibliography

\end{document}

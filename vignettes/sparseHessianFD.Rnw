\documentclass[10pt]{article}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{parskip}
\usepackage{setspace}
\linespread{1.1}

\usepackage[dvipsnames,svgnames,x11names,hyperref]{xcolor}
\usepackage[colorlinks=true, urlcolor=NavyBlue]{hyperref}

%%make knitr environment line spacing separate from LaTeX
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\Chol{Chol }
\DeclareMathOperator\cov{cov}

\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
\newcommand{\filename}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}

 \newcommand{\Prior}{\pi(\theta)}
 \newcommand{\Post}{\pi(\theta|y)}
 \newcommand{\Gtheta}{g(\theta)}
 \newcommand{\Phitheta}{\Phi(\theta|y)}
 \newcommand{\Ly}{\mathcal{L}(y)}
 \newcommand{\Dy}{\mathcal{D}(\theta,y)}


\usepackage[style=authoryear,%
			backend=biber,%
			maxbibnames=99,%
                        bibencoding=utf8,%
                        maxcitenames=1,
                        citetracker=true,
                        dashed=false,
                        maxalphanames=1,%
                        backref=false,%
			doi=true,%
                        url=true, %
			isbn=false,%
                        mergedate=basic,%
                        dateabbrev=true,%
                        natbib=true,%
                        uniquename=false,%
                        uniquelist=false,%
                        useprefix=true,%
                        firstinits=false
			]{biblatex}

 \addbibresource{~/OneDriveBusiness/References/Papers/braun_refs.bib}

 \setlength{\bibitemsep}{1em}
\AtEveryCitekey{\ifciteseen{}{\defcounter{maxnames}{2}}}
\DeclareFieldFormat[article,incollection,unpublished]{title}{#1} %No quotes for article titles
\DeclareFieldFormat[thesis]{title}{\mkbibemph{#1}} % Theses like book
                                % titles
\DeclareFieldFormat{pages}{#1} %% no pp prefix before page numbers
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{ % Don't print In: for journal articles
  \printtext{\bibstring{in}\intitlepunct}} %% but use elsewhere
}
\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \printfield{number}%
  \setunit{\addcomma\addspace}%
  \printfield{eid}}

\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{date}{#1}
\AtEveryBibitem{\clearfield{day}}

\renewbibmacro*{issue+date}{% print month only
  \printtext{%
    \printfield{issue}\addspace%
    \newunit%
%\printtext{\printfield{month}}%
}
  \newunit}

\renewbibmacro*{publisher+location+date}{% no print year
  \printlist{location}%
  \iflistundef{publisher}
    {\setunit*{\addcomma\space}}
    {\setunit*{\addcolon\space}}%
  \printlist{publisher}%
  \setunit*{\addcomma\space}%
%%\printdate
  \newunit}

\renewcommand*{\nameyeardelim}{~} %no comma in cite
\renewcommand{\bibitemsep}{1ex}
\def\bibfont{\small}

\title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
\author{Michael Braun\\Cox School of Business\\Southern Methodist University}
\date{April 12, 2015}

\begin{document}

\maketitle

<<setup1, echo=FALSE, cache=FALSE>>=
library(devtools)
library(methods, quietly = TRUE)
library(numDeriv, quietly = TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE, comment = "#",
                           message=FALSE, tidy=FALSE, cache = FALSE) #
@

The Hessian matrix of a log likelihood function or log posterior
density function plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  More broadly, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $p$ variables has $p^2$
elements, of which $p(p+1)/2$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the number of
decision variables.   For
functions with hundreds of thousands of variables, computing the
Hessian just once might not be practical for applications constrained
by time, storage or processor limitations.

However, for a large general class of problems, the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  Consider a log
posterior density of unknown parameters in a Bayesian hierarchical
model.  If the outcomes across heterogeneous units are conditionally
independent, the cross-partial derivatives with respect to those
parameters are zero.  As the number of units
increases, the size of the Hessian still grows quadratically, but the number
of \emph{non-zero} elements grows only linearly, and the Hessian
becomes increasingly sparse.  Under this hierarchical structure, we
know that parameters within a unit may be correlated, but parameters
across units are not.  Thus, we can determine exactly which elements
of the Hessian are non-zero.  The row and column indices of the
non-zero elements comprise the \emph{sparsity pattern} of the Hessian.

The \pkg{sparseHessianFD} package is a tool for estimating
sparse Hessians. The
method is efficient for models and datasets with a large number of
conditionally independent heterogeneous units, a small
number of parameters specific to each unit, and a relatively small
number of population-level parameters.  Such hierarchical models with
``high $N$, low $k$'' appear frequently in business-related fields like marketing
research, in which databases record transactions for a massive
number of households, but each household's activity is governed by a
small number of randomly distributed latent parameters.

\pkg{sparseHessianFD} estimates the Hessian numerically, through computing finite
differences of gradients. It exploits the sparsity of the Hessian by
partitioning decision variables into what we will colloquially call
``colors.''  Two variables can have the same color only if perturbing
their first derivatives simultaneously does


uses finite differencing (FD) to generate a numerical
estimate of the Hessian.  FD is a well-known method for estimating
derivatives of functions, and we explain it in Section \ref{sec:FD}.
Using FD, approximating the first derivative of $f(x)$ with respect to
$x_k$, for $k=1,...,K$ variables, is
conceptually simple:  the slope between $(x_k, f(x_k))$ and $(x_k+h,
f(x_k+h))$ approaches the derivate as $h$ gets arbitrarily close to
zero. The disadvantages are that the cost of FD grows with the number of
variables $K$, and that as $h$ approaches zero, the result is
increasingly unstable.

The problem is even worse when computing a
dense Hessian.  The $k^{th}$ column of the Hessian, $H_k$, can be approximated
by a difference in the gradients.  Let $e_k$ be the $k^{th}$
coordinate vector, so

\begin{align}
  H_k &\approx\frac{f'(x+he_k)-f'(x)}{h}
\end{align}

If $f'(x)$ is estimated using FD, then the complexity of estimating
the Hessian is quadratic, and numerical error from the gradient and
Hessian computations are combined.  In many cases, the gradient is
computed quickly and accurately by other means, such as through
deriving it analytically or using automatic differentiation (more on
that later).  In that case, numerical error comes from only the
Hessian computation, and complexity is only linear.

In order to justify the use of \pkg{sparseHessianFD}, we need to
maintain a set of assumptions about the underlying model, and the
workflow of the user.

\begin{enumerate}
\item The objective function is twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in an objective function that itself is hard to compute.
\item The gradient can be computed exactly (within machine precision,
  of course), and that computation time is a small, constant multiple of
  the time to compute the objective function, regardless of the number
  of variables.
\item Preferred alternatives to computing the Hessian are not
  available.  Examples include computing the Hessian directly from a
  symbolic derivation, or using AD software, as with the gradient.
  Direct computation of the Hessian will be numerically exact, but not necessarily
  easier or faster than \pkg{sparseHessianFD} when
  \pkg{sparseHessianFD} can exploit the sparsity in the Hessian.  AD
  can be used to compute the Hessian as well as the gradient, and many
  modern AD libraries are facile with sparse matrices.  But AD is not
  always available, hence the need for \pkg{sparseHessianFD}.
  \item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.  The user must be able to provide
    the row and column indices of the non-zero elements in the lower
    triangle of the Hessian.  This should be easy to do for
    hierarchical Bayesian models.
  \item The user can accept some small amount of approximation error.
    Since FD is a numerical approximation technique, the estimate of the Hessian
    will not be exact as either computing the Hessian directly from a
    symbolic derivation, or using AD.  By ``small,'' we mean a
    relative error roughly on the order of $10^{-6}$ or less.  In our
    experience should be widely achievable on double-precision machines.   If such errors are
    too large for a particular application, then \pkg{sparseHessianFD}
    may not be the best tool for the job. Exceptions
    would be cases of objective functions that are poorly conditioned,
    with ridges, plateaus or other pathologies, or cases for which
    such tiny errors will incur large costs.
\end{enumerate}


  The ``time to compute the gradient'' restriction is surprisingly weak,
  since XXX show that gradients computed via a reverse mode automatic
  differentiation meet this restriction.  When AD computation is not
  available, one would need to derive the gradient symbolically.


  \section{Background}

  \subsection{Numerical differentiation}


  The use of finite differences for approximating derivatives
  numerically starts with a Taylor series approximation to a
  scalar-valued function.  Because the highest-order derivative we
  will consider in this paper is the second, we will truncate our
  Taylor series there.

  \begin{align}
    \label{eq:1}
    f(x+h)&=f(x) + f'(x)h + \frac{1}{2}f''(x)h^2 + o(h^3)\\
  \end{align}
  After rearranging terms, we can see that we can approximate $f(x)$
  by choosing and arbitrarily small $h>0$.
  \begin{align}
    \label{eq:2}
    f'(x)&\approx\frac{f(x+h)-f(x)}{h}
  \end{align}

By differentiating the Taylor series, we see that we can approximate a
second derivative by estimate a finite difference of the first
derivative.

\begin{align}
  \label{eq:3}
  f'(x+h)&=f'(x) + f''(x)h + o(h^2)\\
 f''(x)&\approx\frac{f'(x+h)-f'(x)}{j}
\end{align}

If $x$ is a vector of length $K$, we can compute each element of the gradient of
$f(x)$ separately.  Suppose we can
compute partial derivatives $f'_k(x)$ exactly for all
$k=1\mathellipsis K$, and let $e_k$ be the $k^{th}$ coordinate vector. We can then approximate each column of the
Hessian via finite differencing of the gradients.
\begin{align}
  \label{eq:4}
  H_k&\approx\frac{f'(x + he_k)-f'(x)}{h}
\end{align}
Using forward differencing, estimating the entire Hessian requires $K+1$ evaluations of
the gradient.  This is why estimating a Hessian can be expensive for
large-scale problems.

However, if we know that some of the cross-partial derivatives are
zero, we can estimate the Hessian with fewer gradient evaluations.
Suppose $K=2$.  The Hessian matrix is

\begin{align}
  \label{eq:5}
  H=
  \begin{pmatrix}
    f'_1(x_1+h,x_2) -f'_1(x_1,x_2)& f'_1(x_1, x_2+h) -f'_1(x_1,x_2)\\
    f'_2(x_1+h, x_2) -f'_2(x_1,x_2) & f'_2(x_1, x_2+h) -f'_2(x_1,x_2)
    \end{pmatrix}            /h
\end{align}
Of course, the off-diagonal elements are equal.  If the cross-partial derivatives are zero, then
\begin{align}
  \label{eq:6}
   f'_1(x_1, x_2+h) -f'_1(x_1,x_2)&=0\\
  f'_2(x_1+h, x_2) -f'_2(x_1,x_2)&=0\\
\text{and}\\
     f'_1(x_1+h, x_2+h) -f'_1(x_1+h,x_2)&=0\\
  f'_2(x_1+h, x_2+h) -f'_2(x_1,x_2+h)&=0\\
\end{align}

By substitution,
\begin{align}
  \label{eq:6}
  H=
  \begin{pmatrix}
    f'_1(x_1+h,x_2+h) -f'_1(x_1,x_2)& 0\\
   0& f'_2(x_1+h, x_2+h) -f'_2(x_1,x_2)
    \end{pmatrix}            /h
\end{align}
A single evaluation of the gradient $f'(x_1+h, x_2+h)$, and a single
evaluation of the gradient at $f'(x_1, x_2)$, provide enough
information to extract the non-zero elements of the Hessian.  Reducing
the number of gradient evaluations from 3 to 2 depends on knowing that
the cross-partial derivatives are zero.


Now let's consider a more general case.  Suppose the Hessian has the
following sparsity pattern.

\begin{align}
  \label{eq:7}
  \begin{pmatrix}
    X&X&X&X&X\\
    X&X&X&0&0\\
    X&X&X&0&0\\
    X&0&0&X&X\\
    X&0&0&X&X
  \end{pmatrix}
\end{align}

First, note that because the Hessian is symmetric, we need to compute
only the elements in the lower triangle.  By perturbing the first
variable, we immediately get values for the first column of the
triangle. We could continue by perturbing each subsequent variable
separately, but what would happen if we perturbed variables 2 and 4 together?


The fundamental idea behind \pkg{sparseHessianFD} is that when some
elements of the Hessian are known to be zero, we do not need to
evaluate $K$ gradients by perturbing each variable one at a time.
If we can perturb several variables at once, we \emph{might} be able
to extract the full Hessian. But there are two difficult tasks at hand.
\begin{enumerate}
\item Which variables can we perturb together in a single gradient
  evaluation, and which need to be perturbed separately?
\item Once we get the gradient evaluations, how do we recover the Hessian?
\end{enumerate}









\section{Example function}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.
 Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities.  Furthermore, let $p_i$ be the probability of
 purchase; $p_i$ is the same for all $T$ opportunities, so we can
 treat $y_i$ as a binomial random variable.  The purchase probability
 $p_i$ is heterogeneous, and depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$, such that
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients can be thought of as sensitivities to the covariates, and they are distributed across the population of households following a multivariate normal distribution with mean $\mu$ and covariance $\Sigma$.   We assume that we know $\Sigma$, but we do not know $\mu$.  Instead, we place a multivariate normal prior on $\mu$, with mean $0$ and covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are $k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}

<<setup2, echo=FALSE>>=
load_all()
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

Since the $\beta_i$ are drawn iid from a multivariate normal,
$\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0$ for all $i\neq
j$.  We also know that all of the $\beta_i$ are correlated with
$\mu$.  The structure of the Hessian depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=\Sexpr{N}$ and $k=\Sexpr{k}$, then there are `\Sexpr{nv1}` total variables, and the Hessian will have the following pattern.

<<>>=
M <- as(kronecker(diag(N),matrix(1,k,k)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@


There are \Sexpr{nels1} elements in this symmetric matrix, but only \Sexpr{nnz1} are
non-zero, and only \Sexpr{nnz1LT} values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=\Sexpr{Q}$ instead.  In that case,
there are \Sexpr{nv2} variables in the problem, and more than $\Sexpr{floor(nels2/10^6)}$ million
elements in the Hessian.  However, only $\Sexpr{nnz2}$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only \Sexpr{nnz2LT} values.


Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "off-diagonal" sparsity pattern.

<<>>=
M <- as(kronecker(matrix(1,k,k), diag(N)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@

In both cases, the number of non-zeros is the same.  The
block-diagonal case may lead to slightly faster initialization,
but repeated computation of the Hessian will take the same time as
with the "off-diagonal" case.


\section{Using the package}

The functions for computing the objective function, gradient and
Hessian for this example are in the \filename{R/binary.R} file.  The package
also includes a sample dataset with simulated data from the binary choice example.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function.

<<>>=
set.seed(123)
data(binary)
str(binary)
N <- length(binary$Y)
k <- NROW(binary$X)
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
@

<<echo=FALSE>>=
options(scipen=-999)
@

This dataset represents the simulated choices for $N= \Sexpr{N}$ customers
over $T= \Sexpr{T}$ purchase opportunties, where the probability of purchase
is influenced by $k= \Sexpr{k}$ covariates.

The objective function for the binary choice example is \func{binary.f} and the gradient function is
\func{binary.grad}.  The first argument to both is the variable vector, and
the argument lists must be the same for both.  For this example, we
need to provide the data list ($X$, $Y$ and $T$) and the prior
parameter list
($\Sigma^{-1}$ and $\Omega^{-1}$). The functions also have an
`order.row` argument to change the ordering of the variables (and
thus, the sparsity pattern). If `order.row=TRUE`, then the Hessian
will have an off-diagonal pattern.  If `order.row=FALSE`, then the
Hessian will have a block-arrow pattern.

For testing and demonstration purposes, we also have a `binary.hess`
function that returns the Hessian as a sparse `dgCMatrix` object (see
the Matrix package).

<<>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two integer
vectors:  one each for the row and column indices of the non-zero
elements of the ***lower triangular part*** of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the following
convenience function to extract the appropriate index vectors.

<<>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

Next, we create a new instance of a sparseHessianFD object with an
"initial variable" $P$, and the row and column indices of the non-zero
elements in the lower triangle of the Hessian.  We also pass in any
other arguments for \func{binary.f} and \func{binary.grad}.  We accept the default
values for other arguments to `sparseHessianFD.new`.

<<>>=
obj <- new("sparseHessianFD", P, binary.f, binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors,
       order.row=FALSE)
@

Now we can evaluate the function value, gradient and Hessian through
`obj`.

<<>>=
f <- obj$fn(P)
gr <- obj$gr(P)
hs <- obj$hessian(P)
@

Note that the member functions in the sparseHessianFD class take only
one argument:  the variable vector.  All of the other arguments are
stored in `obj`.

Do we get the same results that we would get after calling \func{binary.f},
`binary.grad` and `binary.hess` directly?  Let's see.


<<>>=
all.equal(f, true.f)
all.equal(gr, true.grad)
all.equal(hs, true.hess)
@

If there is any difference, keep in mind that `hs` is a numerical
estimate that is not always exact.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.

\subsection{Speed comparison}

Instead of using this package, we could treat the Hessian as dense,
and use the hessian function numDeriv package. The advantage of the
numDeriv package is that it does not require the gradient.  However,
you can see that it takes some time to run.

<<echo=FALSE>>=
options(scipen=0)
@

<<>>=
hess.time <- system.time(H1 <- obj$hessian(P))
fd.time <- system.time(H2 <- hessian(obj$fn, P))
H2 <- drop0(H2, 1e-7) ## treat values < 1e-8 as zero
print(hess.time)
print(fd.time)
@

The sparseHessianFD package can be substantially faster than estimating a dense
Hessian by brute force finite differencing.  The cost of this speed up
is that the user does need to provide the gradient and the sparsity
structure.  As with everything in life, there are trade-offs.

\subsection{Quick summary}
In short, to use the package, follow the following steps:

\begin{enumerate}
\item Write R functions to return the value and gradient of the
objective function.
\item Determine the row and column indices of the non-zero elements of
the lower triangle of the Hessian.
\item Pick a variable vector (i.e., a starting value) at which you can
    initialize the sparseHessian object.  It doesn't really matter
    what this vector is, as long as the function value and gradient
    elements are all finite.
  \item Create a new sparseHessianFD object using the sparseHessianFD.new
function.  For this example, call that object F.
\item Compute Hessian at x by calling F\$hessian(x).
  \end{enumerate}

\end{document}

\documentclass[10pt]{article}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{parskip}
\usepackage{setspace}
\linespread{1.1}

\usepackage[dvipsnames,svgnames,x11names,hyperref]{xcolor}
\usepackage[colorlinks=true, urlcolor=NavyBlue]{hyperref}

%%make knitr environment line spacing separate from LaTeX
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\Chol{Chol }
\DeclareMathOperator\cov{cov}

\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
\newcommand{\filename}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}

 \newcommand{\Prior}{\pi(\theta)}
 \newcommand{\Post}{\pi(\theta|y)}
 \newcommand{\Gtheta}{g(\theta)}
 \newcommand{\Phitheta}{\Phi(\theta|y)}
 \newcommand{\Ly}{\mathcal{L}(y)}
 \newcommand{\Dy}{\mathcal{D}(\theta,y)}


\usepackage[style=authoryear,%
			backend=biber,%
			maxbibnames=99,%
                        bibencoding=utf8,%
                        maxcitenames=1,
                        citetracker=true,
                        dashed=false,
                        maxalphanames=1,%
                        backref=false,%
			doi=true,%
                        url=true, %
			isbn=false,%
                        mergedate=basic,%
                        dateabbrev=true,%
                        natbib=true,%
                        uniquename=false,%
                        uniquelist=false,%
                        useprefix=true,%
                        firstinits=false
			]{biblatex}

 \addbibresource{~/OneDriveBusiness/References/Papers/braun_refs.bib}

 \setlength{\bibitemsep}{1em}
\AtEveryCitekey{\ifciteseen{}{\defcounter{maxnames}{2}}}
\DeclareFieldFormat[article,incollection,unpublished]{title}{#1} %No quotes for article titles
\DeclareFieldFormat[thesis]{title}{\mkbibemph{#1}} % Theses like book
                                % titles
\DeclareFieldFormat{pages}{#1} %% no pp prefix before page numbers
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{ % Don't print In: for journal articles
  \printtext{\bibstring{in}\intitlepunct}} %% but use elsewhere
}
\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \printfield{number}%
  \setunit{\addcomma\addspace}%
  \printfield{eid}}

\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{date}{#1}
\AtEveryBibitem{\clearfield{day}}

\renewbibmacro*{issue+date}{% print month only
  \printtext{%
    \printfield{issue}\addspace%
    \newunit%
%\printtext{\printfield{month}}%
}
  \newunit}

\renewbibmacro*{publisher+location+date}{% no print year
  \printlist{location}%
  \iflistundef{publisher}
    {\setunit*{\addcomma\space}}
    {\setunit*{\addcolon\space}}%
  \printlist{publisher}%
  \setunit*{\addcomma\space}%
%%\printdate
  \newunit}

\renewcommand*{\nameyeardelim}{~} %no comma in cite
\renewcommand{\bibitemsep}{1ex}
\def\bibfont{\small}

\title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
\author{Michael Braun\\Cox School of Business\\Southern Methodist University}
\date{April 12, 2015}

\begin{document}

\maketitle

<<setup1, echo=FALSE, cache=FALSE>>=
library(devtools)
library(methods, quietly = TRUE)
library(numDeriv, quietly = TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE, comment = "#",
                           message=FALSE, tidy=FALSE, cache = TRUE) #
@

The Hessian matrix of a log likelihood function or log posterior
density function plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  More broadly, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $p$ variables has $p^2$
elements, of which $p(p+1)/2$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the number of
decision variables.   For
functions with hundreds of thousands of variables, computing the
Hessian just once might not be practical for applications constrained
by time, storage or processor limitations.

However, for a large general class of problems, the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  Consider a log
posterior density of unknown parameters in a Bayesian hierarchical
model.  If the outcomes across heterogeneous units are conditionally
independent, the cross-partial derivatives with respect to those
parameters are zero.  As the number of units
increases, the size of the Hessian still grows quadratically, but the number
of \emph{non-zero} elements grows only linearly, and the Hessian
becomes increasingly sparse.  Under this hierarchical structure, we
know that parameters within a unit may be correlated, but parameters
across units are not.  Thus, we can determine exactly which elements
of the Hessian are non-zero.  The row and column indices of the
non-zero elements comprise the \emph{sparsity pattern} of the Hessian.

The \pkg{sparseHessianFD} package is a tool for estimating
sparse Hessians. The
method is efficient for models and datasets with a large number of
conditionally independent heterogeneous units, a small
number of parameters specific to each unit, and a relatively small
number of population-level parameters.  Such hierarchical models with
``high $N$, low $k$'' appear frequently in business-related fields like marketing
research, in which databases record transactions for a massive
number of households, but each household's activity is governed by a
small number of randomly distributed latent parameters.

\pkg{sparseHessianFD} estimates the Hessian numerically, through computing finite
differences of gradients. It exploits the sparsity of the Hessian by
partitioning decision variables into what we will colloquially call
``colors.''  Two variables can have the same color only if perturbing
their first derivatives simultaneously does


uses finite differencing (FD) to generate a numerical
estimate of the Hessian.  FD is a well-known method for estimating
derivatives of functions, and we explain it in Section \ref{sec:FD}.
Using FD, approximating the first derivative of $f(x)$ with respect to
$x_k$, for $k=1,...,K$ variables, is
conceptually simple:  the slope between $(x_k, f(x_k))$ and $(x_k+h,
f(x_k+h))$ approaches the derivate as $h$ gets arbitrarily close to
zero. The disadvantages are that the cost of FD grows with the number of
variables $K$, and that as $h$ approaches zero, the result is
increasingly unstable.

The problem is even worse when computing a
dense Hessian.  The $k$th column of the Hessian, $H_k$, can be approximated
by a difference in the gradients.  Let $e_k$ be the $k$th
coordinate vector, so

\begin{align}
  H_k &\approx\frac{f'(x+\delta e_k)-f'(x)}{\delta}
\end{align}

If $f'(x)$ is estimated using FD, then the complexity of estimating
the Hessian is quadratic, and numerical error from the gradient and
Hessian computations are combined.  In many cases, the gradient is
computed quickly and accurately by other means, such as through
deriving it analytically or using automatic differentiation (more on
that later).  In that case, numerical error comes from only the
Hessian computation, and complexity is only linear.

In order to justify the use of \pkg{sparseHessianFD}, we need to
maintain a set of assumptions about the underlying model, and the
workflow of the user.

\begin{enumerate}
\item The objective function is twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in an objective function that itself is hard to compute.
\item The gradient can be computed exactly (within machine precision,
  of course), and that computation time is a small, constant multiple of
  the time to compute the objective function, regardless of the number
  of variables.
\item Preferred alternatives to computing the Hessian are not
  available.  Examples include computing the Hessian directly from a
  symbolic derivation, or using AD software, as with the gradient.
  Direct computation of the Hessian will be numerically exact, but not necessarily
  easier or faster than \pkg{sparseHessianFD} when
  \pkg{sparseHessianFD} can exploit the sparsity in the Hessian.  AD
  can be used to compute the Hessian as well as the gradient, and many
  modern AD libraries are facile with sparse matrices.  But AD is not
  always available, hence the need for \pkg{sparseHessianFD}.
  \item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.  The user must be able to provide
    the row and column indices of the non-zero elements in the lower
    triangle of the Hessian.  This should be easy to do for
    hierarchical Bayesian models.
  \item The user can accept some small amount of approximation error.
    Since FD is a numerical approximation technique, the estimate of the Hessian
    will not be exact as either computing the Hessian directly from a
    symbolic derivation, or using AD.  By ``small,'' we mean a
    relative error roughly on the order of $10^{-6}$ or less.  In our
    experience should be widely achievable on double-precision machines.   If such errors are
    too large for a particular application, then \pkg{sparseHessianFD}
    may not be the best tool for the job. Exceptions
    would be cases of objective functions that are poorly conditioned,
    with ridges, plateaus or other pathologies, or cases for which
    such tiny errors will incur large costs.
\end{enumerate}


  The ``time to compute the gradient'' restriction is surprisingly weak,
  since XXX show that gradients computed via a reverse mode automatic
  differentiation meet this restriction.  When AD computation is not
  available, one would need to derive the gradient symbolically.



  \section{Example function}



  \section{Background}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.
 Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities, and let $p_i$ be the probability of
 purchase.  The heterogeneous parameter $p_i$ is the same for all $T$ opportunities, so we can
 treat $y_i$ as a binomial random variable.  Define $p_i$ such that it depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$.
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients can be thought of as sensitivities to the covariates, and they are distributed across the population of households following a multivariate normal distribution with mean $\mu$ and covariance $\Sigma$.   We assume that we know $\Sigma$, but we do not know $\mu$.  Instead, we place a multivariate normal prior on $\mu$, with mean $0$ and covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are $k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}
We will return to this example throughout the article.


\subsection{Sparsity patterns}

<<setup2, echo=FALSE>>=
load_all()
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

The log posterior density in Equation \ref{eq:LPD} has a sparse
Hessian.  Since the $\beta_i$ are drawn iid from a multivariate
normal, and the $y_i$ are conditionally independent,
$\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0$ for all $i\neq
j$.  However, all of the $\beta_i$ are correlated with
$\mu$.

The sparsity pattern depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=\Sexpr{N}$ and $k=\Sexpr{k}$, then there are `\Sexpr{nv1}` total variables, and the Hessian will have the following pattern.

<<>>=
M <- as(kronecker(diag(N),matrix(1,k,k)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@

Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "off-diagonal" sparsity pattern.

<<>>=
M <- as(kronecker(matrix(1,k,k), diag(N)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@

In both cases, the number of non-zeros is the same.   There are \Sexpr{nels1} elements in this symmetric matrix, but only \Sexpr{nnz1} are
non-zero, and only \Sexpr{nnz1LT} values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=\Sexpr{Q}$ instead.  In that case,
there are \Sexpr{nv2} variables in the problem, and more than $\Sexpr{floor(nels2/10^6)}$ million
elements in the Hessian.  However, only $\Sexpr{nnz2}$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only \Sexpr{nnz2LT} values.



  \subsection{Numerical differentiation}


  The use of finite differences for approximating derivatives
  numerically starts with a Taylor series approximation to a
  scalar-valued function.  Because the highest-order derivative we
  will consider in this paper is the second, we will truncate our
  Taylor series there.

  \begin{align}
    \label{eq:1}
    f(x+h)&=f(x) + f'(x)\delta + \frac{1}{2}f''(x)\delta^2 + O(\delta^2)\\
  \end{align}

  \subsubsection{First order}


  After rearranging terms, we can see that we can approximate $f(x)$
  by choosing and arbitrarily small $h>0$.
  \begin{align}
    \label{eq:FD1}
    f'(x)&\approx\frac{f(x+\delta)-f(x)}{\delta}
  \end{align}

For a multivariate $x$, with length $K$, estimating $f'(x)$ involves
computing $f(x)$ once, and then Equation \ref{eq:FD1} $K$ times by
perturbing each of the elements of $x$ separately.

Error and complexity analysis here

\subsubsection{Second order}

  By differentiating the Taylor series, we see that we can approximate a
second derivative by computing a finite difference of the first
derivative.

\begin{align}
  \label{eq:3}
  f'(x+\delta)&=f'(x) + f''(x)\delta + O(\delta^2)\\
 f''(x)&\approx\frac{f'(x+\delta)-f'(x)}{\delta}
\end{align}

Suppose we can compute partial derivatives $f'_k(x)$ exactly for all
$k=1\mathellipsis K$, and let $e_k$ be the $k$th coordinate vector. We can then approximate each column of the
Hessian via finite differencing of the gradients.
\begin{align}
  \label{eq:4}
  H_k&\approx\frac{f'(x + \delta e_k)-f'(x)}{\delta}
\end{align}
Using forward differencing, estimating the entire Hessian requires $K+1$ evaluations of
the gradient, or the equivalent of $K^2+1$ function evaluations.  This
is another reason why estimating a Hessian can be expensive for
large problems.


Error and complexity analysis here


\subsection{Exploiting sparsity}

We can reduce the number of gradient evaluations if the Hessian is sparse.


If we know that some of the cross-partial derivatives are
zero, we \emph{might} be able to estimate the Hessian with fewer gradient evaluations.
Suppose $K=2$.  The Hessian matrix is

\begin{align}
  \label{eq:5}
  H=
  \begin{pmatrix}
    f'_1(x_1+\delta,x_2) -f'_1(x_1,x_2)& f'_1(x_1, x_2+\delta) -f'_1(x_1,x_2)\\
    f'_2(x_1+\delta, x_2) -f'_2(x_1,x_2) & f'_2(x_1, x_2+\delta) -f'_2(x_1,x_2)
    \end{pmatrix}            /\delta
\end{align}
Of course, the off-diagonal elements are equal.  If the cross-partial derivatives are zero, then
\begin{align}
  \label{eq:6}
   f'_1(x_1, x_2+\delta) -f'_1(x_1,x_2)&=0\\
  f'_2(x_1+\delta, x_2) -f'_2(x_1,x_2)&=0\\
\text{and, therefore,}\nonumber\\
     f'_1(x_1+\delta, x_2+\delta) -f'_1(x_1+\delta,x_2)&=0\\
  f'_2(x_1+\delta, x_2+\delta) -f'_2(x_1,x_2+\delta)&=0\\
\end{align}

By substitution,
\begin{align}
  \label{eq:6}
  H=
  \begin{pmatrix}
    f'_1(x_1+\delta,x_2+h) -f'_1(x_1,x_2)& 0\\
   0& f'_2(x_1+\delta, x_2+\delta) -f'_2(x_1,x_2)
    \end{pmatrix}            /\delta
\end{align}
A single evaluation of the gradient $f'(x_1+\delta, x_2+\delta)$, and a single
evaluation of the gradient at $f'(x_1, x_2)$, provide enough
information to extract the non-zero elements of the Hessian.  Reducing
the number of gradient evaluations from 3 to 2 depends on knowing that
the cross-partial derivatives are zero.

Now let's consider a general case, starting with a ``direct'' method
first proposed in \citet{CurtisPowellReid1974} for Jacobian matrices, and described in \citet{PowellToint1979}. To begin, partition the variables
into $G$ mutually exclusive groups, or ``colors,'' so $g_k$ indexes
the color of variable $k$. Let $D$ and $Y$ be $K\times G$
matrices, where $D_{kg}=\delta$ if variable $k$ belongs to group $g$, and zero
otherwise, and let $D_g$ be the $g$th column of $D$.  Each column in $Y$ is defined as
\begin{align}
  \label{eq:Yg}
  Y_g&=f'(x+D_g)-f'(x)
\end{align}

If $G=K$ and $g_k=k$, then $D$ is a diagonal matrix with $\delta$ in each
diagonal element.  The matrix equation $HD=Y$ represents the Taylor
series approximation
$H_{ik}\delta\approx y_k$, and we can solve for all elements of $H$
just by computing $Y$. But if $G<K$,
there must be at least one column of $D$ with $\delta$ in at least two
elements. Column $Y_g$ is computed by perturbing two variables at
once, and a particular $Y_{ig}$ would be equal to $\delta$ times a
\emph{sum} of multiple elements in row $H_i$.  Without further restrictions on values in
$H$, $HD=Y$ is an underdetermined system of linear equations, and we
would not be able to compute $H$ by computing a $Y$ with
fewer than $K$ columns.

The necessary restrictions on $H$ come from the sparsity pattern.  We
know which $h_{ik}=0$, so we can add those equations to $HD=Y$.  For
example, consider a function with the following Hessian.

\begin{align}
  \label{eq:7}
 H= \begin{pmatrix}
    h_{11}&0&h_{31}&0&0\\
    0&h_{22}&0&h_{42}&0\\
    h_{31}&0&h_{33}&0&h_{53}\\
    0&h_{42}&0&h_{44}&0\\
    0&0&h_{53}&0&h_{55}
  \end{pmatrix}
\end{align}
Note the subscripts on the elements take the symmetry of $H$ into account.

Suppose $G=2$, and define the colors through the following $D$ matrix.

\begin{align}
  \label{eq:7}
 D= \delta\begin{pmatrix}
   1&0\\
   1&0\\
   0&1\\
   0&1\\
   1&0
  \end{pmatrix}
\end{align}
Variables 1 and 2 have color 1, and variables 3, 4 and 5 have color
2.  For the moment, we will postpone the discussion of how to
choose $G$ and how to color the variables.

Next, compute the columns of $Y$ using Equation \ref{eq:Yg}.  We now
have the following system of linear equations from $HD=Y$.
\begin{align}
  \label{eq:11}
  \begin{aligned}[c]
  h_{11}&=y_{11}\\
  h_{22}&=y_{21}\\
  h_{31}+h_{53}&=y_{31}\\
  h_{42}&=y_{41}\\
  h_{55}&=y_{51}\\
  \end{aligned}
  \qquad
  \begin{aligned}[c]
  h_{31}&=y_{12}\\
  h_{42}&=y_{22}\\
  h_{33}&=y_{32}\\
  h_{44}&=y_{42}\\
  h_{53}&=y_{52}
  \end{aligned}
\end{align}

Note that this system is overdetermined; \citet{CurtisPowellReid1974} did
not assume that their Jacobian is symmetric.  Both $h_{31}=y_{12}$ and $h_{53}=y_{52}$
can be determined directly, but $h_{31}+h_{53}=y_{31}$, and $h_{42}$
could be either $y_{41}$ or $y_{22}$.  \Citet{PowellToint1979} prove
that it is sufficient to solve $LD=Y$ via a substitution method, where $L$ is the lower
triangular part of $H$.  This has the effect of removing the equations
$h_{42}=y_{22}$ and $h_{31}=y_{12}$ from the system, but retaining
$h_{53}=y_{52}$.  We can then solve for
$h_{31}=y_{31}-h_{53}=y_{31}-y_{52}$. Thus, we have determined a
$5\times 5$ Hessian matrix with only three gradient evaluations, in
contrast with the six that would have been needed had $H$ been treated
as dense.

Solving $HD=Y$, using the full Hessian is known as a ``direct''
method, because the elements of the Hessian can be determined directly
from $Y$, as long as an appropriate coloring of the variables is
used.  Solving $LD=Y$ is known as either an ``indirect,'' or
``triangular substitution'' method, for obvious reasons.  The
substitution method is often preferred over direct methods for
symmetric matrices because we can partition the
variables with a lower $G$, and estimate the Hessian with fewer
evaluations of the gradient.
Direct methods will not necessarily be able to exploit the fact that
the Hessian is symmetric. This is particularly true for banded
Hessians, which was described by \citet{ColemanMore1984}, and reveal
in the preceding example.  Therefore, we will restrict our attention
in the subsequent sections to triangular substitution methods.
to

\subsection{Coloring the variables}

Both \citet{CurtisPowellReid1974} (for direct methods) and
\citet{PowellToint1979} (for substitution methods) propose the
following rule for partitioning the variables:  two variables cannot
have the same color if their respective columns of the Hessian
(direct) or lower triangle of the Hessian (substitution) have a
non-zero element in the same row.  In the example above, variables 1
and 3 cannot be in the same group because columns 1 and 3 both have
non-zero elements in row 3.  But this is not a positive rule.  It
tells us how to exclude certain partitons, but not how to find one
that is optimal, with the fewest possible number of colors.

\Citet{ColemanMore1984} were the first to recognize that this partitioning problem
is analogous to the famous graph coloring problems in
graph theory (finally, we learn why we use the term ``color'' to
describe a partition). We can think of the sparsity pattern of $L$
as an adjacency matrix in an undirected graph.  If $h_{i,j}\neq 0$ ,
then variables $i$ and $j$ are ``neighbors'' in the graph, and they
cannot have the same color.  Now, let's introduce a new variable $k$,
where $j$ and $k$ are neighbors, but $i$ and $k$ are not.  The colors
of $i$ and $k$ still have to be different, because they have a common
neighbor in $j$.

For a partition to be compatible with estimating a Hessian via
substitution, it is sufficient, but not necessary, that no variable
have the same color as any other variable with which it shares a
common neighbor.  That is, no variable within two ``steps'' on the
undirected graph may have the same color.  But in the example above,
variables 1 and 5 have the same color, even though 1 is connected to
3, and 3 is connected to 5.  \Citet{ColemanCai1986} prove that the
rule that no same-colored columns may have non-zero elements in the
same row is equivalent to an ``acyclic'' graph coloring scheme, and
that coloring the variables in this way is also sufficient for
estimating a sparse Hessian.  Thus, as \citet{GebremedhinTarafdar2009}
point out, minimizing the number of partitions is equivalent to
minimizing the number of colors in an acyclic graph.

Define acyclic.



\section{Example function}



\section{Using the package}

\subsection{The \class{sparseHessianFD} class}

The package functionality is implemented as a reference class
\class{sparseHessianFD}.  The initializer takes the following arguments.
\begin{itemize}
\item[x.init] A numeric vector of variables at which the object will be
  initialized and tested.  It is not stored in the object, so it can
  really be any value, as long as the objective function, gradient and
  Hessian are all finite.
\item[fn,gr] \proglang{R} functions that return the value of the
  objective function, and its gradient. The first argument is the numeric
  variable vector.  Other arguments can be passed as ... .
\item[rows, cols] Integer vectors of the row and column indices of
  the non-zero elements in the lower triangle of the Hessian.
\item[direct] This argument is deprecated, and is included only for
  backwards compatibility with earlier versions.
\item[eps] The perturbation amount for finite differencing of the
  gradient to compute the Hessian. Defaults to
  sqrt(.Machine\$double.eps)
\item [index1] If TRUE (the default), \funcarg{row} and \funcarg{col} use one-based
  indexing.  If FALSE, zero-based indexing is used (which is the
  internal storage format for matrix classes in the \pkg{Matrix}
  package).
\item [...] Additional arguments to be passed to \func{fn} and \func{gr}.
\end{itemize}

To create a \class{sparseHessianFD} object, just call
\func{sparseHessianFD}.  If you are accepting all of the default
arguments, and not passing additional arguments to \func{fn} and
\func{gr}, the call will look like:

<<eval=FALSE>>=
obj <- sparseHessianFD(x.init, fn, gr, rows, cols)
@

The class defines a number of different fields, none of which should
be accessed directly.    The initializer automatically calls the graph
coloring subroutine, and evaluates the Hessian at $x$, so it may take
some time to create the object.


\subsection{Evaluating the Hessian}

The \func{fn}, \func{gr} and \func{hessian} methods respectively evaluate the
function, gradient and Hessian at a variable vector $x$.

<<eval=FALSE>>=
f <- obj$fn(x)
df <- obj$gr(x)
hess <- obj$hessian(x)
@

The \func{fn} and \func{gr} methods are simply closures around the
functions that were provided to the class initializer.  Since the
additional arguments were already supplied as \texttt{...}, they do
not need to be supplied again.  This feature makes subsequent calling
of \func{fn} and \func{gr} simpler, because only the variable is
included in the call.

Similarly, the \func{hessian} method takes the single argument $x$.
The return value is always a \class{dgCMatrix} object (defined in the
\pkg{Matrix} package).  \class{dgCMatrix} objects are sparse matrices,
stored in a compressed, column-oriented format, and includes all
non-zero elements in both the upper and lower triangles.  One could
coerce the Hessian into a symmetric \class{dsCMatrix} if necessary.

The \class{sparseHessianFD} class also provides methods \func{fngr}
and \func{fngrhs} that return the function, gradient and possibly the
Hessian, in a single list.


\subsection{Providing the sparsity pattern}

The sparsity pattern of the Hessian is defined as the row and column
indices of the non-zero elements in the \emph{lower triangle} the Hessian.  Internally, this
pattern is stored in a compressed format, but the
\class{sparseHessianFD} initializer requires rows and columns, to keep
things simple.  It is the responsibility of the user to ensure that
the sparsity pattern is correct. Any elements in the upper triangle
will be automatically removed, but there is no check that a
corresponding element in the lower triangle exists.

The \func{Matrix.to.Coord} function extracts
row and column indices from a sparse matrix.  The argument $M$ is a
matrix that could be coerced to a \pkg{Matrix} object that is derived
from the \class{TsparseMatrix} class (a virtual class that defines
sparse matrices stored in row-column format).  Standard base
\proglang{R} matrices, and most \pkg{Matrix} matrices, fall into this
category.  If the \funcarg{index1} argument is \code{TRUE} (the
default), then \func{Matrix.to.Coord} returns 1-based indexes.

The input matrix to \func{Matrix.to.Coord} does not have to include
the values (if the full Hessian were known, that would possibly defeat the
purpose of this package).  It is sufficient to supply a logical
or pattern matrix, such as \class{lgCMatrix} or \class{ngCMatrix}.
Rather than trying to keep track of the row and column indices
directly, it might be easier to construct a pattern matrix first,
check visually that the matrix has the right pattern, and then extract
the indices.

The following code constructs a block diagonal matrix, and extracts
the sparsity pattern from its lower triangle.

<<exPattern>>=
M <- as(kronecker(Diagonal(3), Matrix(T,2,2)),"nMatrix")
M
tril(M)
mc <- Matrix.to.Coord(tril(M))
mc
@
We then use \funcarg{mc\$row} and \funcarg{mc\$col} to construct the
\class{sparseHessianFD} object.

To visually check that a proposed sparsity pattern represents the
intended matrix, use the \func{Coord.to.Pattern.Matrix} function,
which is just a wrapper to \pkg{Matrix}'s \func{sparseMatrix} constructor.

<<>>=
M2 <- Coord.to.Pattern.Matrix(mc$rows, mc$cols, dims=dim(M))
M2
@


One could


The functions for computing the objective function, gradient and
Hessian for this example are in the \filename{R/binary.R} file.  The package
also includes a sample dataset with simulated data from the binary choice example.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function.

<<>>=
set.seed(123)
data(binary_small)
binary <- binary_small
str(binary)
N <- length(binary$Y)
k <- NROW(binary$X)
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
@

<<echo=FALSE>>=
options(scipen=-999)
@

This dataset represents the simulated choices for $N= \Sexpr{N}$ customers
over $T= \Sexpr{T}$ purchase opportunties, where the probability of purchase
is influenced by $k= \Sexpr{k}$ covariates.

The objective function for the binary choice example is \func{binary.f} and the gradient function is
\func{binary.grad}.  The first argument to both is the variable vector, and
the argument lists must be the same for both.  For this example, we
need to provide the data list ($X$, $Y$ and $T$) and the prior
parameter list
($\Sigma^{-1}$ and $\Omega^{-1}$). The functions also have an
`order.row` argument to change the ordering of the variables (and
thus, the sparsity pattern). If `order.row=TRUE`, then the Hessian
will have an off-diagonal pattern.  If `order.row=FALSE`, then the
Hessian will have a block-arrow pattern.

For testing and demonstration purposes, we also have a `binary.hess`
function that returns the Hessian as a sparse `dgCMatrix` object (see
the Matrix package).

<<>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two integer
vectors:  one each for the row and column indices of the non-zero
elements of the ***lower triangular part*** of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the following
convenience function to extract the appropriate index vectors.

<<>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

Next, we create a new instance of a sparseHessianFD object with an
"initial variable" $P$, and the row and column indices of the non-zero
elements in the lower triangle of the Hessian.  We also pass in any
other arguments for \func{binary.f} and \func{binary.grad}.  We accept the default
values for other arguments to `sparseHessianFD.new`.

<<>>=
obj <- new("sparseHessianFD", P, binary.f, binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors,
       order.row=FALSE)
@

Now we can evaluate the function value, gradient and Hessian through
`obj`.

<<>>=
f <- obj$fn(P)
gr <- obj$gr(P)
hs <- obj$hessian(P)
@

Note that the member functions in the sparseHessianFD class take only
one argument:  the variable vector.  All of the other arguments are
stored in `obj`.

Do we get the same results that we would get after calling \func{binary.f},
`binary.grad` and `binary.hess` directly?  Let's see.


<<>>=
all.equal(f, true.f)
all.equal(gr, true.grad)
all.equal(hs, true.hess)
@

If there is any difference, keep in mind that `hs` is a numerical
estimate that is not always exact.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.

\subsection{Speed comparison}

Instead of using this package, we could treat the Hessian as dense,
and use the hessian function numDeriv package. The advantage of the
numDeriv package is that it does not require the gradient.  However,
you can see that it takes some time to run.

<<echo=FALSE>>=
options(scipen=0)
@

<<>>=
hess.time <- system.time(H1 <- obj$hessian(P))
fd.time <- system.time(H2 <- hessian(obj$fn, P))
H2 <- drop0(H2, 1e-7) ## treat values < 1e-8 as zero
print(hess.time)
print(fd.time)
@

The sparseHessianFD package can be substantially faster than estimating a dense
Hessian by brute force finite differencing.  The cost of this speed up
is that the user does need to provide the gradient and the sparsity
structure.  As with everything in life, there are trade-offs.

\subsection{Quick summary}
In short, to use the package, follow the following steps:

\begin{enumerate}
\item Write R functions to return the value and gradient of the
objective function.
\item Determine the row and column indices of the non-zero elements of
the lower triangle of the Hessian.
\item Pick a variable vector (i.e., a starting value) at which you can
    initialize the sparseHessian object.  It doesn't really matter
    what this vector is, as long as the function value and gradient
    elements are all finite.
  \item Create a new sparseHessianFD object using the sparseHessianFD.new
function.  For this example, call that object F.
\item Compute Hessian at x by calling F\$hessian(x).
  \end{enumerate}

\end{document}

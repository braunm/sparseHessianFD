\documentclass[10pt]{article}


%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{sparseHessianFD}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{placeins} %% for \FloatBarrier
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{colortbl}
\usepackage{parskip}
\usepackage{setspace}
\linespread{1.1}


\usepackage[colorlinks=true, urlcolor=NavyBlue]{hyperref}

%%make knitr environment line spacing separate from LaTeX
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}

\DeclareMathOperator\logit{logit}
\DeclareMathOperator\Chol{Chol }
\DeclareMathOperator\cov{cov}

\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\class}[1]{\texttt{#1}}
\newcommand{\funcarg}[1]{\texttt{#1}}
\newcommand{\filename}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\variable}[1]{\texttt{#1}}
\newcommand{\method}[1]{\textsl{#1}}

 \newcommand{\Prior}{\pi(\theta)}
 \newcommand{\Post}{\pi(\theta|y)}
 \newcommand{\Gtheta}{g(\theta)}
 \newcommand{\Phitheta}{\Phi(\theta|y)}
 \newcommand{\Ly}{\mathcal{L}(y)}
 \newcommand{\Dy}{\mathcal{D}(\theta,y)}
 \newcommand{\df}[3]{\mathsf{d}^{#1}f(#2;#3)}
 \newcommand{\parD}[3]{\mathsf{D}^{#1}_{#2}f(#3)}


\usepackage[style=authoryear,%
			backend=biber,%
			maxbibnames=99,%
                        bibencoding=utf8,%
                        maxcitenames=1,
                        citetracker=true,
                        dashed=false,
                        maxalphanames=1,%
                        backref=false,%
			doi=true,%
                        url=true, %
			isbn=false,%
                        mergedate=basic,%
                        dateabbrev=true,%
                        natbib=true,%
                        uniquename=false,%
                        uniquelist=false,%
                        useprefix=true,%
                        firstinits=false
			]{biblatex}

 \addbibresource{~/OneDriveBusiness/References/Papers/braun_refs.bib}

 \setlength{\bibitemsep}{1em}
\AtEveryCitekey{\ifciteseen{}{\defcounter{maxnames}{2}}}
\DeclareFieldFormat[article,incollection,unpublished]{title}{#1} %No quotes for article titles
\DeclareFieldFormat[thesis]{title}{\mkbibemph{#1}} % Theses like book
                                % titles
\DeclareFieldFormat{pages}{#1} %% no pp prefix before page numbers
\renewbibmacro{in:}{%
  \ifentrytype{article}{}{ % Don't print In: for journal articles
  \printtext{\bibstring{in}\intitlepunct}} %% but use elsewhere
}
\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \printfield{number}%
  \setunit{\addcomma\addspace}%
  \printfield{eid}}

\DeclareFieldFormat[article]{volume}{\mkbibbold{#1}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}
\DeclareFieldFormat[article]{date}{#1}
\AtEveryBibitem{\clearfield{day}}

\renewbibmacro*{issue+date}{% print month only
  \printtext{%
    \printfield{issue}\addspace%
    \newunit%
%\printtext{\printfield{month}}%
}
  \newunit}

\renewbibmacro*{publisher+location+date}{% no print year
  \printlist{location}%
  \iflistundef{publisher}
    {\setunit*{\addcomma\space}}
    {\setunit*{\addcolon\space}}%
  \printlist{publisher}%
  \setunit*{\addcomma\space}%
%%\printdate
  \newunit}

\renewcommand*{\nameyeardelim}{~} %no comma in cite
\renewcommand{\bibitemsep}{1ex}
\def\bibfont{\small}

\title{\pkg{sparseHessianFD}: An \proglang{R} Package for Estimating Sparse Hessian Matrices}
\author{Michael Braun\\Cox School of Business\\Southern Methodist University}
\date{April 12, 2015}
%\usepackage[dvipsnames,svgnames,x11names,hyperref]{xcolor}



\begin{document}

\maketitle

<<setup1, echo=FALSE, cache=FALSE>>=
library(devtools)
library(methods, quietly = TRUE)
library(numDeriv, quietly = TRUE)
knitr::opts_chunk[["set"]](collapse = FALSE, eval=TRUE, echo=TRUE,
                          message=FALSE, tidy=FALSE, cache = TRUE)
@

The Hessian matrix of a log likelihood function or log posterior
density function plays
an important role in statistics.  From a frequentist point of view,
the inverse of the negative Hessian is the asymptotic covariance of the sampling distribution of a maximum
likelihood estimator.  In Bayesian analysis, when evaluated at the
posterior mode, it is the covariance of a Gaussian approximation to
the posterior distribution.  More broadly, many numerical optimization
algorithms require repeated computation, estimation or approximation
of the Hessian or its inverse; see \citet{NocedalWright2006}.

The Hessian of an objective function with $K$ variables has $K^2$
elements, of which $\binom{K}{2}+K$ are unique.  Thus, the storage
requirements of the Hessian, and computational cost of many linear
algebra operations on it, grow quadratically with the number of
decision variables.   For
functions with hundreds of thousands of variables, computing the
Hessian even once might not be practical for applications constrained
by time, storage or processor availability.

For many problems, the Hessian is
\emph{sparse}, meaning that the proportion of non-zero elements in the
Hessian is small.  Consider a log
posterior density in a Bayesian hierarchical
model.  If the outcomes across heterogeneous units are conditionally
independent, the cross-partial derivatives with respect to those
parameters are zero.  As the number of units
increases, the size of the Hessian still grows quadratically, but the number
of \emph{non-zero} elements grows only linearly, and the Hessian
becomes increasingly sparse.  Under this hierarchical structure,
parameters within a unit may be correlated, but parameters
across units are not.  The row and column indices of the
non-zero elements comprise the \emph{sparsity pattern} of the Hessian.

The \pkg{sparseHessianFD} package is a tool for estimating
sparse Hessians using \emph{finite differencing}.  Section XX will
cover the specifics, but the basic idea is as follows.  Consider a
function $f(x)$ and its derivative $Df(x)$.  Let $e_k$ be the $k$th
coordinate vector, and let $\delta$ be a sufficiently small scalar
constant. The vector $H_kf(x)=\left(Df(x+\delta e_k) - Df(x)\right)/\delta$ is
a linear approximation of the $k$th column of the Hessian matrix
$Hf(x)$. Estimating a dense Hessian involves $K+1$ calculations of the
derivative: one for the derivative at $x$, and $K$ after perturbing each
of the elements of $x$ one at a time.  However, if the Hessian is sufficiently sparse, we can perturb
more than one element of $x$ at a time, and still recover the non-zero
Hessian values with fewer than $K$ derivative
evaluations.  Not all sparsity patterns allow for fewer than $K$
perturbations, but for others, exploiting sparsity can be profoundly
efficient.  In fact, for the hierarchical models that we consider in
this paper, the number of derivative evaluations is \emph{constant},
even as additional heterogeneous units are added to the model.

%% At the outset, we have to mention that there may be some applications for
%% which \pkg{sparseHessianFD} is not an appropriate package to use. To
%% extract the maximum benefit from using \pkg{sparseHessianFD}, we need
%% to accept a few conditions or assumptions. $\alpha$.

\begin{enumerate}
\item The objective function $f(x)$ should be twice differentiable, and can be
  computed ``quickly and easily,'' even for a large number of
  variables.  We leave the definition of ``quickly and easily''
  intentionally murky, since no method of differentiation can overcome
  pathologies in an objective function that itself is hard to compute.
\item The gradient (the transpose of the first derivative vector) $Df(x)^\top$ can
  be computed quickly, easily and \emph{exactly} (within machine
  precision, of course).  This means that while we are using finite
  differencing to estimate the Hessian matrix, we should not use it to
  compute the gradient.  The time required to finite-difference
  $Df(x)$ grows with $K$, while for algorithmic differentiation, the
  time is a small constant multiple of the time required to compute
  $f(x)$.  Also, the estimate of each $H_kf(x)$ would be a
  finite difference of finite differences, and the approximation
  error would be compounded.
\item Preferred alternatives to computing the Hessian are not
  available.  Finite differencing is not generally a ``first choice''
  method.  Deriving a gradient or Hessian symbolically, and writing a
  subroutine to compute it, will give an exact answer, but might be
  tedious or difficult to implement. Algorithmic differentiation is
  probably the most efficient method, but requires specialized
  libraries that, at this moment, are not yet available in \proglang{R}.
 \pkg{sparseHessianFD} makes the most sense when the gradient is easy to get, but the Hessian is
  not.
\item The sparsity pattern is known in advance, and does not depend
    on the values of the variables.
\end{enumerate}


  \section{Background}

Before going into the details of how to use the package, let's
consider the following example of an objective function with a sparse
Hessian.  Suppose we have a dataset of $N$ households, each with $T$
 opportunities to purchase a particular product.  Let $y_i$ be the
 number of times household $i$ purchases the product, out of the $T$
 purchase opportunities, and let $p_i$ be the probability of
 purchase.  The heterogeneous parameter $p_i$ is the same for all $T$
 opportunities, so $y_i$ is a binomial random variable.  Define each $p_i$ such that it depends on both $k$ continuous covariates
 $x_i$, and a heterogeneous coefficient vector $\beta_i$.
\begin{align}
  p_i=\frac{\exp(x_i'\beta_i)}{1+\exp(x_i'\beta_i)},~i=1 ... N
\end{align}

The coefficients are distributed across the population of households
following a multivariate normal distribution with mean $\mu$ and
covariance $\Sigma$.   Assume that we know $\Sigma$, but not $\mu$.
Instead, place a multivariate normal prior on $\mu$, with mean $0$ and
covariance $\Omega_0$.  Thus, each $\beta_i$, and $\mu$ are
$k-$dimensional vectors, and the total number of unknown variables in the model is $(N+1)k$.

The log posterior density, ignoring any normalization constants, is
\begin{align}
  \label{eq:LPD}
  \log \pi(\beta_{1:N},\mu|Y, X, \Sigma_0,\Omega_0)=\sum_{i=1}^Np_i^{y_i}(1-p_i)^{T-y_i}
  -\frac{1}{2}\left(\beta_i-\mu\right)'\Sigma^{-1}\left(\beta_i-\mu\right)
-\frac{1}{2}\mu'\Omega_0^{-1}\mu
\end{align}
We will return to this example throughout the article.


\subsection{Sparsity patterns}

<<setup2, echo=FALSE>>=
load_all()
N <- 6
k <- 2
nv1 <- (N+1)*k
nels1 <- nv1^2
nnz1 <- (N+1)*k^2 + 2*N*k^2
nnz1LT <- (N+1)*k*(k+1)/2 + N*k^2
Q <- 1000
nv2 <- (Q+1)*k
nels2 <- nv2^2
nnz2 <- (Q+1)*k^2 + 2*Q*k^2
nnz2LT <- (Q+1)*k*(k+1)/2 + Q*k^2
options(scipen=999)
@

The log posterior density in Equation \ref{eq:LPD} has a sparse
Hessian.  Since the $\beta_i$ are drawn iid from a multivariate
normal, and the $y_i$ are conditionally independent,
$H_{ij}=\dfrac{\partial^2\log\pi }{\partial\beta_i\beta_j}=0$ for all $i\neq
j$.  However, all of the $\beta_i$ are correlated with
$\mu$.

The sparsity pattern depends on how the variables are
ordered within the vector. One such ordering is to group all of the
coefficients for each unit together.

\begin{align}
\beta_{11},...,\beta_{1k},\beta_{21},...,\beta_{2k},...~,~...~,~\beta_{N1}~,~...~,~\beta_{Nk},\mu_1,...,\mu_k
\end{align}

In this case, the Hessian has a "block-arrow" structure.  For example,
if $N=\Sexpr{N}$ and $k=\Sexpr{k}$, then there are `\Sexpr{nv1}` total variables, and the Hessian will have the following pattern.

<<>>=
M <- as(kronecker(diag(N),matrix(1,k,k)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@

Another possibility is to group coefficients for each covariate
together.

\begin{align}
\beta_{11},...,\beta_{1N},\beta_{21},...,\beta_{2N},...,...,\beta_{k1},...,\beta_{kN},\mu_1,...,\mu_k
\end{align}

Now the Hessian has an "off-diagonal" sparsity pattern.

<<>>=
M <- as(kronecker(matrix(1,k,k), diag(N)),"lMatrix")
M <- rBind(M, Matrix(TRUE,k,N*k))
M <- cBind(M, Matrix(TRUE, k*(N+1), k))
print(M)
@

In both cases, the number of non-zeros is the same.   There are \Sexpr{nels1} elements in this symmetric matrix, but only \Sexpr{nnz1} are
non-zero, and only \Sexpr{nnz1LT} values are unique.  Although the reduction in
RAM from using a sparse matrix structure for the Hessian may be
modest, consider what would happen if $N=\Sexpr{Q}$ instead.  In that case,
there are \Sexpr{nv2} variables in the problem, and more than $\Sexpr{floor(nels2/10^6)}$ million
elements in the Hessian.  However, only $\Sexpr{nnz2}$ of those elements are
non-zero.  If we work with only the lower triangle of the Hessian we only need to work with
only \Sexpr{nnz2LT} values.



  \subsection{Numerical differentiation}


  In this section, we review the theoretical basis for approximating
  derivatives using finite differences.  To facilitate this
  discussion, we use the notation of \citet{MagnusNeudecker2007}, and
  their notion of the differential, but we will not be as formal in
  our proofs.

  Let $f(x)$ be a scalar-valued function, and let $x$ and $u$ be
  $K$-dimensional vectors.  Let $u$ be a sufficiently small positive
  real value.  If $k=1$, then the definition of the first \emph{derivative} of $f(x)$ is
  \begin{align}
    \label{eq:9}
    f'(x)=\lim\limits_{u\to 0}\frac{f(x+u)-f(x)}{u}
  \end{align}

which is equivalent to
\begin{align}
  \label{eq:10}
  f(x+u)=f(x)+uf'(x)+o(u)
\end{align}

For $K\ge 1$, \citet{MagnusNeudecker2007} define the first \emph{differential} of
$f(x)$, with increment $u$, as
\begin{align}
  \label{eq:12}
  \df{}{x}{u}=uf'(x)
\end{align}
The differential $\df{}{x}{u}$ is the linear component of the difference between
$f(x+u)$ and $f(x)$, and $o(u)$ is a error term that goes to zero as
$\|u\|\to 0$.

The \emph{partial} derivative of $f(x)$ with respect to $x_j$ (the $j$th
component of $x$) is defined as
\begin{align}
  \label{eq:13}
\parD{}{j}{x}=\lim\limits_{t\to 0}\frac{f(x+te_j)-f(x)}{t}
\end{align}
with $\parD{}{}{x}=\left(\parD{}{1}{x},\mathellipsis,\parD{}{K}{x}\right)$ as
the vector of all partial derivatives.

By the \citet{MagnusNeudecker2007} ``First Identification Theorem'',
$\df{x}{u}=\left(\parD{}{}{x}\right)u$, for
$x,u\in\mathbb{R}^K$. Thus, as $\|u\|\to 0$, we can solve for each
partial derivative by computing a finite difference between $f(x)$ and
$f(x+u_je_j)$ and dividing by $u_j$.
The \emph{gradient} is defined
as $\nabla f(x)=\left(\parD{}{}{x}\right)^\top$.

The second-order partial derivative is defined as
\begin{align}
  \label{eq:14}
  \parD{2}{jk}{x}=\lim\limits_{t\to 0}\frac{\parD{}{j}{x+te_k}-\parD{}{j}{x}}{t}
\end{align}
and the Hessian matrix is defined as
\begin{align}
  \label{eq:15}
  \mathsf{H}f(x)=
  \begin{pmatrix}
    \parD{2}{11}{x}&  \parD{2}{12}{x}&  \mathellipsis &  \parD{2}{1K}{x}\\
    \parD{2}{21}{x}&  \parD{2}{22}{x}&  \mathellipsis &  \parD{2}{2K}{x}\\
    \vdots&\vdots&&\vdots\\
    \parD{2}{K1}{x}&  \parD{2}{K2}{x}&  \mathellipsis &  \parD{2}{KK}{x}
    \end{pmatrix}
\end{align}


A second-order approximation to $f(x)$ is
\begin{align}
  \label{eq:16}
  f(x+u)=f(x)+\left(\parD{}{}{x}\right)u+\frac{1}{2}u^\top \mathsf{H}f(x)u+o\left(\|u\|^2\right).
\end{align}

Differentiating this approximation,

\begin{align}
  \label{eq:16a}
  \parD{}{}{x+u}=\parD{}{}{x}+\left(\parD{}{}{x}\right)u+\frac{1}{2}u^\top \mathsf{H}f(x)u+o\left(\|u\|^2\right).
\end{align}


The second differential is the differential of the first differential.
\begin{align}
  \label{eq:17}
  \mathsf{d}^2f(x;u)&=\mathsf{d}\left(\mathsf{d}f\left(x;u\right);u\right)\\
 &=\mathsf{d}\left(\parD{}{}{x}u\right);u\\
 &=\mathsf{D}\left(\parD{}{}{x}u\right)u\\
 &=\mathsf{D}\left(\sum_{j=1}^K\parD{}{j}{x}u_j\right)u\\
 &=\sum_{i,j=1}^Ku_iu_j\parD{2}{ij}{x}\\
 &=u'\left(\mathsf{H}f(x)\right)u
\end{align}

The ``Second Identification Theorem'' in \citet{MagnusNeudecker2007}
states that, for $x,u\in\mathbb{R}^K$,
\begin{align}
  \label{eq:ID2}
  f(x+u)=f(x)+\df{}{x}{u}+\frac{1}{2}\df{2}{x}{u}+o\left(\|u\|^2\right)
\end{align}

Taylor's Theorem (MN, theorem 12): for some $\theta\in\left[0,1\right]$,

\begin{align}
  \label{eq:Taylors}
  f(x+u)=f(x)+\df{}{x}{u}+\frac{1}{2}\df{2}{x+\theta u}{u}
\end{align}

Rearranging some stuff,

\begin{align}
  \label{eq:18}
  \parD{}{}{x}u&=
\end{align}



By differentiating Taylors Theorem,

\begin{align}
  \label{eq:19}
  \df{}{x+u}{v}-\df{}{x}{v}&\approx\mathsf{d}\left(\df{}{x}{u};v\right)\\
\end{align}

Somehow, we want to get from here to how a finite difference of first
derivatives is a valid approximation to second derivatives.


Finite differencing (FD) is a numerical differentiation method that
operationalizes a Taylor series approximation to a differentiable
function.  Let $f(x)$ be the scalar-valued function to be
differentiated, and let $\delta$ be some small, positive, scalar
value.  For scalar $x$, the second order
approximation to $f(x)$ is
  \begin{align}
    \label{eq:1}
  f(x+\delta)  &=f(x) + f'(x)\delta + \frac{1}{2}f''(x)\delta^2 +\mathcal{O}(\delta^2) \qquad\text{ as
                 }\delta\rightarrow 0
  \end{align}
Dropping the terms of second order and above,
    \begin{align}
    \label{eq:FD1}
    f'(x)&\approx\frac{f(x+\delta)-f(x)}{\delta}
  \end{align}

  The dividing the difference between $f(x+\delta)$ and $f(x)$ by a
  suitably small $\delta$ is a first-order numerical approximation to
  the derivative.

  Differentiating the Taylor series, and rearranging terms,

    \begin{align}
    \label{eq:1}
  f''(x)  &\approx \frac{f'(x+\delta) - f'(x)}{\delta}
  \end{align}

The second derivative is approximated by a difference in the first derivatives.

For multivariate $x$, the Taylor series is

\begin{align}
  \label{eq:8}
  f(x+\iota\delta)=f(x) +
  \delta\iota'D{x}+\frac{1}{2}\delta\iota'H{x}\delta\iota + \mathcal{O}(\delta^2)
\end{align}


  Define $D(x)$ as the gradient of $f(x)$ at $x$,
  and define $H(x)$ as the Hessian matrix. Let $\iota$ be a vector of ones. The multivariate Taylor series is

  \begin{align}
    \label{eq:2}
    f(x+\delta\iota)=f(x) + \delta\iota' D(x) + \frac{1}{2}\delta\iota'H(x) \delta\iota+\mathcal{O}(\delta^2)
  \end{align}

The gradient can be computed by adding $\delta$ successively to each
element of $x$, so the partial derivative with respect to $x_k$ is

      \begin{align}
    \label{eq:FD1K}
    f_k'(x)&\approx\frac{f(x+\delta e_k)-f(x)}{\delta}\\
        D(x)&=\left(f_1'(x), f_2'(x),\mathellipsis f_K'(x)\right)
  \end{align}



  a describes methods of approximating
  derivates The use of finite differences for approximating derivatives
  numerically starts with a Taylor series approximation to a
  scalar-valued function.  Because the highest-order derivative we
  will consider in this paper is the second, we will truncate our
  Taylor series there.



  \subsubsection{First order}


  After rearranging terms, we can see that we can approximate $f(x)$
  by choosing and arbitrarily small $h>0$.


For a multivariate $x$, with length $K$, estimating $f'(x)$ involves
computing $f(x)$ once, and then Equation \ref{eq:FD1} $K$ times by
perturbing each of the elements of $x$ separately.

Error and complexity analysis here

\subsubsection{Second order}

  By differentiating the Taylor series, we see that we can approximate a
second derivative by computing a finite difference of the first
derivative.

\begin{align}
  \label{eq:3}
  f'(x+\delta)&=f'(x) + f''(x)\delta + O(\delta^2)\\
 f''(x)&\approx\frac{f'(x+\delta)-f'(x)}{\delta}
\end{align}

Suppose we can compute partial derivatives $f'_k(x)$ exactly for all
$k=1\mathellipsis K$, and let $e_k$ be the $k$th coordinate vector. We can then approximate each column of the
Hessian via finite differencing of the gradients.
\begin{align}
  \label{eq:4}
  H_k&\approx\frac{f'(x + \delta e_k)-f'(x)}{\delta}
\end{align}
Using forward differencing, estimating the entire Hessian requires $K+1$ evaluations of
the gradient, or the equivalent of $K^2+1$ function evaluations.  This
is another reason why estimating a Hessian can be expensive for
large problems.


Error and complexity analysis here


\subsection{Exploiting sparsity}

We can reduce the number of gradient evaluations if the Hessian is sparse.


If we know that some of the cross-partial derivatives are
zero, we \emph{might} be able to estimate the Hessian with fewer gradient evaluations.
Suppose $K=2$.  The Hessian matrix is

\begin{align}
  \label{eq:5}
  H=
  \begin{pmatrix}
    f'_1(x_1+\delta,x_2) -f'_1(x_1,x_2)& f'_1(x_1, x_2+\delta) -f'_1(x_1,x_2)\\
    f'_2(x_1+\delta, x_2) -f'_2(x_1,x_2) & f'_2(x_1, x_2+\delta) -f'_2(x_1,x_2)
    \end{pmatrix}            /\delta
\end{align}
Of course, the off-diagonal elements are equal.  If the cross-partial derivatives are zero, then
\begin{align}
  \label{eq:6}
   f'_1(x_1, x_2+\delta) -f'_1(x_1,x_2)&=0\\
  f'_2(x_1+\delta, x_2) -f'_2(x_1,x_2)&=0\\
\text{and, therefore,}\nonumber\\
     f'_1(x_1+\delta, x_2+\delta) -f'_1(x_1+\delta,x_2)&=0\\
  f'_2(x_1+\delta, x_2+\delta) -f'_2(x_1,x_2+\delta)&=0\\
\end{align}

By substitution,
\begin{align}
  \label{eq:6}
  H=
  \begin{pmatrix}
    f'_1(x_1+\delta,x_2+h) -f'_1(x_1,x_2)& 0\\
   0& f'_2(x_1+\delta, x_2+\delta) -f'_2(x_1,x_2)
    \end{pmatrix}            /\delta
\end{align}
A single evaluation of the gradient $f'(x_1+\delta, x_2+\delta)$, and a single
evaluation of the gradient at $f'(x_1, x_2)$, provide enough
information to extract the non-zero elements of the Hessian.  Reducing
the number of gradient evaluations from 3 to 2 depends on knowing that
the cross-partial derivatives are zero.

Now let's consider a general case, starting with a ``direct'' method
first proposed in \citet{CurtisPowellReid1974} for Jacobian matrices, and described in \citet{PowellToint1979}. To begin, partition the variables
into $G$ mutually exclusive groups, or ``colors,'' so $g_k$ indexes
the color of variable $k$. Let $D$ and $Y$ be $K\times G$
matrices, where $D_{kg}=\delta$ if variable $k$ belongs to group $g$, and zero
otherwise, and let $D_g$ be the $g$th column of $D$.  Each column in $Y$ is defined as
\begin{align}
  \label{eq:Yg}
  Y_g&=f'(x+D_g)-f'(x)
\end{align}

If $G=K$ and $g_k=k$, then $D$ is a diagonal matrix with $\delta$ in each
diagonal element.  The matrix equation $HD=Y$ represents the Taylor
series approximation
$H_{ik}\delta\approx y_k$, and we can solve for all elements of $H$
just by computing $Y$. But if $G<K$,
there must be at least one column of $D$ with $\delta$ in at least two
elements. Column $Y_g$ is computed by perturbing two variables at
once, and a particular $Y_{ig}$ would be equal to $\delta$ times a
\emph{sum} of multiple elements in row $H_i$.  Without further restrictions on values in
$H$, $HD=Y$ is an underdetermined system of linear equations, and we
would not be able to compute $H$ by computing a $Y$ with
fewer than $K$ columns.

The necessary restrictions on $H$ come from the sparsity pattern.  We
know which $h_{ik}=0$, so we can add those equations to $HD=Y$.  For
example, consider a function with the following Hessian.

\begin{align}
  \label{eq:7}
 H= \begin{pmatrix}
    h_{11}&0&h_{31}&0&0\\
    0&h_{22}&0&h_{42}&0\\
    h_{31}&0&h_{33}&0&h_{53}\\
    0&h_{42}&0&h_{44}&0\\
    0&0&h_{53}&0&h_{55}
  \end{pmatrix}
\end{align}
Note the subscripts on the elements take the symmetry of $H$ into account.

Suppose $G=2$, and define the colors through the following $D$ matrix.

\begin{align}
  \label{eq:7}
 D= \delta\begin{pmatrix}
   1&0\\
   1&0\\
   0&1\\
   0&1\\
   1&0
  \end{pmatrix}
\end{align}
Variables 1 and 2 have color 1, and variables 3, 4 and 5 have color
2.  For the moment, we will postpone the discussion of how to
choose $G$ and how to color the variables.

Next, compute the columns of $Y$ using Equation \ref{eq:Yg}.  We now
have the following system of linear equations from $HD=Y$.
\begin{align}
  \label{eq:11}
  \begin{aligned}[c]
  h_{11}&=y_{11}\\
  h_{22}&=y_{21}\\
  h_{31}+h_{53}&=y_{31}\\
  h_{42}&=y_{41}\\
  h_{55}&=y_{51}\\
  \end{aligned}
  \qquad
  \begin{aligned}[c]
  h_{31}&=y_{12}\\
  h_{42}&=y_{22}\\
  h_{33}&=y_{32}\\
  h_{44}&=y_{42}\\
  h_{53}&=y_{52}
  \end{aligned}
\end{align}

Note that this system is overdetermined; \citet{CurtisPowellReid1974} did
not assume that their Jacobian is symmetric.  Both $h_{31}=y_{12}$ and $h_{53}=y_{52}$
can be determined directly, but $h_{31}+h_{53}=y_{31}$, and $h_{42}$
could be either $y_{41}$ or $y_{22}$.  \Citet{PowellToint1979} prove
that it is sufficient to solve $LD=Y$ via a substitution method, where $L$ is the lower
triangular part of $H$.  This has the effect of removing the equations
$h_{42}=y_{22}$ and $h_{31}=y_{12}$ from the system, but retaining
$h_{53}=y_{52}$.  We can then solve for
$h_{31}=y_{31}-h_{53}=y_{31}-y_{52}$. Thus, we have determined a
$5\times 5$ Hessian matrix with only three gradient evaluations, in
contrast with the six that would have been needed had $H$ been treated
as dense.

Solving $HD=Y$, using the full Hessian is known as a ``direct''
method, because the elements of the Hessian can be determined directly
from $Y$, as long as an appropriate coloring of the variables is
used.  Solving $LD=Y$ is known as either an ``indirect,'' or
``triangular substitution'' method, for obvious reasons.  The
substitution method is often preferred over direct methods for
symmetric matrices because we can partition the
variables with a lower $G$, and estimate the Hessian with fewer
evaluations of the gradient.
Direct methods will not necessarily be able to exploit the fact that
the Hessian is symmetric. This is particularly true for banded
Hessians, which was described by \citet{ColemanMore1984}, and reveal
in the preceding example.  Therefore, we will restrict our attention
in the subsequent sections to triangular substitution methods.
to

 \subsection{Note:  Who proved what}

Consistent partitioning of $L$:  No column in the same group has a
non-zero element in the same row.

\Citet{PowellToint1979} showed that a consistent partitioning of $L$
allows for a substitution method (mentioned by
\citet{ColemanMore1984}.  This is a substitutable partition.

\Citet{PowellToint1979} show that the order in which variables are
assigned to partitions affects the partition.

\Citet{ColemanMore1984} characterize \cite{PowellToint1979} as a graph
coloring problem.  Also justify using the smalest-last ordering on the lower
triangle to color the variables

\Citet{ColemanCai1986}:  there are other substitutable partitions than
lower triangular.  Theorem 2.2.  A mapping induces a substitution
method if and only if the mapping is a cyclic coloring.

Cyclic coloring:  At least 3 colors in every cycle.

From \citet{ColemanCai1986}, general result of substitutable
(beyond \citet{PowellToint1979}.  Order nonzero elements
  $1\mathellipsis M$. If, for nonzero element $(i_m, j_m)$,


  \begin{enumerate}
\item  columns  $j_m$ and another other $j_{m'}$ are in the same
  group, and both $j_m$ and $j_{m'}$ have a nonzero in row $i_m$, then
  $i_{m'}, j_{m'}$ must be ordered before $i_m, j_m$; \emph{or}
\item  columns  $i_m$ and another other $i_{m'}$ are in the same
  group, and both $i_m$ and $i_{m'}$ have a nonzero in row $j_m$, then
  $j_{m'}, i_{m'}$ must be ordered before $j_m, i_m$
  \end{enumerate}

  What this means is that we can ignore column intersections that
  occur in lower rows (what?).

  The point is that lower triangular substitution qualifies, which
  means that we just need a cyclic coloring of the graph.


  Note:  My coloring algorithm is a smallest last ordering of the full
  symmetric Hessian  This is \emph{slpt} in \citet{ColemanMore1984}.
  What I should really do is a smallest-last on the lower triangle
  (\emph{slsl} in citet{ColemanMore1984}.  So I need to change that.

  In any event, it's still finding a cyclic coloring of the adjacency
  graph.  Just using a different heuristic for the coloring.




\subsection{Partitioning the variables}


Both \citet{CurtisPowellReid1974} (for direct methods) and
\citet{PowellToint1979} (for substitution methods) propose the
following rule for partitioning the variables:  two variables cannot
be in the same group if their respective columns of the Hessian
(direct) or its lower triangle (substitution) have a
non-zero element in the same row.  In the example above, variables 1
and 3 cannot be in the same group because columns 1 and 3 both have
non-zero elements in row 3.

This rule tells us how to exclude certain groupings, but not how to
find an optimal one, with the fewest possible number of groups.
\Citet{ColemanMore1984} were the first to recognize that the
partitioning task is actually a graph coloring problem.  The sparsity pattern of $L$
is an adjacency matrix in an undirected graph.

\begin{figure}
  \begin{tabular}{r|c|c|c|c|c|c|c}
   & 1&2&3&4&5&6&7\\
    \hline
    1& 1&&&&&&\\
    2&1&1&&&&&\\
    3&0&0&1&&&&\\
    4&0&0&1&1&&&\\
    5&0&0&0&0&1&&\\
    6&0&0&0&0&1&1&\\
    7&1&1&1&1&1&1&1
  \end{tabular}
\end{figure}



\begin{figure}
  \begin{subfigure}[b]{.5\textwidth}
  \begin{tabular}{r|>{\columncolor{red!20}}c%
    >{\columncolor{blue!20}}c%
    >{\columncolor{red!20}}c%
    >{\columncolor{blue!20}}c%
    >{\columncolor{red!20}}c%
    >{\columncolor{blue!20}}c%
    >{\columncolor{green!20}}c}
   & 1&2&3&4&5&6&7\\
    \hline
    1& 1&&&&&&\\
    2&1&1&&&&&\\
    3&0&0&1&&&&\\
    4&0&0&1&1&&&\\
    5&0&0&0&0&1&&\\
    6&0&0&0&0&1&1&\\
    7&1&1&1&1&1&1&1
  \end{tabular}
\end{subfigure}
\begin{subfigure}[b]{.5\textwidth}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[draw, shape=circle]
\path node at (0.5,1.5) [fill=red!20] (v1) {1};
\path node at  (1.5,1.5) [fill=blue!20] (v2) {2};
\path node at  (2.5,1.5) [fill=red!20] (v3) {3};
\path node at  (3.5,1.5) [fill=blue!20] (v4) {4};
\path node at  (4.5,1.5) [fill=red!20] (v5) {5};
\path node at  (5.5,1.5) [fill=blue!20] (v6) {6};
\path node at  (3,0)  [fill=green!20] (v7) {7};
\draw (v1) -- (v2)
(v1) -- (v7)
(v2) -- (v7)
(v3) -- (v4)
(v3) -- (v7)
(v4) -- (v7)
(v5) -- (v6)
(v5) -- (v7)
(v6) -- (v7);
\end{tikzpicture}
\end{subfigure}
\end{figure}



If $h_{i,j}\neq 0$ ,
then variables $i$ and $j$ are ``neighbors'' in the graph, and they
cannot have the same color.  Now, let's introduce a new variable $k$,
where $j$ and $k$ are neighbors, but $i$ and $k$ are not.  The colors
of $i$ and $k$ still have to be different, because they have a common
neighbor in $j$.

For a partition to be compatible with estimating a Hessian via
substitution, it is sufficient, but not necessary, that no variable
have the same color as any other variable with which it shares a
common neighbor.  That is, no variable within two ``steps'' on the
undirected graph may have the same color.  But in the example above,
variables 1 and 5 have the same color, even though 1 is connected to
3, and 3 is connected to 5.  \Citet{ColemanCai1986} prove that the
rule that no same-colored columns may have non-zero elements in the
same row is equivalent to an ``acyclic'' graph coloring scheme, and
that coloring the variables in this way is also sufficient for
estimating a sparse Hessian.  Thus, as \citet{GebremedhinTarafdar2009}
point out, minimizing the number of partitions is equivalent to
minimizing the number of colors in an acyclic graph.

Define acyclic.



\section{Example function}



\section{Using the package}

\subsection{The sparseHessianFD class}

The package functionality is implemented as a reference class
\class{sparseHessianFD}.  The initializer takes the following arguments.
\begin{itemize}
\item[x.init] A numeric vector of variables at which the object will be
  initialized and tested.  It is not stored in the object, so it can
  really be any value, as long as the objective function, gradient and
  Hessian are all finite.
\item[fn,gr] \proglang{R} functions that return the value of the
  objective function, and its gradient. The first argument is the numeric
  variable vector.  Other arguments can be passed as ... .
\item[rows, cols] Integer vectors of the row and column indices of
  the non-zero elements in the lower triangle of the Hessian.
\item[direct] This argument is deprecated, and is included only for
  backwards compatibility with earlier versions.
\item[eps] The perturbation amount for finite differencing of the
  gradient to compute the Hessian. Defaults to
  sqrt(.Machine\$double.eps)
\item [index1] If TRUE (the default), \funcarg{row} and \funcarg{col} use one-based
  indexing.  If FALSE, zero-based indexing is used (which is the
  internal storage format for matrix classes in the \pkg{Matrix}
  package).
\item [...] Additional arguments to be passed to \func{fn} and \func{gr}.
\end{itemize}

To create a \class{sparseHessianFD} object, just call
\func{sparseHessianFD}.  If you are accepting all of the default
arguments, and not passing additional arguments to \func{fn} and
\func{gr}, the call will look like:

<<eval=FALSE>>=
obj <- sparseHessianFD(x.init, fn, gr, rows, cols)
@

The class defines a number of different fields, none of which should
be accessed directly.    The initializer automatically calls the graph
coloring subroutine, and evaluates the Hessian at $x$, so it may take
some time to create the object.


\subsection{Evaluating the Hessian}

The \func{fn}, \func{gr} and \func{hessian} methods respectively evaluate the
function, gradient and Hessian at a variable vector $x$.

<<eval=FALSE>>=
f <- obj$fn(x)
df <- obj$gr(x)
hess <- obj$hessian(x)
@

The \func{fn} and \func{gr} methods are simply closures around the
functions that were provided to the class initializer.  Since the
additional arguments were already supplied as \texttt{...}, they do
not need to be supplied again.  This feature makes subsequent calling
of \func{fn} and \func{gr} simpler, because only the variable is
included in the call.

Similarly, the \func{hessian} method takes the single argument $x$.
The return value is always a \class{dgCMatrix} object (defined in the
\pkg{Matrix} package).  \class{dgCMatrix} objects are sparse matrices,
stored in a compressed, column-oriented format, and includes all
non-zero elements in both the upper and lower triangles.  One could
coerce the Hessian into a symmetric \class{dsCMatrix} if necessary.

The \class{sparseHessianFD} class also provides methods \func{fngr}
and \func{fngrhs} that return the function, gradient and possibly the
Hessian, in a single list.


\subsection{Providing the sparsity pattern}

The sparsity pattern of the Hessian is defined as the row and column
indices of the non-zero elements in the \emph{lower triangle} the Hessian.  Internally, this
pattern is stored in a compressed format, but the
\class{sparseHessianFD} initializer requires rows and columns, to keep
things simple.  It is the responsibility of the user to ensure that
the sparsity pattern is correct. Any elements in the upper triangle
will be automatically removed, but there is no check that a
corresponding element in the lower triangle exists.

The \func{Matrix.to.Coord} function extracts
row and column indices from a sparse matrix.  The argument $M$ is a
matrix that could be coerced to a \pkg{Matrix} object that is derived
from the \class{TsparseMatrix} class (a virtual class that defines
sparse matrices stored in row-column format).  Standard base
\proglang{R} matrices, and most \pkg{Matrix} matrices, fall into this
category.  If the \funcarg{index1} argument is \code{TRUE} (the
default), then \func{Matrix.to.Coord} returns 1-based indexes.

The input matrix to \func{Matrix.to.Coord} does not have to include
the values (if the full Hessian were known, that would possibly defeat the
purpose of this package).  It is sufficient to supply a logical
or pattern matrix, such as \class{lgCMatrix} or \class{ngCMatrix}.
Rather than trying to keep track of the row and column indices
directly, it might be easier to construct a pattern matrix first,
check visually that the matrix has the right pattern, and then extract
the indices.

The following code constructs a block diagonal matrix, and extracts
the sparsity pattern from its lower triangle.

<<exPattern>>=
M <- as(kronecker(Diagonal(3), Matrix(T,2,2)),"nMatrix")
M
tril(M)
mc <- Matrix.to.Coord(tril(M))
mc
@
We then use \funcarg{mc\$row} and \funcarg{mc\$col} to construct the
\class{sparseHessianFD} object.

To visually check that a proposed sparsity pattern represents the
intended matrix, use the \func{Coord.to.Pattern.Matrix} function,
which is just a wrapper to \pkg{Matrix}'s \func{sparseMatrix} constructor.

<<>>=
M2 <- Coord.to.Pattern.Matrix(mc$rows, mc$cols, dims=dim(M))
M2
@


One could


The functions for computing the objective function, gradient and
Hessian for this example are in the \filename{R/binary.R} file.  The package
also includes a sample dataset with simulated data from the binary choice example.

To start, we load the data, set some dimension parameters, set prior
values for $\Sigma^{-1}$ and $\Omega^{-1}$, and simulate a
vector of variables at which to evaluate the function.

<<>>=
set.seed(123)
data(binary_small)
binary <- binary_small
str(binary)
N <- length(binary$Y)
k <- NROW(binary$X)
nvars <- as.integer(N*k + k)
P <- rnorm(nvars) ## random starting values
priors <- list(inv.Sigma = rWishart(1,k+5,diag(k))[,,1],
               inv.Omega = diag(k))
@

<<echo=FALSE>>=
options(scipen=-999)
@

This dataset represents the simulated choices for $N= \Sexpr{N}$ customers
over $T= \Sexpr{T}$ purchase opportunties, where the probability of purchase
is influenced by $k= \Sexpr{k}$ covariates.

The objective function for the binary choice example is \func{binary.f} and the gradient function is
\func{binary.grad}.  The first argument to both is the variable vector, and
the argument lists must be the same for both.  For this example, we
need to provide the data list ($X$, $Y$ and $T$) and the prior
parameter list
($\Sigma^{-1}$ and $\Omega^{-1}$). The functions also have an
`order.row` argument to change the ordering of the variables (and
thus, the sparsity pattern). If `order.row=TRUE`, then the Hessian
will have an off-diagonal pattern.  If `order.row=FALSE`, then the
Hessian will have a block-arrow pattern.

For testing and demonstration purposes, we also have a `binary.hess`
function that returns the Hessian as a sparse `dgCMatrix` object (see
the Matrix package).

<<>>=
true.f <- binary.f(P, binary, priors, order.row=FALSE)
true.grad <- binary.grad(P, binary, priors, order.row=FALSE)
true.hess <- binary.hess(P, binary, priors, order.row=FALSE)
@

The sparsity pattern of the Hessian is specified by two integer
vectors:  one each for the row and column indices of the non-zero
elements of the ***lower triangular part*** of the Hessian.  If you
happen have have an example of a matrix with the same sparsity pattern
of the Hessian you are trying to compute, you can use the following
convenience function to extract the appropriate index vectors.

<<>>=
pattern <- Matrix.to.Coord(tril(true.hess))
str(pattern)
@

Next, we create a new instance of a sparseHessianFD object with an
"initial variable" $P$, and the row and column indices of the non-zero
elements in the lower triangle of the Hessian.  We also pass in any
other arguments for \func{binary.f} and \func{binary.grad}.  We accept the default
values for other arguments to `sparseHessianFD.new`.

<<>>=
obj <- new("sparseHessianFD", P, binary.f, binary.grad,
       rows=pattern[["rows"]], cols=pattern[["cols"]],
       data=binary, priors=priors,
       order.row=FALSE)
@

Now we can evaluate the function value, gradient and Hessian through
`obj`.

<<>>=
f <- obj$fn(P)
gr <- obj$gr(P)
hs <- obj$hessian(P)
@

Note that the member functions in the sparseHessianFD class take only
one argument:  the variable vector.  All of the other arguments are
stored in `obj`.

Do we get the same results that we would get after calling \func{binary.f},
`binary.grad` and `binary.hess` directly?  Let's see.


<<>>=
all.equal(f, true.f)
all.equal(gr, true.grad)
all.equal(hs, true.hess)
@

If there is any difference, keep in mind that `hs` is a numerical
estimate that is not always exact.  I certainly wouldn't worry about
mean relative differences smaller than, say, $10^{-6}$.

\subsection{Speed comparison}

Instead of using this package, we could treat the Hessian as dense,
and use the hessian function numDeriv package. The advantage of the
numDeriv package is that it does not require the gradient.  However,
you can see that it takes some time to run.

<<echo=FALSE>>=
options(scipen=0)
@

<<>>=
hess.time <- system.time(H1 <- obj$hessian(P))
fd.time <- system.time(H2 <- hessian(obj$fn, P))
H2 <- drop0(H2, 1e-7) ## treat values < 1e-8 as zero
print(hess.time)
print(fd.time)
@

The sparseHessianFD package can be substantially faster than estimating a dense
Hessian by brute force finite differencing.  The cost of this speed up
is that the user does need to provide the gradient and the sparsity
structure.  As with everything in life, there are trade-offs.

\subsection{Quick summary}
In short, to use the package, follow the following steps:

\begin{enumerate}
\item Write R functions to return the value and gradient of the
objective function.
\item Determine the row and column indices of the non-zero elements of
the lower triangle of the Hessian.
\item Pick a variable vector (i.e., a starting value) at which you can
    initialize the sparseHessian object.  It doesn't really matter
    what this vector is, as long as the function value and gradient
    elements are all finite.
  \item Create a new sparseHessianFD object using the sparseHessianFD.new
function.  For this example, call that object F.
\item Compute Hessian at x by calling F\$hessian(x).
  \end{enumerate}


  The user can accept some small amount of approximation error.
    Since FD is a numerical approximation technique, the estimate of the Hessian
    will not be exact as either computing the Hessian directly from a
    symbolic derivation, or using AD.  By ``small,'' we mean a
    relative error roughly on the order of $10^{-6}$ or less.  In our
    experience should be widely achievable on double-precision machines.   If such errors are
    too large for a particular application, then \pkg{sparseHessianFD}
    may not be the best tool for the job. Exceptions
    would be cases of objective functions that are poorly conditioned,
    with ridges, plateaus or other pathologies, or cases for which
    such tiny errors will incur large costs.

\end{document}

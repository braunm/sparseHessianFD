\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{enumitem}

\newcolumntype{d}[1]{D{.}{.}{#1}}
\newcommand{\pkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\filename}[1]{\texttt{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\func}[1]{\code{#1}}
\newcommand{\class}[1]{\textsl{#1}}
\newcommand{\funcarg}[1]{\code{#1}}
\newcommand{\variable}[1]{\code{#1}}
\newcommand{\method}[1]{\func{#1}}


 \newcommand{\df}[3]{\mathsf{d}^{#1}f(#2;#3)}
 \newcommand{\parD}[3]{\mathsf{D}^{#1}_{#2}#3}
\newcommand{\hess}[2]{\mathsf{H}_{#1}#2}
\newcommand{\hessLT}[2]{\mathsf{L}_{#1}#2}
\newcommand{\Mat}[1]{\mathbf{#1}}
\newcommand{\Real}[1]{\mathbb{R}^{#1}}

\newenvironment{revQuote}{\itshape}{\vspace{\baselineskip}}
\newenvironment{response}{\normalfont}{\vspace{\baselineskip}}


\usepackage{geometry}
\geometry{margin=1.1in}

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}

\begin{document}

{\bfseries
\begin{center}
  Manuscript JSS2717\\
 ``sparseHessianFD: An R Package for Estimating Sparse
 Hessian Matrices''\\
 \vspace{\baselineskip}
 Response to reviewer
\end{center}
}
\vspace{\baselineskip}

I would like to express my sincere appreciation for the time and effort that
you invested in the review.  It is obvious to me that you went through
the paper and code in great detail.  In
particular, thank you for bringing the complex step
method to my attention.  I \emph{never} use complex numbers or
functions in my own work, so I typically ignore anything with the word
``complex'' in it.  But this review prompted me to learn something
new, and that is always fun.

I am happy to write that I was able to incorporate a large majority of
your suggestions into the the revised paper and package. For the
others, I hope that you, and the editor, will accept my
explanations and excuses.  Of course, if there are any strong
objections to how I responded to any of the comments, I am happy
to make further changes in advance of publication.

Below are my responses to each item, in the same order as in the
review.  The original comment is in italics, and is followed by my
response in normal font.  To save space, I removed the code and output
that were included in the review.

\vspace{\baselineskip}

\begin{enumerate}[align=left]
 \setcounter{enumi}{-1}

\item \begin{revQuote}
I had a minor problem checking the package tarball:

{\normalfont (error messages related to algorithm.sty removed).}

I don't remember if there is an easy way to distribute ``special'' sty files,
or if you must force the user to install them. If the sty is not really
critical it might be better to omit it.  
  
\end{revQuote}

\begin{response}
Because \pkg{sparseHessianFD} passed CRAN check, I suspect that the
error is a result of the \pkg{algorithm} package
not being present in your LaTeX installation.  It is a standard
package in the MacTeX distribution, but I cannot speak with authority about other
platforms. It should be easily installed through CTAN.

I would prefer to continue to use the \pkg{algorithm} package, but I
could try to find another way to format the algorithms if there is a
strong objection.
\end{response}

\item \begin{revQuote}
I am not familiar with reference classes, but for most users I think it
should not be important that the package uses them. However, as a user, I
find it disconcerting that sparseHessianFD() is used as a (constructor)
function (eg. p12, and in the example in ?sparseHessianFD) but is not
documented in the help with 'usage' and 'arguments' as is usual for
functions. There seems to be little guidance in the usual R places about how
reference classes should be documented. Perhaps you could seek some guidance
from the community on this point.
\end{revQuote}


\begin{response}

  Note to me:  issue here is that there is no documentation for
  \func{sparseHessianFD()} as a function.  Good idea to create a
  documentation page for that.

  
\end{response}



\item \begin{revQuote}
It is pointed out (p2) that requirements are burdensome and emphasized
that the package is not appropriate for all uses. Also, the conditions 1-5
are fairly stringent. The reader is left with the impression that there may
be few applications. I think the value of the package could be promoted
better. Perhaps a small list of different types or applications of
hierarchical problems could be given, or an indication of other classes of
problems where the conditions would apply.
    
  \end{revQuote}

  \begin{response}
I included this section primarily as a way to avoid overselling the
method, but the point that the reader might be scared away by this list of
restrictions is well-taken.  In the revision, I removed the list, and
wove the restrictions into the text.  The revision projects a more
positive tone that in the original submission.
\end{response}

\item \begin{revQuote}
Condition 5 might be relaxed to a local condition in a neighborhood of
the evaluation point, rather than a global condition, but I do not know if
that would have any practical value.
  \end{revQuote}

\begin{response}
  The list of conditions is no longer in the paper, but to this point, the
  condition is global and not local.  I added a defintion of a ``structural zero'' in the third
  paragraph. The sparsity pattern represents the structure of the
  Hessian, not its value, so any element that \emph{could} be zero,
  but does not have to be zero, is not a structural zero.

   At the end of Section 2.2, I added a paragraph that explains that
   if there is uncertainty about whether an element is a structural
   zero or not, it is better to include it in the sparsity pattern.
\end{response}

\item \begin{revQuote}
Section 2.1 p7 I think could be made more easily readable by adding a few
more hints about dimension, and more care about use of the term 
"coefficients vector", for example:
\begin{itemize}
  \item 'continuous covariates $x_i$,' -> 'continuous covariates $x_i \in
\Real{k}$,'

  \item  'heterogeneous coefficient vector $\beta_i$' -> 'heterogeneous
coefficient vector $\beta_i \in \Real{k}$'

  \item 'The coefficients are distributed' -> 'The coefficient vectors
$\beta_i$ are distributed'
\end{itemize}
  \end{revQuote}

\begin{response}
  Thank you for the suggestions.  Changes have been made throughout the paper.
\end{response}


\item \begin{revQuote}
p7 following "the cross-partial derivatives"
$\hess{\beta_i,\beta_j}{}=\parD{2}{\beta_i, \beta_j}{}=0$ for all $i\neq
j$.  

  Is this a definition of $\hess{\beta_i,\beta_j}{}$ in terms of  $\parD{2}{}{}$
or a statement of equality?
  
  Also, I am confused by the single subscript $\beta_i$ to \\hess here, but a
double subscript just above in "Thus, $\hess{\beta_{ik},\mu_k}{}\neq 0$".
  \end{revQuote}

\begin{response}
  Based on this comment, I rewrote Section 2.2 to distinguish the
  cross-partial derivatives $\parD{2}{}{}$ from the corresponding
  elements of the Hessian $\hess{}{}$.  I did this by defining $x_i$ as
  a subset of the elements of $x$, and Ind$(x_i)$ as an 
  ``indexing'' function.  This change required me to redefine the
  covariate matrix in the example from $\Mat{X}$ to $\Mat{Z}$.

  The revised version is slightly more formal
  and precise, and I hope that this is the kind of change you had in
  mind.
\end{response}

\item \begin{revQuote}
 I think possibly (16) is ``banded'' and (17) ``block arrow'' rather than the
reverse which is indicated.
\end{revQuote}

\begin{response}
The descriptions of the two sparsity patterns are correct. The
first index in the subscript of $\beta$ refers to a household, and the
second index refers to a covariate. Thus,
$\parD{2}{\beta_{11},\beta_{12}}{}\neq 0$, but
$\parD{2}{\beta_{11},\beta_{21}}{}=0$.  When the second index changes
faster than the first, the pattern is ``block-arrow.''  When the first
index changes faster than the second, the pattern is ``banded.''
  
\end{response}


\item \begin{revQuote}
Code in the paper prior to table 4 needs to be copied from the
vinettes/sparseHessianFD.Rnw file. For those trying to reproduce results it
would be nice if this where mentioned in the file replication.R.
  \end{revQuote}

  \begin{response}
Assuming that the R package builder works the way I think it does,
the \filename{replication.R} file, and the data for Table 4 (\filename{vignette\_tab4.Rdata}), will be
copied to the \filename{doc\textbackslash} directory of the package
once it is installed.  At the end of the second paragraph of Section
4, I now write that ``code to replicate Table 4 is available as an
online supplement to this paper.''  I cannot be more specific about
the name of the replication file, because I believe it depends on the
volume and issue number of the published version.
\end{response}


\item \begin{revQuote}
p9, last line. The R> at the beginning of
 
   R> obj <- sparseHessianFD(x, fn, gr, rows, cols, ...)

suggests that this is code that can be entire at the command line, but ...
causes an error when entered (and the arguments have not been defined at
this point in the vignette. Instead it should be indicated as the usage
syntax.
  \end{revQuote}

\begin{response}
  I removed the \func{R>} prompt, and now refer to that code as a usage
  syntax.  Thank you for the suggestion.
\end{response}


\item \begin{revQuote}
p10. "where ... represents all other named arguments"
   I think the usual usage is the ... represents the arguments other than
the "named" ones, so it is probably better to just say "other arguments".
\end{revQuote}

\begin{response}
  Done.
\end{response}


\item \begin{revQuote}
On p12, the function calls all.equal(f, true.f) and all.equal(gr,
true.grad) are comparing f and gr calculated with the exact calculation, so
the difference is zero:


(Code removed)


It is not clear to me whether obj\$fn() and obj\$gr() use code as in the true
functions or a modified version using sparse techniques. Some further
clarification would be helpful.

On the other hand, all.equal(hs, true.hess) is comparing a true analytic
calculation with a first order simple difference aproximation using the true
gradient function:

(Code removed)

which might also be mentioned in the text. (Really just for exposition
purposes, after all, it is almost the main purpose of the package.)
  \end{revQuote}

\begin{response}
  Good point.  I revised that section to explain that evaluations of
  the function and gradient, as called from the
  \class{sparseHessianFD} object, have to be identical to the true values.
  I replaced \func{all.equal} with \func{identical} to
  highlight that fact. I then explain that the
  \class{sparseHessianFD} calculation of the Hessian will not be
  identical to the true value, and report the mean relative
  difference.  I chose the mean relative difference over maximum
  absolute difference to be consistent with what \func{all.equal}
  uses to test if two matrices are equal within numeric tolerance.
\end{response}


\item \label{it:complex1}\begin{revQuote}
I think it would be instructive to add some of the following comparisons
with the above on p12.  The package numDeriv function hessian by default
does a second order Richardson approximation using the true function value
approximation. This involves a very large number of function evaluations in
an attempt to obtain some accuracy, but the accuracy is limited by being an
approximation of a second difference:

(Code removed)

Since the hessian is the first difference of the gradient, which is the
calculation used by obj\$hessian() in sparseHessianFD, one could also use the
function numDeriv::jacobian:

(Code removed)

This is still doing the calculation intensive Richardson approximation. The
calculation which would seem to most closely resemble what is done by
obj\$hessian() is

(Code removed)


Another very interesting comparison is

(Code removed)

The complex step derivative provides extremely accurate approximations with
a  number of function evaluation similar to the simple method. (This does
not seem to be anticipated by footnote 1 in the paper.) However, the method
imposes some serious requirements on the function. (Something like complex
analytic even though the user may only be interested in the real part.) The
code also has to accept complex arguments and return the complex result.
Fortunately most R primitive work with complex numbers so the code
requirement may happen accidentally, which can be partly verified by

binary.grad(P + 0+1i, data=binary, priors=priors, order.row=order.row)

returning a complex result. (This does not rule out all possible problems.)

As I recall, sums, multiplication, and exponentiation are all complex
analytic, so it would not be too surprising if the example in the paper is
too, but I have not analyzed that. However, based on the result being very
good, it seems highly likely.
  \end{revQuote}

  \begin{response}
    See response to Item \ref{it:complex2}
  \end{response}


\item\label{it:complex2} \begin{revQuote}
A possible extension to the package would be to implement the complex
method in the sparse code. The function numDeriv:::jacobian.default
implements both simple and complex, so provides a good comparison of the
necessary (non-sparse) computation. 

  \end{revQuote}

\begin{response}

Here I am combining my responses to items \ref{it:complex1} and
\ref{it:complex2} together.

  The complex step function is now implemented in the package, and is
  introduced in Section 2.5 of the paper.  You
  are correct that ``most R primitives work with complex numbers,''
  but there are many commonly used functions that do not, including 
  \func{log1p}, \func{gamma}, and the probability distribution
  functions. This limitation, combined with the requirement that the function be
  holomorphic, and my own inexperience with the complex step method, make
  me hesitant to promote it too heavily in the paper. But it is a nifty
  trick, and I am happy that it is now included in the package.
 
In terms of accuracy, the paper now includes comparisons between the true
Hessian and the two \class{sparseHessianFD} methods (finite differences
and complex step). To maintain the focus of the paper on
\pkg{sparseHessianFD}, I am not including an assessment of the accuracy of
\pkg{numDeriv}.  However, for the
timing comparisons in Table 4, I did
change the baseline \pkg{numDeriv} method from \func{hessian} with the
\method{Richardson} method, to \func{jacobian} with both the
\method{simple} and 
\method{complex} methods. Removing the second-order approximation saves
a lot of time, and lets me compute Table 4 without resorting to
parallel computation. That will simplify attempts at replication, and
removes another variable that might influence results.

Footnote 1 was expanded to acknowledge the complex step method.



\end{response}


\item \begin{revQuote}
 p.13  l. -7  Figure 3b  should be Figure 3c
  \end{revQuote}

\begin{response}
  Fixed.  Thanks.
\end{response}


\item \begin{revQuote}
 The file replication.R does not set the RNG seed. This may not be too
important if only times are generated, but will be if resulting values are
included.
    
  \end{revQuote}

\begin{response}
  I added a \func{set.seed} statement near the top of
  \filename{replication.R}, but you are correct that because only
  times are reported, this does not matter much.
\end{response}


\item \begin{revQuote}
Table 4 and 5. Some OS, processor, and memory details are helpful to put
timing results in context.
  \end{revQuote}

\begin{response}
  This information is now included in the first paragraph of Section 4.
\end{response}


\item\begin{revQuote}
Table 4 is really not the proper comparison. I think a comparison with
       numDeriv::jacobian( binary.grad, P, method="simple", 
                   data=binary, priors=priors,
order.row=order.row)

really serves to highlight the improvement of the sparse calculation because
it is a valid comparison. Even though the results are not as exaggerated,
they are still important:

(My laptop is a  Intel(R) Core(TM) i5-3337U CPU @ 1.80GHz,  4GB RAM, SSD
swap, running Mint variant of Ubuntu 14.04.2 LTS.)

(Code and output removed)
  \end{revQuote}
  
  \begin{response}
    I agree.  See my response to Item \ref{it:complex2}.  Table 4
    includes times from the \pkg{numDeriv}
    \method{complex} and \method{simple} methods.

    To reduce clutter in Table 4, I removed comparisons of the
    hessian-to-gradient computation time ratio.  That was not a
    particularly interesting metric.
    
  \end{response}

\item\begin{revQuote}
It is possible to do a larger example with this comparison:

(On my laptop the next took about 30 hrs of which 24 was for the last,
N=2500, k=8 comparison.)

(Code and output removed)

  \end{revQuote}
  
  \begin{response}
Because the \func{numDeriv::jacobian} function runs so much faster
than  \func{numDeriv::hessian}, I was able to increase the size of the
examples in Table 4. One objective of Table 4 is to compare the relative
increases in computation time for different $N$ and $k$ between
\pkg{numDeriv} and \pkg{sparseHessianFD}, and I think that objective
is met with $N=(15, 50, 100, 500)$.  Please forgive me for not going as high as
$N=2500$, $k=8$. For the reader who wants to
replicate the results, a run time of more than a day is a lot to ask.
  \end{response}

\item\begin{revQuote}

 p.16  l. -7 "to to compute" -> "to compute"


  \end{revQuote}
  
  \begin{response}
    Fixed.  Thanks.
  \end{response}

\item\begin{revQuote}

While that package is useful, and reasonably demonstrated in the paper,
I think it would be nice to expand the paper in some ways that might be
deemed more "original research". Some possibilities are:
\begin{itemize}
 \item Explain and try to assess how much of the speedup is due to simple
sparseness and how much is due to the "sparse patern" (p2) allowing for
perturbing multiple elements together. (I think these are related but
slightly different?)
 
 \item Try to assess how much of the speedup is due to reduce computing demand
and how much is due to different memory demand. (I had the impression in the
larger problems with numDeriv that my computer started to use swap space,
which resulted in a big slow down.)
 
 \item Consider implementing a complex step method, and do a comparison.
 
 \item  Assess the difference when multiple CPUs are used. (run.par == FALSE vs
run.par == TRUE)
\end{itemize}
  \end{revQuote}
  
  \begin{response}
These are all reasonable suggestions.  Let me address them point by point.
    \begin{itemize}
    \item It really is all about the sparsity pattern, and
      specifically, the how few partitions are needed to group the
      variables.  Of course, it will be hard to keep the number of
      partitions small if there are too many non-zero elements, so the
      sparsity itself has some effect.  The arrangement of the non-zeros is
      related to the ordering of the variables, but the permutation
      handles that during the initialization of the
      \class{sparseHessianFD} argument.  For example, variables
      arranged in a way that the Hessian has a block-arrow pattern are
      permuted to get a Hessian with a banded pattern instead.  This
      is all discussed in Section 3.1.
      
\item I have no idea how to do this in a generalizable way.  I think
  it has a lot to do with the architecture of the computer
  itself.  For example, my computer has 64 GB of RAM, so memory usage
  is not a binding constraint, and swap space never comes into play.

  Figure 4 does break down the components of the computation
  into the time it takes to reserve memory (initialization), partition
  the variables, and compute gradients themselves.  Perhaps that is
  the kind of comparison you had in mind.

  I do not want to ignore or dismiss this suggestion, but I am simply not clear on
  what is being asked.  I am open to more specific recommendations.

\item The package now implements the complex step method, and comparisons of
  accuracy and computation time are included in the paper.  Thank you
  for this suggestion.

\item Parallel computing on a shared memory architecture really
  muddles up timing comparisons.  The \emph{only} reason I generated
  Table 4 in parallel was because of the run time of the \pkg{numDeriv} \func{hessian}
  function. Now that \func{hessian} was replaced with \func{jacobian},
  the baseline measures from \pkg{numDeriv} run fast enough that I can
  generate Table 4 in serial.

Your point that timing comparisons can be affected by
whether computation is being run on multiple processors is well taken, but the
``serialization'' of Table 4 makes the issue moot.

  Data for Figure 4 were generated in parallel because of the large
  number of required simulations.
    \end{itemize}

    
  \end{response}



  


\end{enumerate}

\end{document}